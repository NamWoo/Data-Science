## 5.7 Supervised Learning Algorithms

### 5.7.1 Probabilistic Supervised Learning

전통적 통계 방법론에서 주로 사용하는 확률을 활용한 지도학습 방법이다. $y$를 추정하기 위한 모확률분포 $p(y \mid \mathbf{x})$를 추정하기 위해 parameter vector $	\mathbf{\theta} $를 maximum likelihood estimation으로 찾는 방법으로, **linear regression** (least square 말고도 maximum likelihood로도 추정이 가능하다고 한다!) 또는 **logisitic regression**에 이용된다. 상세한 내용은 통계나 딥러닝 기초에서도 많이 다뤘으니 그냥 넘어가도록 하자.

 

### 5.7.2 Support Vector Machines

Support Vector Machines 는 위의 방법들과는 다르게, 어떠한 모확률분포를 가정하지 않는다는 점에서 위 모델들과 차별점을 가진다. 하지만, 그에 대한 trade-off로 추정한 y에 대한 확률값을 얻어낼 수가 없다. Decision Boundary를 구한다는 개념은 여타 모델과 비슷하나, Decision Boundary에서 가장 가까이 있는 Data (Support Vector)까지의 거리 (Margin)을 최대화 한다는 특이한 구조를 가지고 있다.

SVM이 각광받았던 이유는 위와 같은 것보단 **kernel trick** 을 활용하여 linear decision boundary가 훨씬 더 유연하게 작용할 수 있도록 했기 때문이다. 이 kernel trick는 vector로 표현되는 linear decision boundary $\mathbf{w}^T \mathbf{x}+b $를 아래와 같이 내적을 활용하여 표현하면서 시작된다.
$$
\mathbf{w}^T\mathbf{x}+b = b + \sum_{i=1}^{m}{\alpha_i}\mathbf{x}^T\mathbf{x}^{(i)}
$$
여기서 $\mathbf{x}^{(i)}$ 는 training example이고 $\mathbf{\alpha}$는 계수벡터다. 여기서 주어진 vector $\mathbf{x}$를 kernel function $k(\mathbf{x}, \mathbf{x}^{(i)}) = \phi(\mathbf{x})\dot{}\phi(\mathbf{x}^{(i)})$ 를 이용하여 
$$
f(\mathbf{x})=b + \sum_{i=1}^{m}{\alpha_i}k(\mathbf{x},\mathbf{x}^{(i)})
$$
으로 수정하는 것이다. kernel을 쉽게 이해할 수 있는 예제로는 XOR 문제가 있다. $(x_1, x_2)$ 의 축으로 구성된 2차원 공간을 kernel trick을 이용하여 $(x_1, x_2, x_1 \dot{}x_2 )$ 3차원 공간으로 변형한다면, linear decision boundary로 정확히 분류할 수 있다. 2차원 공간의 관점에서는 non-linear decision boundary 이지만, 3차원 공간에서는 linear problem이기 때문에 convex optimization 이 효율적으로 수렴할 수 있다.

일반적으로 많이 사용하는 kernel은 **Gausian kernel**과 **radial basis function** 이 있다. 이 또한, ISL에서 많이 다루기에 간단하게 설명만 하고 넘어간다.



### 5.7.3 Other Simple Supervised Learning Algorithms

여기서 필자는 kNN과 Decision Tree를 설명하고 있다. kNN은 순전히 데이터간의 '거리'만을 이용하여 classification이나 regression을 진행하기 때문에 어떠한 가정도 없이 사용할 수 있는 비모수 방법이다. 하지만, 마찬가지로 그에 대한 trade-off로 분류나 예측을 하고 나서도 어떤 변수가 이 분류나 회귀에 중요한 역할을 했는지에 대한 해석이 불가능하다. 또한, 주변의 모든 데이터에 대해 거리를 계산해야하기 때문에 많은 연산량이 필요하다. 또한, 차원(변수)가 높아질 수록 (많아질 수록) 데이터 간의 '거리'가 무의미해지기 때문에 short-fat data에는 적합한 학습방법이 아니다.



두 번째로 소개하는 Decision Tree 또한 모분포에 대한 가정이 전혀 없는 비모수 방법이다. (1984에 나온 논문이었군요!) Decision Tree는 정규화를 max_depth 같은 파라미터를 이용해서 (교재에서는 size_constraint 라고 묘사되어 있습니다.) 한다고 설명하고, 딱히 상세한 설명을 하지도 않습니다.. 궁금하면 ISL, ESL 보는걸로!



## 5.8 Unsupervised Learning Algorithms

비지도 학습의 핵심은 'label'을 가진 변수가 없고 feature만 존재한다는 것이다. 어떻게 하면 $\mathbf{x}$ 의 정보는 최대한 살리면서 *접근성이* 좋거나 *단순한* 데이터로 정제하는지가 핵심 논의이며 데이터의 특성, representation의 개념이 이 교재의 중심이될 것이라고 필자가 언급하면서 시작한다.



### 5.8.1 Principal Components Analysis

ISL 스터디에서 많이 다루었던 내용이다. 차원을 줄이면서 representation을 최대한으로 보존하는 비지도 학습 방법이며, 서로 간의 상관관계를 학습하고 데이터를 선형변환 (회전변환)한다. 정보를 최대한 보존하면서 차원을 줄일 수 있는 단순하면서도 효율적인 방법이며 주성분끼리는 수직이기 때문에 raw data를 decorrelate할 수 있다는 장점이 있다. 하지만, 각 주성분이 무엇을 의미하는지에 대한 해석이 어려워진다는 단점이 존재한다.

$$
Var(x) =\frac{1}{m-1}\mathbf{W{\sum}^2 W^T}
$$

$$
Var(z) =\frac{1}{m-1}\mathbf{{\sum}^2 }
$$

선형변환 $\mathbf{W}$에 의해 정의되는 $x$의 이미지 $z$는 위와 같은 성질을 가지는데 $\mathbf{{\sum}^2}$으로 표현되는 diagonal covariance matrix는 $z$의 각각의 원소들이 uncorrelated 되어 있다는 것을 확인해준다.

변수간의 독립성이 중요한 가정인 확률적 지도학습에는 이러한 단순 선형변환으로 정보를 최대한 보존하면서 가정을 만족시킬 수 있다.



### 5.8.2 k-means Clustering

k-means Clustering은 단순한 representation learning algorithm인데, training set을 k개의 다른 묶음으로 분리해준다. 이는 단순히 cluster의 이름만 남기기 때문에, one-hot encoding을 진행하게되면 매우 sparse한 representation이 된다. (사실상 거의 대부분의 정보를 날리는 것이니까)

알고리즘은 단순하다. k개의 다른 centroid
$$
{\mathbf{\mu}^{(1)},...,{\mu}^{(k)}}
$$
들을 정의하고 난 다음에, 데이터를 가장 거리가 가까운 centroid에 할당하는 것이다.

clustering의 문제점은 두 데이터가 실제로 '가까운'지에 대한 정의 기준이 애매하다는 것이다. 일반적으로 거리를 측정하기 위해 Euclidean distance를 많이 이용하나, 이 또한 차원이 높아지면 큰 의미가 없어지게 된다. 또한, [red, gray], [car, truck]의 binary 변수 값을 가지는 4개의 집단을 클러스터링 기법을 이용하여 구분하였을 때, 어떤 clustering 알고리즘은 빨강, 회색 집단을 나눌 수도 있고 어떤 알고리즘은 차와 트럭 2개의 집단으로 나눌 수도 있다. 이는 명확한 정보의 손실이며 빨강 차가 회색 차에 더 가까운지, 회색 트럭에 더 가까운지에 대한 정보 또한 얻을 수 없다.

이러한 이유때문에 필자는 distributed representation이 one-hot representation보다 선호되는 이유라고 말한다. (너무 컴과 관점인 것 같기도 하고)



## 5.9 Stochastic Gradient Descent

거의 대부분의 딥러닝 알고리즘은 SGD에 의해 powered된다고 한다. 이는 gradient descent algorithm의 확장 버전인데 large training set의 연산량의 어마어마한 것을 해결하기 위해 나온 알고리즘이다.  

예를 들어 cost function
$$
J(\mathbf{\theta})=\frac{1}{m}\sum_{i=1}^m L(x^{(i)},y^{(i)},\theta)
$$
를 연산하기 위해 필요한 gradient descent의 연산량은 $O(m)$ 이다.

training set size가 엄청나게 커질수록 연산량은 어마해지기 마련이다. 그렇기 때문에 training set으로 부터 uniform하게 추출된 적은 사이즈의 샘플 **minibatch** $\boldsymbol{\mathbb{B}}=\left\{  x^{(1)},...,x^{(m^{'})} \right\}$를 사용하여 **기댓값**을 추정할 수 있을 것이라는 것이 알고리즘의 기본 아이디어이다. $m^{'}$의 숫자는 1부터 few hundred까지 정도를 일반적으로 사용하는데 training set의 사이즈가 무한에 가까워질 수록 cost per SGD update, 즉 mini batch를 활용한 SGD에서의 cost는 변하지 않는다.

앞에서 언급한 kernel trick의 deep learning의 도래 전에는 매우 핫했으나, mxm matrix에 대해 kernel 연산을 하는 것은 $O(m^2)$ 의 연산량이 필요하고, 이는 big data에 적합하지 않다. deep learning의 등장 초기에는 경쟁 알고리즘 (SVM 등)을 이기는 것에 관심이 많았기에 초점이 medium-size data에 맞추어져 있으나, 실제 산업군에 관심을 가지고 난 뒤부터는 large data set에 초점이 맞춰지게 되었고, SGD의 개념이 탄생하였다. 이는 8장에서 조금 더 상세하게 다룬다.



## 5.10 Building a Machine Learning Algorithm

deep learning algorithm 들은 **데이터셋의 명확화, 비용함수, 최적화 과정, 모델**등의 simple recipe들로 설명이 가능하다. 비용함수는 일반적으로 통계적 추정을 하게 하는 term이 일반적으로 존재하는데, 일반적으로 많이 쓰이는 cost function인 negative log-likelihood의 경우 필연적으로 maximum likelihood estimation을 야기한다. 어떤, cost function의 경우는 실제 evaluating에 사용되는 function이 아닌 경우도 있는데, 이는 연산량의 문제 때문이다. 교재에 나와있지는 않지만, AUC (Area Under Curve)가 evaluating function인 경우도 이를 cost function으로 사용하지 않고 Misclassification error나 F1-Score	등을 이용한다.

최적화 과정은 일반적으로 GD, SGD 등이 많이 쓰이지만, 모든 모델에 대해 사용할 수 있는 것은 아니다. decision tree와 k-means clustering같은 경우가 그 예시인데, 그들의 cost function이 평평한 구간, flat region을 가지고 있기 때문에 Gradient Descent로 최적화 하는 것이 불가능하다. (Decision Tree에서는 Greedy Algorithm 등이 초기에 사용되었다고 배웠고, 지금은 모르겠네요 죄송..) 



## 5.11 Challenges Motivating Deep Learning

머신러닝을 사용하면서 발생하는 여러 문제들에 대해 소개한다. 하지만, 현대의 딥러닝의 주요 과제인 reconizing speech, recognizing objects 등에 해당되는 이야기는 아니다. deep learning이 전통적 AI 알고리즘의 아래와 같은 문제점들에 해결책을 제시해준다고 합니다..!



### 5.11.1 The Curse of Dimensionality

### 5.11.2 Local Constancy and Smoothness Regularization

### 5.11.3 Manifold Learning

