## Chapter 6. 선형모델 선택 및 Regularization

$$Y=\beta_0+\beta_1X_1+...+\beta_pX_p+\epsilon$$

- 최소제곱법 대신에 표준선형모델을 적합하기 위한 방법

  1) 서브셋 선택   2) 수축(shrinkage)   3) 차원축소



## 6.2 Shrinkage 방법

- 계수 추정치들을 제한하여 **p개의 설명변수를 모두 포함**하는 모델 적합

  => 계수 추정치들을 0으로 수축 => 추정치들의 분산 감소

- 회귀계수를 0으로 수축하는 대표적인 방법

  1) 능형회귀	2) lasso



### 6.2.1 능형회귀

> 능형회귀의 계수추정 절차

![6-1](.\image\6-1.PNG) <= **최소제곱 적합**에서는 RSS가 최소가 되도록 회귀계수 추정

![6-2](.\image\6-2.PNG) <= **능형회귀**에서는 RSS에 페널티 항 추가



  1) RSS를 작게 만들어 데이터에 잘 적합하도록 계수 추정치를 찾는다

  2) $$\lambda\sum_{j=1}^p\beta_j^2$$ (수축 페널티) 항은 $$\beta_1,...,\beta_p$$가 0에 가까울 때 작아지므로 $\beta_j$의 추정치를 0으로 

​      수축하는 효과

  3) **$\lambda$(조율 파라미터)**는 회귀계수 추정치에 대해 이 두 항의 상대적인 영향력을 제어한다



> Credit 자료에 대한 응용

![6-3](.\image\6-3.PNG)

- 왼쪽 패널

  - $\lambda$가 작을 경우, 능형계수 추정치 $\doteqdot$ 최소제곱 추정치
  - $\lambda$가 클 경우, 모든 능형 계수추정치가 0이 되어 영모델이 됨

- 오른쪽 패널

  -  $\hat{\beta}$는 최소제곱 계수 추정치들의 벡터
  -  $\lVert \beta \rVert_2=\sqrt{\sum_{j=1}^p\beta_j^2)}$ = 0에서 $\beta$까지의 거리(벡터의 $l_2  norm$)
  - $\lambda$ 증가 => $\hat{\beta}_\lambda^R$의 $l_2 norm$  감소 => $\frac{\lVert \hat{\beta}_\lambda^R \rVert_2}{\lVert \hat\beta \rVert_2}$ 감소( 범위: 1~0)
  -  x축을 능형회귀 계수 추정치가 0을 향해 수축된 양이라 생각할 수 있음

- 능형회귀와 Lasso는 다중회귀처럼 변수의 unit 변환시, 예를 들어 10배일때 계수추정치는 단순히 1/10이 되지 않는다. 오히려 계수는 현저하게 바뀔 수 있다(페널티 항 때문에)!  Scale에 의존적이지 않은 계수를 구하고 싶다면, 표준편차가 1인 설명변수의 표준화를 통해 계수 추정치를 나타낸다

  ![6-4](.\image\6-4.PNG)


> 능형회귀가 최소제곱보다 나은 이유

[능형회귀의 장점]

- 능형회귀의 장점은 **편향-분산 절충**에 있다

  $\lambda$ 증가 => 능형회귀 적합의 유연성 감소 => 분산 감소, 편향 증가

![6-5](.\image\6-5.PNG)

- 왼쪽 패널
  - p=45/ n=50/ 녹색: 능형회귀 예측값의 분산/ 검정색: 제곱편차/ 보라색: 검정 MSE/ 수평파선: 가능한 최소 MSE
- $\lambda$가 증가함에 따라 능형 계수추정치의 수축은 편향을 약간 증가시키지만 **분산을 현저하게 줄일 수 있다**
- 능형회귀는 **최소제곱 추정치가 높은 분산을 가질 때 가장 잘 동작**한다
- 최상의 부분집합 선택에서는 $2^p$개의 모델을 탐색해야 하는 계산상의 어려움이 있는 반면, 능형회귀는 **고정된 $\lambda$ 값에 대해 단 하나의 모델만 적합**하기에 시간적으로 효율적이다



[능형회귀의 단점]

- 최종모델에 p개 설명변수를 모두 포함 => 모델 해석력 ↓



### 6.2.2 Lasso

> Lasso의 계수추정 절차

![6-6](.\image\6-6.PNG)

- Lasso의 $l_1$ 페널티 항($\lVert \beta \rVert_1=\sum |\beta_j|$)은 조율파라미터 $\lambda$가 충분히 클 경우, **계수 추정치들 일부**를 **정확하게 0으로 추정** => **변수 선택** => 모델 해석력 ↑


![6-8](.\image\6-8.PNG)

- Credit 자료에 대한 lasso 적합
- **Lasso**는 $\lambda$의 값에 따라 **임의의 수의 변수들을 포함**하는 모델 생성
- **능형회귀**는 $\lambda$의 값에 따라 계수 추정치의 크기가 다르긴 하지만, **항상 모든 변수를 포함**하는 모델 생성






> 능형회귀와 Lasso에 대한 또 다른 구성

![6-9](.\image\6-9.PNG)<= lasso

![6-10](.\image\6-10.PNG)<= 능형회귀

![6-11](.\image\6-11.PNG)<= 최상의 부분집합 선택

<p=2일 때> 

- **lasso** 계수 추정치는 $|\beta_1|+|\beta_2|\le s$로 정의된 **마름모 내의 모든 점 중에서 RSS를 최소로** 함
- **능형회귀** 계수 추정치는 $\beta_1^2+\beta_2^2\le s$로 정의된 **원 내부의 모든 점 중에서 RSS를 최소로** 함

<p가 큰 경우>

- 최상의 부분집합 선택은 s개의 설명변수를 포함하는 ${p}\choose{s}$개의 모델을 모두 탐색해야 하는 문제
- **능형회귀와 lasso**는 위와 같은 상황에서의 **최상의 부분집합 선택의 계산 가능한 대안**으로 해석 가능



> Lasso의 변수선택 성질

![6-12](.\image\6-12.PNG)

- $\hat{\beta}$: 최소제곱의 해/ 빨간색 타원: 동일한 RSS에 해당하는 해들의 집합
- lasso와 능형회귀 계수 추정치는 **"어느 타원이 초록색에 해당하는 제한영역과 처음으로 만나는가?"**에 의해 주어진다
- **능형회귀**- **뾰족한 부분이 없는 원의 제한영역**이므로, 타원과 제한영역의 교점은 일반적으로 축상에 존재하지 않는다. 따라서 능형회귀 **계수 추정치는 0이 되지 않는다**.
- **lasso**- **각 축에 뾰족한 모서리를 가지는 제한영역**이므로, 타원은 종종 축상에서 제한영역과 교점을 가질 것이다. 따라서 **계수들 중 일부는 정확히 0이 될 것**이다. 



> Lasso와 능형회귀 비교

![6-13](.\image\6-13.PNG)

<45개의 설명변수가 모두 반응변수와 관련된 경우>

- 오른쪽 패널
  - 파선: 능형회귀 적합
  - lasso와 능형회귀의 편향은 비슷, 분산은 능형회귀의 경우 더 낮아 능형회귀의 최소 MSE가 더 낮음



<45개의 설명변수 중 단 2개만이 반응변수와 관련된 경우>

![6-14](.\image\6-14.PNG)

- 오른쪽 패널
  - lasso가 편향, 분산, MSE 모두에서 능형회귀보다 성능이 좋음

[어떤 경우에 lasso와 능형회귀를 사용해야 하는가?]

- 두 방법 모두 **최소제곱 추정치가 높은 분산을 가질 때** 약간의 편향 증가로 분산을 크게 줄임

- **lasso**: 비교적 적은 수의 설명변수가 상당히 큰 계수를 가지고 나머지 변수들은 계수가 아주 작거나 

  ​            0인 경우 성능 good

- **능형회귀**: 반응변수가 많은 설명변수들의 함수이고 그 계수들이 거의 동일한 크기일 때 성능 good



> 능형회귀와 Lasso에 대한 특별한 사례

<n=p이고 대각원소는 1, 비대각원소는 0인 대각행렬 **X**를 가지고 절편이 없는 회귀를 가정>

- 최소제곱:           $\sum_{j=1}^{p}(y_j-\beta_j)^2$
- 능형회귀:           $\sum_{j=1}^{p}(y_j-\beta_j)^2+\lambda\sum_{j=1}^{p}\beta_j^2$
- lasso:                 $\sum_{j=1}^{p}(y_j-\beta_j)^2+\lambda\sum_{j=1}^{p}|\beta_j|$



- 능형회귀 추정치:   $\hat{\beta_{j}^{R}}=\frac{y_j}{1+\lambda}$
- lasso 추정치: ![6-15](.\image\6-15.PNG)
- **능형회귀**는 데이터의 모든 차원을 **같은 비율로 축소**
- **lasso**는 모든 계수들을 **같은 양만큼 0을 향해 축소**하고 **충분히 작은 계수들은 완전히 0으로 수축**



> 능형회귀와 Lasso에 대한 베이즈 해석

1. 계수벡터 $\beta$가 사전분포 $p(\beta)$를 가진다고 가정

- $\beta=(\beta_0,\beta_1,...,\beta_p)^T$

- $X=(X_1,...,X_p)$

- 가능도: $f(Y|X,\beta)$

- 사후분포는 사전분포와 가능도를 곱해서 얻을 수 있다

  $p(\beta|X,Y) \varpropto f(Y|X,\beta)p(\beta|X)=f(Y|X,\beta)p(\beta)$



2. 표준선형모델에서 오차들은 서로 독립이고 정규분포를 따른다고 가정

   $$Y=\beta_0+\beta_1X_1+...+\beta_pX_p+\epsilon$$



3. 어떤 밀도함수 $g$에 대해  $p(\beta)=\prod_{j=1}^{p}g(\beta_j)$라고 가정

- 능형회귀와 lasso는 $g$의 두가지 특수한 경우로부터 얻어지는 것으로 해석 가능
- $g$가 평균이 0이고 표준편차가 $\lambda$의 함수인 가우스 분포이면, $\beta$에 대한 사후모드, 즉 주어진 데이터에서 $\beta$에 대해 가장 가능성이 높은 값은 능형회귀의 해
- $g$가 평균이 0이고 스케일 파라미터가 $\lambda$의 함수인 이중지수 분포이면, $\beta$에 대한 사후모드는 lasso의 해

![6-16](.\image\6-16.PNG)

- 왼쪽 패널
  - 가우스 사전분포(능형회귀)
  - 0에서 평평=> 계수들이 0 부근에서 랜덤하게 분포
- 오른쪽 패널
  - 이중지수 사전분포(lasso)
  - 0에서 급격하게 뾰족해짐=> 계수들 중 다수가 정확히 0이 될 것



### 6.2.3 조율 파라미터 선택

1. $\lambda$ 값들의 격자를 선택
2. 각각의 $\lambda$ 값에 대한 교차검증 오차 계산
3. 교차검증 오차를 최소로 하는 조율 파라미터($\lambda$) 선택



## 6.3 차원축소 방법

- **변환된 설명변수들을 사용**해 최소제곱모델 적합

- $Z_1,Z_2,...,Z_m$은 원래의 p개 설명변수들의 M(<p)개 선형결합이라 가정

- 어떤 상수들 $\phi_{1m},\phi_{2m},...,\phi_{pm}, m=1,...,M$에 대해 

  $Z_m=\sum_{j=1}^{p}\phi_{jm}X_j$

- 최소제곱을 사용하여 선형회귀모델 적합 가능

   $y_i=\theta_0+\sum_{m=1}^{M}\theta_mz_{im}+\epsilon_i$

- p가 n에 비해 상당히 큰 경우 M<<p인 값을 선택하면 적합된 계수들의 분산을 현저하게 줄일 수 있다

- $Z_1,Z_2,...,Z_n$의 선택 

  1) 주성분   2)부분최소제곱



### 6.3.1 주성분회귀

- 주성분분석(PCA): 커다란 변수들의 집합에서 저차원의 변수집합을 유도하기 위해 사용

  ​				n x p 데이터 행렬 $X$의 차원을 줄이는 기법

> 주성분분석의 개요

![6-17](.\image\6-17.PNG)

- **첫번째 주성분 방향**은 **관측치들이 가장 크게 변화하는 방향**
  - 100개의 관측치를 해당 직성 위로 투영하면 투영된 관측치들은 가장 큰 분산을 가짐
  - $Z_1$=0.839 x ($pop-\overline{pop}$) + 0.544 x ($ad-\overline{ad}$)
    - $\phi_{11}^2+\phi_{21}^2=1$인 pop과 ad의 모든 가능한 선형결합 중 분산을 가장 높게 하는 형태
  - $z_{i1}(주성분점수)$ =  0.839 x ($pop_i-\overline{pop}$) + 0.544 x ($ad_i-\overline{ad}$) 

![6-18](.\image\6-18.PNG)

- 두번째 주성분 $Z_2$는 $Z_1$과 무상관(직교)이며 가장 큰 분산을 갖는 변수들의 선형결합

  - $Z_2$=0.544 x ($pop-\overline{pop}$) - 0.839 x ($ad-\overline{ad}$)

    ![6-19](.\image\6-19.PNG)


- 주성분들은 순차적으로 선행하는 주성분들과 상관관계가 없다는 조건 하에서 분산을 최대로 함

  > 주성분의 기하학적 해석

  ![6-26](.\image\6-26.PNG)




> 주성분회귀기법(PCR)

- **적은 수의 설명변수들**로 데이터 내의 대부분의 변동성과 반응변수와의 상관관계를 설명할 수 있다

- 기본가정: $X_1,...,X_p$의 변동이 큰 방향들이 $Y$와 관련 있는 방향이다

  ![6-20](.\image\6-20.PNG)

  - 왼쪽 패널: 반응변수가 모든 설명변수의 함수일 때
  - 오른쪽 패널: 반응변수가 오로지 2개의 설명변수의 함수일 때
  - 더 많은 주성분이 사용될수록 편향 감소, 분산 증가
  - 적절한 M을 선택하면 최소제곱에 비해 성능 개선

[어떤 경우에 PCR이 좋은 성능을 보이는가?]

- 처음 몇 개의 주성분으로 설명변수들의 변동 대부분과 반응변수와의 상관관계를 얻을 수 있는 경우

  ![6-21](.\image\6-21.PNG)

  - 오른쪽 패널: lasso(실선)/ 능형회귀(점선)
  - M이 증가함에 따라 편향 급격하게 감소
  - MSE는 M=5일 때 최소가 됨



- PCR은 M(<p) 개의 설명변수를 사용하지만 변수선택 방법은 아님 => 능형회귀와 관련!
- 주성분의 수 M은 보통 교차검증에 의해 선택 



### 6.3.2 부분최소제곱(PLS)

- PCR 기법은 비지도방식

  - 반응변수가 주성분 방향을 결정하는 데 이용되지 않기 때문

- 부분최소제곱은 **PCR의 지도식 대안**

  - 원래 변수들의 선형결합인 새로운 변수들 $Z_1,...,Z_m$을 지도식 방식으로 찾은 뒤 최소제곱 적용
  - 반응변수 Y를 이용해 원래 변수들을 잘 근사시키고 반응변수와 관련있는 새로운 변수들을 식별

- 첫번째 PLS 방향

  - p개의 설명변수들을 표준화한 후, $Z_m=\sum_{j=1}^{p}\phi_{jm}X_j$에서 $\phi_{j1}$ 각각을 $Y$의 $X_j$에 대한 단순선형회귀 계수와 동일하게 설정하여 $Z_1$ 계산
  - 반응변수와 가장 강하게 관련되어 있는 변수들에 가장 높은 가중치 부여

  ![6-22](.\image\6-22.PNG)

  - 실선: 첫번째 PLS 방향/ 점선: 첫번째 PCA 방향
  - PLS 방향은 PCA만큼 가깝게 설명변수들을 적합하지 않지만 **반응변수를 더 잘 설명**

- 두번째 PLS 방향

  - $Z_1$에 대해 각 변수의 회귀를 수행하고 잔차를 취해 변수 각각을 $Z_1$에 대해 먼저 조정
  - 원래 데이터로 $Z_1$을 계산했던 동일한 방법으로 $Z_2$ 계산
  - M번 반복해 $Z_1,...,Z_m$ 찾기

- PLS의 지도식 차원축소는 편향을 줄이지만 분산을 증가시킬 가능서이 있어 PCR보다 우월하다고 할 수는 x



## 6.4 고차원의 고려

### 6.4.1 고차원 데이터

- 회귀와 분류에 관한 대부분의 기법은 n>>p(저차원)인 경우를 위한 것
- 고차원: 관측치보다 더 많은 변수를 포함



### 6.4.2  고차원에서 무엇이 문제인가?

- p>n인 경우, 설명변수와 반응변수 사이에 상관관계가 존재하는지 여부에 관계없이 최소제곱은 데이터에 완벽하게 적합되어 잔차들이 0인 계수 추정치들의 집합을 제공할 것 

  ![6-23](.\image\6-23.PNG)

  - 설명변수가 1개이고, 관측치가 각각 20개,2개인 경우

  - p>n 혹은 p$\approx$n일 때, 회귀적합의 유연성이 커져 데이터를 과적합한다

  ![6-24](.\image\6-24.PNG)

  - n=20이고, 반응변수와 전혀 상관이 없는 변수를 1개~20개까지 사용하여 회귀 수행
  - 모델에 포함되는 변수의 수가 증가할수록 $R^2\uparrow$, $train MSE \downarrow$
  - but test MSE ↑
    - 추가적인 설명변수를 포함할수록 계수 추정치들의 분산이 커지기 때문



### 6.4.3 고차원에서의 회귀

- **전진 단계적 선택/능형회귀/lasso/주성분회귀**와 같은 방법은 최소제곱보다 **유연성이 낮은 적합 방식**을 사용해 **과적합을 피하고**, **고차원 설정에서 회귀를 수행**하는 데 유용하다

  ![6-25](.\image\6-25.PNG)

  - 20개의 설명변수만 반응변수와 관련이 있는 경우 lasso 적합 시 검정 MSE 측정

  - x축인 자유도는 영이 아닌 계수 추정치들의 개수이며 lasso 적합의 유연성의 측도

  - 추가되는 변수가 반응변수가 실제로 관련이 있지 않으면, 검정오차는 차원이 증가함에 따라 증가

    - 차원의 저주

    - noise 변수들이 차원을 증가시켜 검정셋 오차면에서는 어떠한 개선도 없이 과적합 위험을 

      악화시키기 때문



### 6.4.4 고차원에서의 결과 해석

- 고차원에서 다중공선성에 매우 유의해야 함
  - 모델 내 임의의 변수는 다른 변수들의 선형결합으로 나타낼 수 있음
  - 어느 변수가 결과예측에 실제로 관련이 있는지 정확한 판단 불가
  - 회귀에서의 가장 좋은 계수 식별 불가능
- 모델적합의 오차와 측도를 알리는 데 유의해야 함
  - p>n일 때 잔차들이 0인 쓸모없는 모델을 얻을 수 있음
  - train data에 대한 모델적합의 RSS, p-value, $R^2$ 등의 측도를 모델적합이 좋다는 증거로 사용해서는 x
  - 대신, **독립적인 검정셋에 대한 결과**나 **교차검증 오차**를 알리는 것이 중요