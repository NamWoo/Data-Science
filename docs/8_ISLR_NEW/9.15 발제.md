## ISL 스터디 계획

| 주차 |   일자    |        진도         |  발제일   | 발제자 |
| :--: | :-------: | :-----------------: | :-------: | :----: |
|  1   | 09월 12일 | 스터디 계획, ~2.1.2 | 09월 15일 | 윤서진 |
|  2   | 09월 19일 |      Chapter 2      | 09월 29일 |        |
|  3   | 09월 26일 |      Chapter 3      | 10월 07일 |        |
|      |           |     중 간 고 사     |           |        |
|  4   | 10월 24일 |    Chapter 4(上)    | 10월 27일 |        |
|  5   | 10월 31일 |  Chapter 4(下), 5   | 11월 03일 |        |
|  6   | 11월 07일 |    Chapter 6(上)    | 11월 10일 |        |
|  7   | 11월 14일 |    Chapter 6(下)    | 11월 17일 |        |
|  8   | 11월 21일 |      Chapter 7      | 11월 24일 |        |
|  9   | 11월 28일 |      Chapter 8      | 12월 01일 |        |
|      |           |     기 말 고 사     |           |        |



__#02 통계학습__

입력변수(X): 설명변수, 예측변수, 독립변수, 특징, 변수

출력변수(Y): 반응변수, 응답변수, 종속변수

일반적으로 fixed unknown function과 error term의 형태로 나타낸다

![Y = f(X)+\epsilon](https://latex.codecogs.com/png.latex?Y%20%3D%20f%28X%29%2B%5Cepsilon)



__2.1.1 ![f](https://latex.codecogs.com/png.latex?f)를 추정하는 이유__

예측과 추론

__예측__

error term averages to zero이기에 다음과 같이 예측

![\hat{Y}=\widehat{f}(X)](https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%3D%5Cwidehat%7Bf%7D%28X%29)

_black box:_

![\hat{f}](https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D) 은 ![Y](https://latex.codecogs.com/png.latex?Y)에 대한 정확한 예측을 제공한다면 정확한 형태에 대해서는 신경쓰지 않아 블랙 박스로 취급

정확도(accuracy): reducible error와 irreducible error에 달려있다

Reducible error : 적절한 통계적 학습 기술을 이용해 줄일 수 있는 ![error](https://latex.codecogs.com/png.latex?error)

Irreducible error:  ![\epsilon](https://latex.codecogs.com/png.latex?%5Cepsilon) 를  설명변수를 사용하여 예측할 수 없기 때문, 관련 변동성은 예측의 정확성에 영향,  ![\epsilon](https://latex.codecogs.com/png.latex?%5Cepsilon) 관련 측정할 수 없는 변동성 포함할 수 있음

$$E[f(X)+\epsilon-\hat{f}(X)]^2 = [f(X)-\hat{f}(X)]^2+ var(\epsilon)$$





__추론__

 ![Y](https://latex.codecogs.com/png.latex?Y) 가 ![X](https://latex.codecogs.com/png.latex?X) 에 의해 어떻게 영향을 받는지에 대한 이해에 관심

 ![\hat{f}](https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D)의 정확한 형태를 알아야하기 때문에 더이상 블랙박스로 취급받지 않음

*어떤 _설명변수_들이 _반응변수_와 관련되어 있는가?

*_반응변수_와 _설명변수_간의 상관관계

*![Y](https://latex.codecogs.com/png.latex?Y)와 각  설명변수의 상관관계는 linear equation을 통해 충분이 요약 될 수 있는가/없는가? (보통 선형 모델로 상관관계 정확히 표현 어려움)

최종 목적이 __예측__, __추론__, 또는 둘의 결합인지에 따라 ![f](https://latex.codecogs.com/png.latex?f) 추정하는 데 사용하는 방법이 달라짐.

-_선형모델_: 간단하고 해석 가능한 추론이 가능하나 정확한 예측이 어려울 수 있음

-_비선형모델_: 정확한 예측 제공, 추론을 어렵게 만드는 이해 어려운 모델 초래



__2.1.2  ![f](https://latex.codecogs.com/png.latex?f)의 추정__

여러 기법의 공통적 특징:  training data에서 언제나 n개의 다른 데이터 포인트 관측을 가정

목적: 통계 학습 방법을 training data에 적용, 알려지지 않은  ![f](https://latex.codecogs.com/png.latex?f) 추정

___Parametric_ vs _Non-parametric Methods___

__*Parametric Method (2 steps)__

-함수 형태/모양에 대해 가정 (예: linear model)

-모델이 선정된 후 training data를 사용해 모델 fit, 훈련 (model fit or train)

Pros : 추정 문제의 단순화

Cons: ![f](https://latex.codecogs.com/png.latex?f)의 실제 형태와 보통 맞지 않음, _유연한 모델 선택 시도_시 overfitting의 문제



**Non-parametric Method**

-![f](https://latex.codecogs.com/png.latex?f)의 함수 형태에 대한 가정 없음. 데이터 포인트에 가능하면 가까운 ![f](https://latex.codecogs.com/png.latex?f)를 추정할 뿐

Pros : 더 넓은 범위의 ![f](https://latex.codecogs.com/png.latex?f)형태에 정확하게 적합될 가능성 높음

Cons: ![f](https://latex.codecogs.com/png.latex?f)에대해 정확한 추정 위해서는 아주 많은 관측치 필요



