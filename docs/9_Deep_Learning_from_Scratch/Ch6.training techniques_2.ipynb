{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화의 장점\n",
    "\n",
    "- 학습을 빠르게 진행할 수 있다. (학습 속도 개선)\n",
    "- 초깃값에 크게 의존하지 않는다. (초깃값 선택 고민을 줄여준다.)\n",
    "- 오버피팅을 억제한다. (드롭아웃 필요성 감소)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why?) 정규화를 통해 각 hidden layer에서 입력데이터의 분포가 일정하게 되고, learning rate를 크게 설정해도 괜찮다. 결과적으로 학습속도가 빨라지게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://i.imgur.com/Tpcfiw4.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화는 각 층의 활성화 함수의 앞에 **배치 정규화 계층(batch norm)**을 넣어, 각 층에서 학습 시 각 미니배치를 단위로 평균이 0, 분산이 1이 되도록 정규화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://i.imgur.com/NiyHN7u.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때 **ε**은 0으로 나누는 경우를 예방한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://i.imgur.com/Ef7P0KX.png\" width=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 **배치 정규화 계층(batch norm)**마다 정규화 시킨 데이터에 확대와 이동 변환을 수행한다. **γ**는 확대 변환을, **β**는 이동 변환을 담당하는 parameter이며, 학습하면서 적합한 parameter로 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://i.imgur.com/7diLWkV.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화 과정을 계산 그래프로 나타내면 위와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    \n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
    "\n",
    "        # 시험할 때 사용할 평균과 분산\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward 시에 사용할 중간 데이터\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta # gamma와 beta를 받아서 학습값을 조정\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma # gamma값 업데이트\n",
    "        self.dbeta = dbeta # beta값 업데이트\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 오버피팅의 억제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅이 일어나는 환경\n",
    "\n",
    "- 매개변수가 많고 표현력이 높은 모델을 사용하는 경우 (Deep learning의 경우에는 신경망의 층을 깊게 만드는 경우)\n",
    "- Training data가 적은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import Trainer\n",
    "from common.util import shuffle_dataset\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오버피팅을 재현하기 위해 의도적으로 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)\n",
    "optimizer = SGD(lr=0.01) # learning rate가 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.07, test acc:0.1045\n",
      "epoch:1, train acc:0.08666666666666667, test acc:0.1154\n",
      "epoch:2, train acc:0.10666666666666667, test acc:0.121\n",
      "epoch:3, train acc:0.12333333333333334, test acc:0.1299\n",
      "epoch:4, train acc:0.15333333333333332, test acc:0.1426\n",
      "epoch:5, train acc:0.17666666666666667, test acc:0.1577\n",
      "epoch:6, train acc:0.21333333333333335, test acc:0.173\n",
      "epoch:7, train acc:0.25666666666666665, test acc:0.1922\n",
      "epoch:8, train acc:0.2866666666666667, test acc:0.2098\n",
      "epoch:9, train acc:0.2966666666666667, test acc:0.2209\n",
      "epoch:10, train acc:0.32, test acc:0.2362\n",
      "epoch:11, train acc:0.32, test acc:0.236\n",
      "epoch:12, train acc:0.36666666666666664, test acc:0.2595\n",
      "epoch:13, train acc:0.37, test acc:0.2766\n",
      "epoch:14, train acc:0.4033333333333333, test acc:0.3023\n",
      "epoch:15, train acc:0.4166666666666667, test acc:0.3067\n",
      "epoch:16, train acc:0.4533333333333333, test acc:0.341\n",
      "epoch:17, train acc:0.4633333333333333, test acc:0.3631\n",
      "epoch:18, train acc:0.49, test acc:0.3794\n",
      "epoch:19, train acc:0.5233333333333333, test acc:0.4028\n",
      "epoch:20, train acc:0.5266666666666666, test acc:0.4169\n",
      "epoch:21, train acc:0.5833333333333334, test acc:0.4484\n",
      "epoch:22, train acc:0.6133333333333333, test acc:0.478\n",
      "epoch:23, train acc:0.6166666666666667, test acc:0.4921\n",
      "epoch:24, train acc:0.6533333333333333, test acc:0.5106\n",
      "epoch:25, train acc:0.67, test acc:0.5162\n",
      "epoch:26, train acc:0.6933333333333334, test acc:0.5344\n",
      "epoch:27, train acc:0.73, test acc:0.5573\n",
      "epoch:28, train acc:0.7233333333333334, test acc:0.5642\n",
      "epoch:29, train acc:0.7333333333333333, test acc:0.5756\n",
      "epoch:30, train acc:0.7433333333333333, test acc:0.5885\n",
      "epoch:31, train acc:0.7633333333333333, test acc:0.5943\n",
      "epoch:32, train acc:0.7666666666666667, test acc:0.5981\n",
      "epoch:33, train acc:0.7766666666666666, test acc:0.6055\n",
      "epoch:34, train acc:0.7833333333333333, test acc:0.6062\n",
      "epoch:35, train acc:0.7833333333333333, test acc:0.6185\n",
      "epoch:36, train acc:0.79, test acc:0.6131\n",
      "epoch:37, train acc:0.8066666666666666, test acc:0.6252\n",
      "epoch:38, train acc:0.81, test acc:0.6314\n",
      "epoch:39, train acc:0.8233333333333334, test acc:0.6331\n",
      "epoch:40, train acc:0.8133333333333334, test acc:0.6367\n",
      "epoch:41, train acc:0.8233333333333334, test acc:0.6372\n",
      "epoch:42, train acc:0.8233333333333334, test acc:0.6463\n",
      "epoch:43, train acc:0.8433333333333334, test acc:0.6503\n",
      "epoch:44, train acc:0.8366666666666667, test acc:0.6555\n",
      "epoch:45, train acc:0.8466666666666667, test acc:0.6608\n",
      "epoch:46, train acc:0.86, test acc:0.6582\n",
      "epoch:47, train acc:0.84, test acc:0.6653\n",
      "epoch:48, train acc:0.8533333333333334, test acc:0.664\n",
      "epoch:49, train acc:0.8633333333333333, test acc:0.6721\n",
      "epoch:50, train acc:0.8533333333333334, test acc:0.6757\n",
      "epoch:51, train acc:0.88, test acc:0.6689\n",
      "epoch:52, train acc:0.8866666666666667, test acc:0.6755\n",
      "epoch:53, train acc:0.88, test acc:0.6834\n",
      "epoch:54, train acc:0.8833333333333333, test acc:0.6825\n",
      "epoch:55, train acc:0.8733333333333333, test acc:0.6609\n",
      "epoch:56, train acc:0.9033333333333333, test acc:0.6765\n",
      "epoch:57, train acc:0.9033333333333333, test acc:0.6852\n",
      "epoch:58, train acc:0.9033333333333333, test acc:0.6924\n",
      "epoch:59, train acc:0.9166666666666666, test acc:0.6945\n",
      "epoch:60, train acc:0.9133333333333333, test acc:0.6942\n",
      "epoch:61, train acc:0.9333333333333333, test acc:0.6975\n",
      "epoch:62, train acc:0.94, test acc:0.6996\n",
      "epoch:63, train acc:0.93, test acc:0.6971\n",
      "epoch:64, train acc:0.9, test acc:0.6798\n",
      "epoch:65, train acc:0.9433333333333334, test acc:0.7005\n",
      "epoch:66, train acc:0.9133333333333333, test acc:0.6992\n",
      "epoch:67, train acc:0.94, test acc:0.698\n",
      "epoch:68, train acc:0.9433333333333334, test acc:0.7008\n",
      "epoch:69, train acc:0.9466666666666667, test acc:0.7089\n",
      "epoch:70, train acc:0.95, test acc:0.7109\n",
      "epoch:71, train acc:0.9466666666666667, test acc:0.7074\n",
      "epoch:72, train acc:0.94, test acc:0.7048\n",
      "epoch:73, train acc:0.9566666666666667, test acc:0.7043\n",
      "epoch:74, train acc:0.96, test acc:0.7089\n",
      "epoch:75, train acc:0.9633333333333334, test acc:0.7105\n",
      "epoch:76, train acc:0.96, test acc:0.7105\n",
      "epoch:77, train acc:0.9633333333333334, test acc:0.7154\n",
      "epoch:78, train acc:0.9633333333333334, test acc:0.7164\n",
      "epoch:79, train acc:0.9766666666666667, test acc:0.715\n",
      "epoch:80, train acc:0.97, test acc:0.7186\n",
      "epoch:81, train acc:0.97, test acc:0.7188\n",
      "epoch:82, train acc:0.9766666666666667, test acc:0.716\n",
      "epoch:83, train acc:0.9733333333333334, test acc:0.7212\n",
      "epoch:84, train acc:0.9766666666666667, test acc:0.7183\n",
      "epoch:85, train acc:0.9733333333333334, test acc:0.7228\n",
      "epoch:86, train acc:0.9766666666666667, test acc:0.7232\n",
      "epoch:87, train acc:0.9766666666666667, test acc:0.7225\n",
      "epoch:88, train acc:0.97, test acc:0.7197\n",
      "epoch:89, train acc:0.9766666666666667, test acc:0.723\n",
      "epoch:90, train acc:0.9833333333333333, test acc:0.7198\n",
      "epoch:91, train acc:0.9833333333333333, test acc:0.7222\n",
      "epoch:92, train acc:0.98, test acc:0.7211\n",
      "epoch:93, train acc:0.98, test acc:0.7249\n",
      "epoch:94, train acc:0.9866666666666667, test acc:0.7271\n",
      "epoch:95, train acc:0.9833333333333333, test acc:0.7241\n",
      "epoch:96, train acc:0.98, test acc:0.7246\n",
      "epoch:97, train acc:0.98, test acc:0.7282\n",
      "epoch:98, train acc:0.98, test acc:0.7285\n",
      "epoch:99, train acc:0.98, test acc:0.7312\n",
      "epoch:100, train acc:0.98, test acc:0.7287\n",
      "epoch:101, train acc:0.9833333333333333, test acc:0.7273\n",
      "epoch:102, train acc:0.9833333333333333, test acc:0.7313\n",
      "epoch:103, train acc:0.99, test acc:0.725\n",
      "epoch:104, train acc:0.9933333333333333, test acc:0.7248\n",
      "epoch:105, train acc:0.99, test acc:0.7333\n",
      "epoch:106, train acc:0.9866666666666667, test acc:0.7329\n",
      "epoch:107, train acc:0.9866666666666667, test acc:0.7291\n",
      "epoch:108, train acc:0.9933333333333333, test acc:0.7289\n",
      "epoch:109, train acc:0.9866666666666667, test acc:0.7281\n",
      "epoch:110, train acc:0.9933333333333333, test acc:0.7296\n",
      "epoch:111, train acc:0.99, test acc:0.7365\n",
      "epoch:112, train acc:0.9933333333333333, test acc:0.736\n",
      "epoch:113, train acc:0.9933333333333333, test acc:0.7332\n",
      "epoch:114, train acc:0.99, test acc:0.7281\n",
      "epoch:115, train acc:0.9933333333333333, test acc:0.7344\n",
      "epoch:116, train acc:0.9966666666666667, test acc:0.7356\n",
      "epoch:117, train acc:0.9966666666666667, test acc:0.7307\n",
      "epoch:118, train acc:0.9966666666666667, test acc:0.7364\n",
      "epoch:119, train acc:0.9966666666666667, test acc:0.7297\n",
      "epoch:120, train acc:0.9966666666666667, test acc:0.7376\n",
      "epoch:121, train acc:0.9966666666666667, test acc:0.7367\n",
      "epoch:122, train acc:0.9933333333333333, test acc:0.7358\n",
      "epoch:123, train acc:0.9933333333333333, test acc:0.7352\n",
      "epoch:124, train acc:1.0, test acc:0.7356\n",
      "epoch:125, train acc:1.0, test acc:0.7378\n",
      "epoch:126, train acc:0.9966666666666667, test acc:0.7375\n",
      "epoch:127, train acc:0.9966666666666667, test acc:0.7392\n",
      "epoch:128, train acc:0.9966666666666667, test acc:0.7318\n",
      "epoch:129, train acc:0.9966666666666667, test acc:0.7349\n",
      "epoch:130, train acc:1.0, test acc:0.7345\n",
      "epoch:131, train acc:0.9966666666666667, test acc:0.7362\n",
      "epoch:132, train acc:1.0, test acc:0.7371\n",
      "epoch:133, train acc:1.0, test acc:0.7365\n",
      "epoch:134, train acc:1.0, test acc:0.7382\n",
      "epoch:135, train acc:1.0, test acc:0.7385\n",
      "epoch:136, train acc:1.0, test acc:0.739\n",
      "epoch:137, train acc:1.0, test acc:0.7391\n",
      "epoch:138, train acc:1.0, test acc:0.739\n",
      "epoch:139, train acc:1.0, test acc:0.7383\n",
      "epoch:140, train acc:1.0, test acc:0.7377\n",
      "epoch:141, train acc:1.0, test acc:0.7395\n",
      "epoch:142, train acc:0.9966666666666667, test acc:0.741\n",
      "epoch:143, train acc:1.0, test acc:0.7403\n",
      "epoch:144, train acc:1.0, test acc:0.7384\n",
      "epoch:145, train acc:1.0, test acc:0.7401\n",
      "epoch:146, train acc:1.0, test acc:0.7409\n",
      "epoch:147, train acc:1.0, test acc:0.7406\n",
      "epoch:148, train acc:1.0, test acc:0.7393\n",
      "epoch:149, train acc:1.0, test acc:0.7439\n",
      "epoch:150, train acc:1.0, test acc:0.743\n",
      "epoch:151, train acc:1.0, test acc:0.738\n",
      "epoch:152, train acc:1.0, test acc:0.7411\n",
      "epoch:153, train acc:1.0, test acc:0.7408\n",
      "epoch:154, train acc:1.0, test acc:0.7429\n",
      "epoch:155, train acc:1.0, test acc:0.7425\n",
      "epoch:156, train acc:1.0, test acc:0.7427\n",
      "epoch:157, train acc:1.0, test acc:0.7409\n",
      "epoch:158, train acc:1.0, test acc:0.741\n",
      "epoch:159, train acc:1.0, test acc:0.7421\n",
      "epoch:160, train acc:1.0, test acc:0.7415\n",
      "epoch:161, train acc:1.0, test acc:0.7413\n",
      "epoch:162, train acc:1.0, test acc:0.7431\n",
      "epoch:163, train acc:1.0, test acc:0.744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:164, train acc:1.0, test acc:0.7438\n",
      "epoch:165, train acc:1.0, test acc:0.7435\n",
      "epoch:166, train acc:1.0, test acc:0.7444\n",
      "epoch:167, train acc:1.0, test acc:0.7433\n",
      "epoch:168, train acc:1.0, test acc:0.7446\n",
      "epoch:169, train acc:1.0, test acc:0.7414\n",
      "epoch:170, train acc:1.0, test acc:0.7438\n",
      "epoch:171, train acc:1.0, test acc:0.744\n",
      "epoch:172, train acc:1.0, test acc:0.7422\n",
      "epoch:173, train acc:1.0, test acc:0.742\n",
      "epoch:174, train acc:1.0, test acc:0.7443\n",
      "epoch:175, train acc:1.0, test acc:0.7441\n",
      "epoch:176, train acc:1.0, test acc:0.7435\n",
      "epoch:177, train acc:1.0, test acc:0.7434\n",
      "epoch:178, train acc:1.0, test acc:0.7434\n",
      "epoch:179, train acc:1.0, test acc:0.743\n",
      "epoch:180, train acc:1.0, test acc:0.7432\n",
      "epoch:181, train acc:1.0, test acc:0.7441\n",
      "epoch:182, train acc:1.0, test acc:0.7442\n",
      "epoch:183, train acc:1.0, test acc:0.7431\n",
      "epoch:184, train acc:1.0, test acc:0.7433\n",
      "epoch:185, train acc:1.0, test acc:0.7457\n",
      "epoch:186, train acc:1.0, test acc:0.7449\n",
      "epoch:187, train acc:1.0, test acc:0.7445\n",
      "epoch:188, train acc:1.0, test acc:0.7433\n",
      "epoch:189, train acc:1.0, test acc:0.744\n",
      "epoch:190, train acc:1.0, test acc:0.7446\n",
      "epoch:191, train acc:1.0, test acc:0.744\n",
      "epoch:192, train acc:1.0, test acc:0.7443\n",
      "epoch:193, train acc:1.0, test acc:0.7437\n",
      "epoch:194, train acc:1.0, test acc:0.743\n",
      "epoch:195, train acc:1.0, test acc:0.7434\n",
      "epoch:196, train acc:1.0, test acc:0.7432\n",
      "epoch:197, train acc:1.0, test acc:0.7433\n",
      "epoch:198, train acc:1.0, test acc:0.7439\n",
      "epoch:199, train acc:1.0, test acc:0.745\n",
      "epoch:200, train acc:1.0, test acc:0.7458\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과를 확인하면, training data의 accuracy가 거의 1.0(100%)에 도달한 반면에, test data의 accuracy는 이를 따라가지 못하는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 감소(weight decay)\n",
    "\n",
    ": 학습 과정(parameter update)에서 큰 가중치에 대해서는 그 양에 해당하는 큰 페널티를 부과하여 오버피팅을 억제하는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습의 목적인 loss fuction의 값을 줄이는 것이다. 이 때 loss fuction에 가중치의 **L2 norm**을 더하게 되면, loss fuction의 값을 줄이는 과정에서 가중치가 커지는 것을 억제할 수 있다. 각각의 loss function에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac { 1 }{ 2 } \\lambda { W }^{ 2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "을 더하여 가중치의 크기에 대한 페널티를 부여한다. $\\lambda$(lambda)는 정규화의 세기를 조절하는 hyperparameter이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, x, t, train_flg=False):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블 \n",
    "        \"\"\"\n",
    "        y = self.predict(x, train_flg)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)] # 각 hidden layer의 weight를 저장\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2) # 저장된 hidden layer의 weight들을 제곱하고, lambda를 곱한다.\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay # weight_decay를 loss함수에 더한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight decay를 사용하면 실제로 overfitting이 해소되는지 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay 설정\n",
    "weight_decay_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.10666666666666667, test acc:0.088\n",
      "epoch:1, train acc:0.11, test acc:0.0895\n",
      "epoch:2, train acc:0.10666666666666667, test acc:0.0916\n",
      "epoch:3, train acc:0.11333333333333333, test acc:0.0939\n",
      "epoch:4, train acc:0.12, test acc:0.0992\n",
      "epoch:5, train acc:0.12333333333333334, test acc:0.1025\n",
      "epoch:6, train acc:0.13333333333333333, test acc:0.1107\n",
      "epoch:7, train acc:0.14333333333333334, test acc:0.1164\n",
      "epoch:8, train acc:0.14333333333333334, test acc:0.1235\n",
      "epoch:9, train acc:0.16, test acc:0.1303\n",
      "epoch:10, train acc:0.17666666666666667, test acc:0.1353\n",
      "epoch:11, train acc:0.17666666666666667, test acc:0.1337\n",
      "epoch:12, train acc:0.18333333333333332, test acc:0.1341\n",
      "epoch:13, train acc:0.18333333333333332, test acc:0.1351\n",
      "epoch:14, train acc:0.20333333333333334, test acc:0.1427\n",
      "epoch:15, train acc:0.20666666666666667, test acc:0.15\n",
      "epoch:16, train acc:0.23, test acc:0.1603\n",
      "epoch:17, train acc:0.25666666666666665, test acc:0.1685\n",
      "epoch:18, train acc:0.26, test acc:0.168\n",
      "epoch:19, train acc:0.2733333333333333, test acc:0.1762\n",
      "epoch:20, train acc:0.31333333333333335, test acc:0.1952\n",
      "epoch:21, train acc:0.33, test acc:0.2065\n",
      "epoch:22, train acc:0.37, test acc:0.2244\n",
      "epoch:23, train acc:0.36666666666666664, test acc:0.2284\n",
      "epoch:24, train acc:0.3933333333333333, test acc:0.2431\n",
      "epoch:25, train acc:0.43, test acc:0.264\n",
      "epoch:26, train acc:0.43333333333333335, test acc:0.269\n",
      "epoch:27, train acc:0.45666666666666667, test acc:0.2872\n",
      "epoch:28, train acc:0.4666666666666667, test acc:0.3038\n",
      "epoch:29, train acc:0.4633333333333333, test acc:0.314\n",
      "epoch:30, train acc:0.4766666666666667, test acc:0.325\n",
      "epoch:31, train acc:0.47, test acc:0.3241\n",
      "epoch:32, train acc:0.47333333333333333, test acc:0.3304\n",
      "epoch:33, train acc:0.48, test acc:0.3399\n",
      "epoch:34, train acc:0.5, test acc:0.3494\n",
      "epoch:35, train acc:0.5, test acc:0.35\n",
      "epoch:36, train acc:0.51, test acc:0.3596\n",
      "epoch:37, train acc:0.5266666666666666, test acc:0.3748\n",
      "epoch:38, train acc:0.5433333333333333, test acc:0.3775\n",
      "epoch:39, train acc:0.55, test acc:0.38\n",
      "epoch:40, train acc:0.5633333333333334, test acc:0.4013\n",
      "epoch:41, train acc:0.5666666666666667, test acc:0.4028\n",
      "epoch:42, train acc:0.59, test acc:0.4204\n",
      "epoch:43, train acc:0.5933333333333334, test acc:0.4307\n",
      "epoch:44, train acc:0.59, test acc:0.4306\n",
      "epoch:45, train acc:0.5966666666666667, test acc:0.4368\n",
      "epoch:46, train acc:0.6133333333333333, test acc:0.4463\n",
      "epoch:47, train acc:0.61, test acc:0.4516\n",
      "epoch:48, train acc:0.6233333333333333, test acc:0.4516\n",
      "epoch:49, train acc:0.6333333333333333, test acc:0.4572\n",
      "epoch:50, train acc:0.63, test acc:0.4569\n",
      "epoch:51, train acc:0.6333333333333333, test acc:0.4652\n",
      "epoch:52, train acc:0.6433333333333333, test acc:0.4776\n",
      "epoch:53, train acc:0.6433333333333333, test acc:0.4781\n",
      "epoch:54, train acc:0.64, test acc:0.4771\n",
      "epoch:55, train acc:0.6566666666666666, test acc:0.4924\n",
      "epoch:56, train acc:0.6633333333333333, test acc:0.497\n",
      "epoch:57, train acc:0.66, test acc:0.4959\n",
      "epoch:58, train acc:0.6666666666666666, test acc:0.5063\n",
      "epoch:59, train acc:0.6666666666666666, test acc:0.5044\n",
      "epoch:60, train acc:0.6766666666666666, test acc:0.5082\n",
      "epoch:61, train acc:0.69, test acc:0.521\n",
      "epoch:62, train acc:0.69, test acc:0.5228\n",
      "epoch:63, train acc:0.68, test acc:0.5284\n",
      "epoch:64, train acc:0.6833333333333333, test acc:0.5256\n",
      "epoch:65, train acc:0.6866666666666666, test acc:0.5266\n",
      "epoch:66, train acc:0.6933333333333334, test acc:0.5325\n",
      "epoch:67, train acc:0.7066666666666667, test acc:0.544\n",
      "epoch:68, train acc:0.72, test acc:0.5467\n",
      "epoch:69, train acc:0.7066666666666667, test acc:0.5558\n",
      "epoch:70, train acc:0.7266666666666667, test acc:0.5632\n",
      "epoch:71, train acc:0.72, test acc:0.5567\n",
      "epoch:72, train acc:0.73, test acc:0.5692\n",
      "epoch:73, train acc:0.7166666666666667, test acc:0.5695\n",
      "epoch:74, train acc:0.7233333333333334, test acc:0.57\n",
      "epoch:75, train acc:0.7233333333333334, test acc:0.5701\n",
      "epoch:76, train acc:0.73, test acc:0.5786\n",
      "epoch:77, train acc:0.7433333333333333, test acc:0.5771\n",
      "epoch:78, train acc:0.7666666666666667, test acc:0.5822\n",
      "epoch:79, train acc:0.7366666666666667, test acc:0.5777\n",
      "epoch:80, train acc:0.7566666666666667, test acc:0.585\n",
      "epoch:81, train acc:0.7433333333333333, test acc:0.5732\n",
      "epoch:82, train acc:0.74, test acc:0.5697\n",
      "epoch:83, train acc:0.74, test acc:0.5782\n",
      "epoch:84, train acc:0.7633333333333333, test acc:0.5942\n",
      "epoch:85, train acc:0.7633333333333333, test acc:0.6018\n",
      "epoch:86, train acc:0.7566666666666667, test acc:0.5967\n",
      "epoch:87, train acc:0.7533333333333333, test acc:0.5875\n",
      "epoch:88, train acc:0.76, test acc:0.5901\n",
      "epoch:89, train acc:0.76, test acc:0.5783\n",
      "epoch:90, train acc:0.7633333333333333, test acc:0.5965\n",
      "epoch:91, train acc:0.78, test acc:0.6087\n",
      "epoch:92, train acc:0.77, test acc:0.5997\n",
      "epoch:93, train acc:0.7633333333333333, test acc:0.6067\n",
      "epoch:94, train acc:0.77, test acc:0.6052\n",
      "epoch:95, train acc:0.7666666666666667, test acc:0.6001\n",
      "epoch:96, train acc:0.78, test acc:0.5978\n",
      "epoch:97, train acc:0.7633333333333333, test acc:0.5923\n",
      "epoch:98, train acc:0.77, test acc:0.5906\n",
      "epoch:99, train acc:0.7666666666666667, test acc:0.6054\n",
      "epoch:100, train acc:0.77, test acc:0.5892\n",
      "epoch:101, train acc:0.79, test acc:0.6079\n",
      "epoch:102, train acc:0.7733333333333333, test acc:0.5864\n",
      "epoch:103, train acc:0.8, test acc:0.625\n",
      "epoch:104, train acc:0.79, test acc:0.5995\n",
      "epoch:105, train acc:0.8033333333333333, test acc:0.6204\n",
      "epoch:106, train acc:0.8066666666666666, test acc:0.6238\n",
      "epoch:107, train acc:0.7966666666666666, test acc:0.6248\n",
      "epoch:108, train acc:0.79, test acc:0.6085\n",
      "epoch:109, train acc:0.7833333333333333, test acc:0.6202\n",
      "epoch:110, train acc:0.7766666666666666, test acc:0.6023\n",
      "epoch:111, train acc:0.7866666666666666, test acc:0.617\n",
      "epoch:112, train acc:0.7933333333333333, test acc:0.6131\n",
      "epoch:113, train acc:0.79, test acc:0.6133\n",
      "epoch:114, train acc:0.8133333333333334, test acc:0.6375\n",
      "epoch:115, train acc:0.8, test acc:0.6345\n",
      "epoch:116, train acc:0.79, test acc:0.6302\n",
      "epoch:117, train acc:0.8, test acc:0.6283\n",
      "epoch:118, train acc:0.8066666666666666, test acc:0.6298\n",
      "epoch:119, train acc:0.8133333333333334, test acc:0.6385\n",
      "epoch:120, train acc:0.81, test acc:0.6433\n",
      "epoch:121, train acc:0.81, test acc:0.6376\n",
      "epoch:122, train acc:0.8133333333333334, test acc:0.6376\n",
      "epoch:123, train acc:0.82, test acc:0.624\n",
      "epoch:124, train acc:0.8133333333333334, test acc:0.6478\n",
      "epoch:125, train acc:0.82, test acc:0.639\n",
      "epoch:126, train acc:0.8066666666666666, test acc:0.6394\n",
      "epoch:127, train acc:0.7933333333333333, test acc:0.6199\n",
      "epoch:128, train acc:0.8033333333333333, test acc:0.6186\n",
      "epoch:129, train acc:0.8033333333333333, test acc:0.6269\n",
      "epoch:130, train acc:0.81, test acc:0.6401\n",
      "epoch:131, train acc:0.8033333333333333, test acc:0.6368\n",
      "epoch:132, train acc:0.81, test acc:0.6362\n",
      "epoch:133, train acc:0.81, test acc:0.647\n",
      "epoch:134, train acc:0.8233333333333334, test acc:0.6593\n",
      "epoch:135, train acc:0.8, test acc:0.6407\n",
      "epoch:136, train acc:0.8033333333333333, test acc:0.634\n",
      "epoch:137, train acc:0.81, test acc:0.6438\n",
      "epoch:138, train acc:0.8, test acc:0.6448\n",
      "epoch:139, train acc:0.82, test acc:0.6532\n",
      "epoch:140, train acc:0.8166666666666667, test acc:0.6571\n",
      "epoch:141, train acc:0.82, test acc:0.6488\n",
      "epoch:142, train acc:0.8033333333333333, test acc:0.6499\n",
      "epoch:143, train acc:0.8, test acc:0.6506\n",
      "epoch:144, train acc:0.83, test acc:0.6589\n",
      "epoch:145, train acc:0.81, test acc:0.645\n",
      "epoch:146, train acc:0.8333333333333334, test acc:0.6591\n",
      "epoch:147, train acc:0.8033333333333333, test acc:0.6535\n",
      "epoch:148, train acc:0.8066666666666666, test acc:0.6438\n",
      "epoch:149, train acc:0.8033333333333333, test acc:0.6527\n",
      "epoch:150, train acc:0.81, test acc:0.6504\n",
      "epoch:151, train acc:0.7966666666666666, test acc:0.6541\n",
      "epoch:152, train acc:0.8066666666666666, test acc:0.6436\n",
      "epoch:153, train acc:0.8033333333333333, test acc:0.6405\n",
      "epoch:154, train acc:0.8166666666666667, test acc:0.6537\n",
      "epoch:155, train acc:0.8066666666666666, test acc:0.6404\n",
      "epoch:156, train acc:0.81, test acc:0.6462\n",
      "epoch:157, train acc:0.8, test acc:0.6528\n",
      "epoch:158, train acc:0.8, test acc:0.6472\n",
      "epoch:159, train acc:0.8066666666666666, test acc:0.6523\n",
      "epoch:160, train acc:0.8166666666666667, test acc:0.6588\n",
      "epoch:161, train acc:0.82, test acc:0.6569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:162, train acc:0.81, test acc:0.6596\n",
      "epoch:163, train acc:0.81, test acc:0.6511\n",
      "epoch:164, train acc:0.8133333333333334, test acc:0.6448\n",
      "epoch:165, train acc:0.82, test acc:0.6522\n",
      "epoch:166, train acc:0.8066666666666666, test acc:0.6525\n",
      "epoch:167, train acc:0.7933333333333333, test acc:0.6582\n",
      "epoch:168, train acc:0.8166666666666667, test acc:0.6648\n",
      "epoch:169, train acc:0.8133333333333334, test acc:0.6674\n",
      "epoch:170, train acc:0.8133333333333334, test acc:0.658\n",
      "epoch:171, train acc:0.8133333333333334, test acc:0.6588\n",
      "epoch:172, train acc:0.8166666666666667, test acc:0.6664\n",
      "epoch:173, train acc:0.8133333333333334, test acc:0.6559\n",
      "epoch:174, train acc:0.81, test acc:0.6586\n",
      "epoch:175, train acc:0.8033333333333333, test acc:0.652\n",
      "epoch:176, train acc:0.8033333333333333, test acc:0.6519\n",
      "epoch:177, train acc:0.81, test acc:0.645\n",
      "epoch:178, train acc:0.8133333333333334, test acc:0.6535\n",
      "epoch:179, train acc:0.8233333333333334, test acc:0.6627\n",
      "epoch:180, train acc:0.8, test acc:0.6572\n",
      "epoch:181, train acc:0.8133333333333334, test acc:0.6553\n",
      "epoch:182, train acc:0.81, test acc:0.6522\n",
      "epoch:183, train acc:0.8166666666666667, test acc:0.6556\n",
      "epoch:184, train acc:0.8133333333333334, test acc:0.6503\n",
      "epoch:185, train acc:0.8133333333333334, test acc:0.6499\n",
      "epoch:186, train acc:0.8066666666666666, test acc:0.6568\n",
      "epoch:187, train acc:0.8, test acc:0.6552\n",
      "epoch:188, train acc:0.8066666666666666, test acc:0.656\n",
      "epoch:189, train acc:0.7966666666666666, test acc:0.6643\n",
      "epoch:190, train acc:0.8033333333333333, test acc:0.66\n",
      "epoch:191, train acc:0.8066666666666666, test acc:0.6562\n",
      "epoch:192, train acc:0.8066666666666666, test acc:0.6468\n",
      "epoch:193, train acc:0.82, test acc:0.6496\n",
      "epoch:194, train acc:0.8, test acc:0.6517\n",
      "epoch:195, train acc:0.8, test acc:0.6448\n",
      "epoch:196, train acc:0.82, test acc:0.6528\n",
      "epoch:197, train acc:0.8233333333333334, test acc:0.6543\n",
      "epoch:198, train acc:0.8233333333333334, test acc:0.6674\n",
      "epoch:199, train acc:0.8366666666666667, test acc:0.6646\n",
      "epoch:200, train acc:0.8133333333333334, test acc:0.6688\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과를 확인하면, training data와 test data에 대한 정확도의 차이가 줄어든 것을 확인할 수 있고, training data의 accuracy가 완전히 1.0(100%)에 도달하지 못한 점을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드롭 아웃(Dropout)\n",
    ": 뉴런을 임의로 삭제하면서 학습하는 방법. Training 때, hidden layer의 뉴런을 무작위로 골라 삭제하면서 학습한다. 즉 각 출력에서 일정한 부위에서의 신호의 전달을 차단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://i.imgur.com/Hin1BWM.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio \n",
    "            # hidden layer와 크기가 같은 배열을 무작위로 생성하여, 그 값이 dropout_ratio보다 작은 node는 False로 설정하여 drop시킨다.\n",
    "            return x * self.mask \n",
    "            # self.mask와 input을 곱하면, drop된 node는 계산에 사용되지 않는다.\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout을 사용하면 실제로 overfitting이 해소되는지 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True\n",
    "dropout_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.323762013080246\n",
      "=== epoch:1, train acc:0.08333333333333333, test acc:0.1053 ===\n",
      "train loss:2.3023489060211055\n",
      "train loss:2.300578945424215\n",
      "train loss:2.290620596242969\n",
      "=== epoch:2, train acc:0.08, test acc:0.1066 ===\n",
      "train loss:2.308906470010802\n",
      "train loss:2.302558282621394\n",
      "train loss:2.3182360811845997\n",
      "=== epoch:3, train acc:0.08, test acc:0.1066 ===\n",
      "train loss:2.299709750834534\n",
      "train loss:2.3098915451925692\n",
      "train loss:2.2832396593108215\n",
      "=== epoch:4, train acc:0.07333333333333333, test acc:0.1073 ===\n",
      "train loss:2.2984168277486265\n",
      "train loss:2.2997288456816314\n",
      "train loss:2.3056230724158997\n",
      "=== epoch:5, train acc:0.07666666666666666, test acc:0.1088 ===\n",
      "train loss:2.3055779950557236\n",
      "train loss:2.3073174576552504\n",
      "train loss:2.2943257117744427\n",
      "=== epoch:6, train acc:0.07666666666666666, test acc:0.1087 ===\n",
      "train loss:2.321798212177259\n",
      "train loss:2.2977232061213457\n",
      "train loss:2.313796115032889\n",
      "=== epoch:7, train acc:0.07333333333333333, test acc:0.1098 ===\n",
      "train loss:2.2852595997705327\n",
      "train loss:2.2949464997638853\n",
      "train loss:2.2929159912966277\n",
      "=== epoch:8, train acc:0.09333333333333334, test acc:0.1109 ===\n",
      "train loss:2.277053338867957\n",
      "train loss:2.2903808948027606\n",
      "train loss:2.290258974884191\n",
      "=== epoch:9, train acc:0.1, test acc:0.1124 ===\n",
      "train loss:2.29471379430931\n",
      "train loss:2.289424727837664\n",
      "train loss:2.2870586167616183\n",
      "=== epoch:10, train acc:0.11, test acc:0.1143 ===\n",
      "train loss:2.2664896220093094\n",
      "train loss:2.2783121014863075\n",
      "train loss:2.288414473680052\n",
      "=== epoch:11, train acc:0.13, test acc:0.1134 ===\n",
      "train loss:2.270997112413911\n",
      "train loss:2.285885770121273\n",
      "train loss:2.3013253230663007\n",
      "=== epoch:12, train acc:0.13, test acc:0.1138 ===\n",
      "train loss:2.2889563781654543\n",
      "train loss:2.296696191845365\n",
      "train loss:2.292693666492414\n",
      "=== epoch:13, train acc:0.12666666666666668, test acc:0.1144 ===\n",
      "train loss:2.2827365690972745\n",
      "train loss:2.2791413209213855\n",
      "train loss:2.268199835661044\n",
      "=== epoch:14, train acc:0.13333333333333333, test acc:0.1172 ===\n",
      "train loss:2.2954828365495747\n",
      "train loss:2.271741144221401\n",
      "train loss:2.2860835767792596\n",
      "=== epoch:15, train acc:0.13, test acc:0.1177 ===\n",
      "train loss:2.2808151687942813\n",
      "train loss:2.285361997276842\n",
      "train loss:2.282004207858121\n",
      "=== epoch:16, train acc:0.12666666666666668, test acc:0.1183 ===\n",
      "train loss:2.2855888608720156\n",
      "train loss:2.271415508273723\n",
      "train loss:2.28030812562085\n",
      "=== epoch:17, train acc:0.13666666666666666, test acc:0.1203 ===\n",
      "train loss:2.2831827392767496\n",
      "train loss:2.2615756279244574\n",
      "train loss:2.2682401306877007\n",
      "=== epoch:18, train acc:0.13666666666666666, test acc:0.1226 ===\n",
      "train loss:2.2835582644729935\n",
      "train loss:2.27398024354344\n",
      "train loss:2.276726011479538\n",
      "=== epoch:19, train acc:0.13666666666666666, test acc:0.1246 ===\n",
      "train loss:2.2664225925559154\n",
      "train loss:2.277087750278082\n",
      "train loss:2.2759887460342454\n",
      "=== epoch:20, train acc:0.13666666666666666, test acc:0.1266 ===\n",
      "train loss:2.264999758496735\n",
      "train loss:2.2765937602102984\n",
      "train loss:2.268729931200646\n",
      "=== epoch:21, train acc:0.14333333333333334, test acc:0.1282 ===\n",
      "train loss:2.2724096047448676\n",
      "train loss:2.247587104686502\n",
      "train loss:2.261237778739542\n",
      "=== epoch:22, train acc:0.14333333333333334, test acc:0.1273 ===\n",
      "train loss:2.2553981755611483\n",
      "train loss:2.26658888781275\n",
      "train loss:2.276657073784049\n",
      "=== epoch:23, train acc:0.14333333333333334, test acc:0.1294 ===\n",
      "train loss:2.2585002621639054\n",
      "train loss:2.2628464048420596\n",
      "train loss:2.2693184679217935\n",
      "=== epoch:24, train acc:0.15666666666666668, test acc:0.1292 ===\n",
      "train loss:2.265183400068665\n",
      "train loss:2.2663900315502503\n",
      "train loss:2.259432289089645\n",
      "=== epoch:25, train acc:0.16, test acc:0.1338 ===\n",
      "train loss:2.2454337683238186\n",
      "train loss:2.259229751711314\n",
      "train loss:2.2541637042539318\n",
      "=== epoch:26, train acc:0.16, test acc:0.1352 ===\n",
      "train loss:2.2645669406324167\n",
      "train loss:2.2485137217935587\n",
      "train loss:2.2631663811970233\n",
      "=== epoch:27, train acc:0.15666666666666668, test acc:0.1389 ===\n",
      "train loss:2.2728350913255637\n",
      "train loss:2.271127862488287\n",
      "train loss:2.269245173662436\n",
      "=== epoch:28, train acc:0.16, test acc:0.1449 ===\n",
      "train loss:2.2460936999138026\n",
      "train loss:2.2483498145632947\n",
      "train loss:2.250621762030195\n",
      "=== epoch:29, train acc:0.16666666666666666, test acc:0.148 ===\n",
      "train loss:2.260964515611002\n",
      "train loss:2.257791355844408\n",
      "train loss:2.258248652746072\n",
      "=== epoch:30, train acc:0.17666666666666667, test acc:0.1495 ===\n",
      "train loss:2.237758030319733\n",
      "train loss:2.248614861690319\n",
      "train loss:2.262991501963403\n",
      "=== epoch:31, train acc:0.18, test acc:0.1494 ===\n",
      "train loss:2.2430347219689333\n",
      "train loss:2.238658430152335\n",
      "train loss:2.2562703098625145\n",
      "=== epoch:32, train acc:0.17, test acc:0.1481 ===\n",
      "train loss:2.2225445183334744\n",
      "train loss:2.2519938574985794\n",
      "train loss:2.2427126513643296\n",
      "=== epoch:33, train acc:0.17, test acc:0.1504 ===\n",
      "train loss:2.26576010065906\n",
      "train loss:2.25207752901998\n",
      "train loss:2.2307687434285373\n",
      "=== epoch:34, train acc:0.17666666666666667, test acc:0.153 ===\n",
      "train loss:2.237563386595261\n",
      "train loss:2.239689078862422\n",
      "train loss:2.2553591502474752\n",
      "=== epoch:35, train acc:0.17333333333333334, test acc:0.1581 ===\n",
      "train loss:2.2608387809096335\n",
      "train loss:2.2144362958207484\n",
      "train loss:2.2326818913431867\n",
      "=== epoch:36, train acc:0.17, test acc:0.1511 ===\n",
      "train loss:2.243615779295189\n",
      "train loss:2.233135118949343\n",
      "train loss:2.244672068302829\n",
      "=== epoch:37, train acc:0.17, test acc:0.1535 ===\n",
      "train loss:2.2317512927379366\n",
      "train loss:2.239798912654366\n",
      "train loss:2.2295894334447603\n",
      "=== epoch:38, train acc:0.17333333333333334, test acc:0.1567 ===\n",
      "train loss:2.246503243572757\n",
      "train loss:2.242597931297574\n",
      "train loss:2.2434399004553662\n",
      "=== epoch:39, train acc:0.17666666666666667, test acc:0.1606 ===\n",
      "train loss:2.237849977343787\n",
      "train loss:2.24157734865977\n",
      "train loss:2.2377405356346713\n",
      "=== epoch:40, train acc:0.18666666666666668, test acc:0.166 ===\n",
      "train loss:2.2025667931419495\n",
      "train loss:2.243767759570782\n",
      "train loss:2.205290328947572\n",
      "=== epoch:41, train acc:0.17333333333333334, test acc:0.159 ===\n",
      "train loss:2.2433263615929553\n",
      "train loss:2.230941909100578\n",
      "train loss:2.2263850137759804\n",
      "=== epoch:42, train acc:0.16333333333333333, test acc:0.1576 ===\n",
      "train loss:2.2089117383336885\n",
      "train loss:2.240309372813452\n",
      "train loss:2.1973442824039022\n",
      "=== epoch:43, train acc:0.16333333333333333, test acc:0.1528 ===\n",
      "train loss:2.242999051527647\n",
      "train loss:2.217284856233435\n",
      "train loss:2.228147383773105\n",
      "=== epoch:44, train acc:0.16666666666666666, test acc:0.1564 ===\n",
      "train loss:2.2332599330719636\n",
      "train loss:2.247417646184372\n",
      "train loss:2.2654428732368523\n",
      "=== epoch:45, train acc:0.17, test acc:0.1591 ===\n",
      "train loss:2.248016373393202\n",
      "train loss:2.2115192888021498\n",
      "train loss:2.239128421079711\n",
      "=== epoch:46, train acc:0.17, test acc:0.1586 ===\n",
      "train loss:2.238768934428174\n",
      "train loss:2.23379716551686\n",
      "train loss:2.2308495600731337\n",
      "=== epoch:47, train acc:0.18666666666666668, test acc:0.1728 ===\n",
      "train loss:2.234884997790748\n",
      "train loss:2.239902196524872\n",
      "train loss:2.2252573112663763\n",
      "=== epoch:48, train acc:0.19, test acc:0.1756 ===\n",
      "train loss:2.224621511798556\n",
      "train loss:2.2066220583629996\n",
      "train loss:2.2211974150153284\n",
      "=== epoch:49, train acc:0.20333333333333334, test acc:0.1783 ===\n",
      "train loss:2.250171817010584\n",
      "train loss:2.2335772182287017\n",
      "train loss:2.2204158874557596\n",
      "=== epoch:50, train acc:0.20333333333333334, test acc:0.1791 ===\n",
      "train loss:2.222985574176256\n",
      "train loss:2.216742486155611\n",
      "train loss:2.2058358311327373\n",
      "=== epoch:51, train acc:0.21, test acc:0.1815 ===\n",
      "train loss:2.257061312623644\n",
      "train loss:2.2322259331144796\n",
      "train loss:2.226227146377827\n",
      "=== epoch:52, train acc:0.23333333333333334, test acc:0.1936 ===\n",
      "train loss:2.2334501652119814\n",
      "train loss:2.2392913154247807\n",
      "train loss:2.215037663537887\n",
      "=== epoch:53, train acc:0.22666666666666666, test acc:0.1984 ===\n",
      "train loss:2.228022574384381\n",
      "train loss:2.2177842935579255\n",
      "train loss:2.221302977713155\n",
      "=== epoch:54, train acc:0.23666666666666666, test acc:0.2043 ===\n",
      "train loss:2.2125246327428894\n",
      "train loss:2.2219217660187045\n",
      "train loss:2.243683504856995\n",
      "=== epoch:55, train acc:0.23, test acc:0.2014 ===\n",
      "train loss:2.2109935667912346\n",
      "train loss:2.1914996699217766\n",
      "train loss:2.2336778568246167\n",
      "=== epoch:56, train acc:0.23666666666666666, test acc:0.2038 ===\n",
      "train loss:2.2086610656644323\n",
      "train loss:2.2236754694881493\n",
      "train loss:2.203500021169893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.24333333333333335, test acc:0.2111 ===\n",
      "train loss:2.216889929321017\n",
      "train loss:2.2086282475006005\n",
      "train loss:2.2238839694479573\n",
      "=== epoch:58, train acc:0.26, test acc:0.2213 ===\n",
      "train loss:2.2283343042674377\n",
      "train loss:2.1982151869259074\n",
      "train loss:2.203875091667029\n",
      "=== epoch:59, train acc:0.26, test acc:0.2262 ===\n",
      "train loss:2.206117280994626\n",
      "train loss:2.200395882594824\n",
      "train loss:2.2141819910504217\n",
      "=== epoch:60, train acc:0.2633333333333333, test acc:0.2295 ===\n",
      "train loss:2.2184994003301757\n",
      "train loss:2.2023963035207283\n",
      "train loss:2.2134967473653147\n",
      "=== epoch:61, train acc:0.2633333333333333, test acc:0.2371 ===\n",
      "train loss:2.2028278187936228\n",
      "train loss:2.2046555083059323\n",
      "train loss:2.2144059057425727\n",
      "=== epoch:62, train acc:0.2633333333333333, test acc:0.2362 ===\n",
      "train loss:2.205295550292366\n",
      "train loss:2.1848412448534953\n",
      "train loss:2.228420590081786\n",
      "=== epoch:63, train acc:0.26, test acc:0.2325 ===\n",
      "train loss:2.2046308671353416\n",
      "train loss:2.169792911194348\n",
      "train loss:2.1995247241558507\n",
      "=== epoch:64, train acc:0.25666666666666665, test acc:0.2318 ===\n",
      "train loss:2.215844003198487\n",
      "train loss:2.1907572190295186\n",
      "train loss:2.216794707230714\n",
      "=== epoch:65, train acc:0.26, test acc:0.2352 ===\n",
      "train loss:2.174324632871855\n",
      "train loss:2.216072466622841\n",
      "train loss:2.2093735722747883\n",
      "=== epoch:66, train acc:0.25666666666666665, test acc:0.2361 ===\n",
      "train loss:2.211295522226978\n",
      "train loss:2.2014028261313503\n",
      "train loss:2.221659888002846\n",
      "=== epoch:67, train acc:0.25333333333333335, test acc:0.2322 ===\n",
      "train loss:2.201428710378417\n",
      "train loss:2.1819422116529683\n",
      "train loss:2.1791753954146365\n",
      "=== epoch:68, train acc:0.25333333333333335, test acc:0.2298 ===\n",
      "train loss:2.175286604898966\n",
      "train loss:2.1879944556877806\n",
      "train loss:2.1729112987797414\n",
      "=== epoch:69, train acc:0.25333333333333335, test acc:0.2279 ===\n",
      "train loss:2.21579167245404\n",
      "train loss:2.16695737454717\n",
      "train loss:2.1768259340643827\n",
      "=== epoch:70, train acc:0.25666666666666665, test acc:0.2271 ===\n",
      "train loss:2.177077564230375\n",
      "train loss:2.192535637908571\n",
      "train loss:2.1301643234141965\n",
      "=== epoch:71, train acc:0.25333333333333335, test acc:0.2316 ===\n",
      "train loss:2.1969601667231955\n",
      "train loss:2.2091807114250415\n",
      "train loss:2.1986851040375304\n",
      "=== epoch:72, train acc:0.25333333333333335, test acc:0.2364 ===\n",
      "train loss:2.174899354187109\n",
      "train loss:2.1781897389845057\n",
      "train loss:2.2001033228781237\n",
      "=== epoch:73, train acc:0.25333333333333335, test acc:0.2358 ===\n",
      "train loss:2.195311106295871\n",
      "train loss:2.190434669350376\n",
      "train loss:2.189373625551839\n",
      "=== epoch:74, train acc:0.25333333333333335, test acc:0.2424 ===\n",
      "train loss:2.214162296413584\n",
      "train loss:2.137156800196252\n",
      "train loss:2.1842515889308127\n",
      "=== epoch:75, train acc:0.26, test acc:0.2463 ===\n",
      "train loss:2.1971790413709305\n",
      "train loss:2.1350099012109283\n",
      "train loss:2.230975921707367\n",
      "=== epoch:76, train acc:0.27, test acc:0.249 ===\n",
      "train loss:2.1160224274471453\n",
      "train loss:2.191801956536484\n",
      "train loss:2.1698079748239953\n",
      "=== epoch:77, train acc:0.26666666666666666, test acc:0.2482 ===\n",
      "train loss:2.184821328378053\n",
      "train loss:2.174962297607837\n",
      "train loss:2.1803263977575575\n",
      "=== epoch:78, train acc:0.27, test acc:0.2502 ===\n",
      "train loss:2.168943308986227\n",
      "train loss:2.208236665450332\n",
      "train loss:2.140028927332502\n",
      "=== epoch:79, train acc:0.2733333333333333, test acc:0.2552 ===\n",
      "train loss:2.1476607723488312\n",
      "train loss:2.180592130278976\n",
      "train loss:2.145034409226811\n",
      "=== epoch:80, train acc:0.2733333333333333, test acc:0.2537 ===\n",
      "train loss:2.187223040315299\n",
      "train loss:2.1436402204498175\n",
      "train loss:2.190955358831241\n",
      "=== epoch:81, train acc:0.27666666666666667, test acc:0.2558 ===\n",
      "train loss:2.152028586353035\n",
      "train loss:2.1543184352966716\n",
      "train loss:2.2001314013332642\n",
      "=== epoch:82, train acc:0.2833333333333333, test acc:0.2586 ===\n",
      "train loss:2.136553861267004\n",
      "train loss:2.165658159044263\n",
      "train loss:2.1550964350200648\n",
      "=== epoch:83, train acc:0.2866666666666667, test acc:0.2593 ===\n",
      "train loss:2.143475596026132\n",
      "train loss:2.1338208918573254\n",
      "train loss:2.148026017942766\n",
      "=== epoch:84, train acc:0.29, test acc:0.2625 ===\n",
      "train loss:2.142481149030072\n",
      "train loss:2.1220353846823126\n",
      "train loss:2.127061766802514\n",
      "=== epoch:85, train acc:0.29333333333333333, test acc:0.2648 ===\n",
      "train loss:2.181008043028992\n",
      "train loss:2.1454387107524924\n",
      "train loss:2.141857067776384\n",
      "=== epoch:86, train acc:0.31333333333333335, test acc:0.2712 ===\n",
      "train loss:2.1906668744368516\n",
      "train loss:2.101130622461229\n",
      "train loss:2.1190918104761143\n",
      "=== epoch:87, train acc:0.30333333333333334, test acc:0.2707 ===\n",
      "train loss:2.0995762942436635\n",
      "train loss:2.1627829368005367\n",
      "train loss:2.1668548296738885\n",
      "=== epoch:88, train acc:0.32, test acc:0.2749 ===\n",
      "train loss:2.167252734607275\n",
      "train loss:2.167616863558048\n",
      "train loss:2.15573867127482\n",
      "=== epoch:89, train acc:0.32, test acc:0.2772 ===\n",
      "train loss:2.1213236482517304\n",
      "train loss:2.1266588189812166\n",
      "train loss:2.1532941108752883\n",
      "=== epoch:90, train acc:0.32, test acc:0.276 ===\n",
      "train loss:2.133056516446327\n",
      "train loss:2.142147855320498\n",
      "train loss:2.115425746875731\n",
      "=== epoch:91, train acc:0.32, test acc:0.2756 ===\n",
      "train loss:2.15044679930436\n",
      "train loss:2.137830176827581\n",
      "train loss:2.1407155362324395\n",
      "=== epoch:92, train acc:0.3233333333333333, test acc:0.2764 ===\n",
      "train loss:2.1111349244690194\n",
      "train loss:2.138703978915751\n",
      "train loss:2.0994189862977577\n",
      "=== epoch:93, train acc:0.32666666666666666, test acc:0.2787 ===\n",
      "train loss:2.1212719951585077\n",
      "train loss:2.1237478036247817\n",
      "train loss:2.1301998866825596\n",
      "=== epoch:94, train acc:0.33, test acc:0.2813 ===\n",
      "train loss:2.068533155099322\n",
      "train loss:2.1029938834348707\n",
      "train loss:2.0705433233114956\n",
      "=== epoch:95, train acc:0.32666666666666666, test acc:0.2802 ===\n",
      "train loss:2.0906678154544323\n",
      "train loss:2.107764528339014\n",
      "train loss:2.0635713399055615\n",
      "=== epoch:96, train acc:0.32666666666666666, test acc:0.2805 ===\n",
      "train loss:2.1525776886479533\n",
      "train loss:2.087843409560829\n",
      "train loss:2.1679949201879256\n",
      "=== epoch:97, train acc:0.3333333333333333, test acc:0.2824 ===\n",
      "train loss:2.107007130710855\n",
      "train loss:2.0721600880757776\n",
      "train loss:2.105354434693222\n",
      "=== epoch:98, train acc:0.3333333333333333, test acc:0.2831 ===\n",
      "train loss:2.127124127922697\n",
      "train loss:2.0981819489272264\n",
      "train loss:2.110550946105038\n",
      "=== epoch:99, train acc:0.33666666666666667, test acc:0.2848 ===\n",
      "train loss:2.1107945913879913\n",
      "train loss:2.093688956885328\n",
      "train loss:2.089153850776099\n",
      "=== epoch:100, train acc:0.34, test acc:0.2891 ===\n",
      "train loss:2.1033679834527548\n",
      "train loss:2.134132211381711\n",
      "train loss:2.1147992009900047\n",
      "=== epoch:101, train acc:0.33666666666666667, test acc:0.2899 ===\n",
      "train loss:2.1063941712784353\n",
      "train loss:2.1075537341953354\n",
      "train loss:2.076789256282423\n",
      "=== epoch:102, train acc:0.34, test acc:0.2905 ===\n",
      "train loss:2.107198641249049\n",
      "train loss:2.09308563028082\n",
      "train loss:2.1388213962568643\n",
      "=== epoch:103, train acc:0.34, test acc:0.2899 ===\n",
      "train loss:2.048986387466713\n",
      "train loss:2.025765493925874\n",
      "train loss:2.1185439393145797\n",
      "=== epoch:104, train acc:0.34, test acc:0.2896 ===\n",
      "train loss:2.113544240901026\n",
      "train loss:2.085626161959653\n",
      "train loss:2.12604777800444\n",
      "=== epoch:105, train acc:0.34, test acc:0.29 ===\n",
      "train loss:2.110209854132828\n",
      "train loss:2.1029749149544577\n",
      "train loss:2.0427337717871197\n",
      "=== epoch:106, train acc:0.3466666666666667, test acc:0.29 ===\n",
      "train loss:2.073973633842721\n",
      "train loss:2.0834859815233564\n",
      "train loss:2.0264402256613434\n",
      "=== epoch:107, train acc:0.3433333333333333, test acc:0.2898 ===\n",
      "train loss:2.09035478200603\n",
      "train loss:2.0311606687424257\n",
      "train loss:2.0652229055367477\n",
      "=== epoch:108, train acc:0.3466666666666667, test acc:0.2888 ===\n",
      "train loss:2.1163190900219773\n",
      "train loss:2.0580638723505635\n",
      "train loss:2.0722689468998206\n",
      "=== epoch:109, train acc:0.35, test acc:0.2922 ===\n",
      "train loss:2.0171069732611087\n",
      "train loss:2.0565936180823945\n",
      "train loss:2.0548308201349155\n",
      "=== epoch:110, train acc:0.35333333333333333, test acc:0.2917 ===\n",
      "train loss:2.054573677582016\n",
      "train loss:2.0277432312892367\n",
      "train loss:2.020852745105398\n",
      "=== epoch:111, train acc:0.35, test acc:0.2918 ===\n",
      "train loss:2.028860932582868\n",
      "train loss:2.000499731694768\n",
      "train loss:2.1078181371590246\n",
      "=== epoch:112, train acc:0.35333333333333333, test acc:0.2923 ===\n",
      "train loss:1.975368665946856\n",
      "train loss:2.0978870316706875\n",
      "train loss:2.108176482172588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:113, train acc:0.3566666666666667, test acc:0.2955 ===\n",
      "train loss:2.0334255709305653\n",
      "train loss:2.065040417844468\n",
      "train loss:2.0061054524754893\n",
      "=== epoch:114, train acc:0.35333333333333333, test acc:0.2943 ===\n",
      "train loss:2.015009492255508\n",
      "train loss:1.995506686764369\n",
      "train loss:2.0086473427107587\n",
      "=== epoch:115, train acc:0.35333333333333333, test acc:0.2947 ===\n",
      "train loss:1.9888549175106423\n",
      "train loss:1.970552999098839\n",
      "train loss:2.0834979267589557\n",
      "=== epoch:116, train acc:0.35, test acc:0.2942 ===\n",
      "train loss:2.0683297639917217\n",
      "train loss:2.026720170657071\n",
      "train loss:2.0101382365541247\n",
      "=== epoch:117, train acc:0.35, test acc:0.2958 ===\n",
      "train loss:1.9909826300526767\n",
      "train loss:2.075430398503011\n",
      "train loss:2.1142092496683347\n",
      "=== epoch:118, train acc:0.3566666666666667, test acc:0.298 ===\n",
      "train loss:1.9626703646860133\n",
      "train loss:2.071719095953297\n",
      "train loss:1.982861304220828\n",
      "=== epoch:119, train acc:0.3566666666666667, test acc:0.2986 ===\n",
      "train loss:1.927284647676846\n",
      "train loss:2.0427484344650906\n",
      "train loss:2.0131015342564638\n",
      "=== epoch:120, train acc:0.3566666666666667, test acc:0.2994 ===\n",
      "train loss:1.8371956323126886\n",
      "train loss:1.9826523682806565\n",
      "train loss:1.969171420672351\n",
      "=== epoch:121, train acc:0.36, test acc:0.2983 ===\n",
      "train loss:1.9743284270369597\n",
      "train loss:2.0312794191895174\n",
      "train loss:2.051949474978884\n",
      "=== epoch:122, train acc:0.36, test acc:0.2998 ===\n",
      "train loss:1.979414897821287\n",
      "train loss:1.967077918271187\n",
      "train loss:1.9177274608028707\n",
      "=== epoch:123, train acc:0.36, test acc:0.2984 ===\n",
      "train loss:2.047738085455229\n",
      "train loss:2.043334584055701\n",
      "train loss:1.9993152358266153\n",
      "=== epoch:124, train acc:0.3566666666666667, test acc:0.3008 ===\n",
      "train loss:1.9941023416139236\n",
      "train loss:1.9929135928329647\n",
      "train loss:2.0140135353341693\n",
      "=== epoch:125, train acc:0.36333333333333334, test acc:0.3023 ===\n",
      "train loss:1.9465693564217665\n",
      "train loss:2.033825965056332\n",
      "train loss:2.018374099156751\n",
      "=== epoch:126, train acc:0.36, test acc:0.3031 ===\n",
      "train loss:2.0096251622083687\n",
      "train loss:2.0067353405058497\n",
      "train loss:1.953907822558243\n",
      "=== epoch:127, train acc:0.3566666666666667, test acc:0.3031 ===\n",
      "train loss:1.918789862665399\n",
      "train loss:1.8847963927150448\n",
      "train loss:2.035948048648824\n",
      "=== epoch:128, train acc:0.36, test acc:0.3029 ===\n",
      "train loss:1.8820973938217265\n",
      "train loss:1.9651314765275396\n",
      "train loss:1.962007318191852\n",
      "=== epoch:129, train acc:0.36333333333333334, test acc:0.3026 ===\n",
      "train loss:1.9298165561754288\n",
      "train loss:1.9979793858564443\n",
      "train loss:1.9673839796971577\n",
      "=== epoch:130, train acc:0.36666666666666664, test acc:0.3037 ===\n",
      "train loss:1.9077972065054727\n",
      "train loss:1.910227417795454\n",
      "train loss:1.9817676320466528\n",
      "=== epoch:131, train acc:0.36666666666666664, test acc:0.304 ===\n",
      "train loss:1.9356776273901486\n",
      "train loss:2.0311014005372114\n",
      "train loss:1.9662485768251656\n",
      "=== epoch:132, train acc:0.36666666666666664, test acc:0.3041 ===\n",
      "train loss:2.0229763655154356\n",
      "train loss:1.9963182653533247\n",
      "train loss:1.913967281322617\n",
      "=== epoch:133, train acc:0.36666666666666664, test acc:0.3052 ===\n",
      "train loss:1.960732306428926\n",
      "train loss:1.9610766870200917\n",
      "train loss:1.9619589825784347\n",
      "=== epoch:134, train acc:0.37333333333333335, test acc:0.3049 ===\n",
      "train loss:1.914465697665662\n",
      "train loss:2.005072855898352\n",
      "train loss:1.913664696559204\n",
      "=== epoch:135, train acc:0.37333333333333335, test acc:0.3062 ===\n",
      "train loss:1.930418465524915\n",
      "train loss:1.9120372509328658\n",
      "train loss:1.998969422366456\n",
      "=== epoch:136, train acc:0.38, test acc:0.3074 ===\n",
      "train loss:1.9526234929011925\n",
      "train loss:1.96201747569532\n",
      "train loss:1.9270822219545969\n",
      "=== epoch:137, train acc:0.38333333333333336, test acc:0.3085 ===\n",
      "train loss:1.9690781964302264\n",
      "train loss:1.9706272985635391\n",
      "train loss:1.9619733945825844\n",
      "=== epoch:138, train acc:0.38666666666666666, test acc:0.3104 ===\n",
      "train loss:1.9087088806204153\n",
      "train loss:1.9995413008648457\n",
      "train loss:1.917705350970661\n",
      "=== epoch:139, train acc:0.39, test acc:0.3108 ===\n",
      "train loss:1.9168331889524408\n",
      "train loss:1.8827913681888513\n",
      "train loss:1.9284703889147787\n",
      "=== epoch:140, train acc:0.39, test acc:0.3103 ===\n",
      "train loss:1.9527043605469274\n",
      "train loss:1.9107279359627267\n",
      "train loss:1.9027232551081605\n",
      "=== epoch:141, train acc:0.3933333333333333, test acc:0.3119 ===\n",
      "train loss:1.9815874601296781\n",
      "train loss:1.9994274207515088\n",
      "train loss:1.843977060497016\n",
      "=== epoch:142, train acc:0.38666666666666666, test acc:0.3127 ===\n",
      "train loss:1.8912329651533648\n",
      "train loss:1.9560597121394312\n",
      "train loss:1.903714704191601\n",
      "=== epoch:143, train acc:0.38, test acc:0.3138 ===\n",
      "train loss:2.0227030714273324\n",
      "train loss:1.9093666750207121\n",
      "train loss:1.8538673230357494\n",
      "=== epoch:144, train acc:0.38, test acc:0.3141 ===\n",
      "train loss:1.8836751066405808\n",
      "train loss:1.8781921863946576\n",
      "train loss:1.861755737455144\n",
      "=== epoch:145, train acc:0.3933333333333333, test acc:0.3136 ===\n",
      "train loss:1.8501117891675736\n",
      "train loss:1.8891921070233406\n",
      "train loss:1.9432542152493733\n",
      "=== epoch:146, train acc:0.3933333333333333, test acc:0.3146 ===\n",
      "train loss:1.9561200770390912\n",
      "train loss:1.8080743847572598\n",
      "train loss:1.8802487641198946\n",
      "=== epoch:147, train acc:0.4033333333333333, test acc:0.3165 ===\n",
      "train loss:1.9505309007832097\n",
      "train loss:1.8802597662337956\n",
      "train loss:1.8923178303163255\n",
      "=== epoch:148, train acc:0.4033333333333333, test acc:0.3201 ===\n",
      "train loss:1.9544760070980414\n",
      "train loss:1.8777812164780965\n",
      "train loss:1.872334742675953\n",
      "=== epoch:149, train acc:0.4066666666666667, test acc:0.3225 ===\n",
      "train loss:1.8114382189980154\n",
      "train loss:1.8737125412025544\n",
      "train loss:1.9629554224424066\n",
      "=== epoch:150, train acc:0.4, test acc:0.3208 ===\n",
      "train loss:1.8971645713352503\n",
      "train loss:1.9407799714751144\n",
      "train loss:1.9074326663432353\n",
      "=== epoch:151, train acc:0.4033333333333333, test acc:0.3224 ===\n",
      "train loss:1.8318797995585085\n",
      "train loss:1.8807907361935146\n",
      "train loss:1.9202627422825167\n",
      "=== epoch:152, train acc:0.4066666666666667, test acc:0.3238 ===\n",
      "train loss:1.696681070775554\n",
      "train loss:1.8082540400708986\n",
      "train loss:1.8983650495744835\n",
      "=== epoch:153, train acc:0.4033333333333333, test acc:0.3215 ===\n",
      "train loss:1.7606829091031913\n",
      "train loss:1.9167174972398962\n",
      "train loss:1.7461502417898211\n",
      "=== epoch:154, train acc:0.3933333333333333, test acc:0.3209 ===\n",
      "train loss:1.8625013288705166\n",
      "train loss:1.849903424395056\n",
      "train loss:1.8740002638230577\n",
      "=== epoch:155, train acc:0.4033333333333333, test acc:0.3226 ===\n",
      "train loss:1.8673437290538906\n",
      "train loss:1.9238690415159747\n",
      "train loss:1.781006519095907\n",
      "=== epoch:156, train acc:0.4066666666666667, test acc:0.3241 ===\n",
      "train loss:1.9795490818324137\n",
      "train loss:1.971342657703963\n",
      "train loss:1.9021438007310214\n",
      "=== epoch:157, train acc:0.41, test acc:0.327 ===\n",
      "train loss:1.6957140310617271\n",
      "train loss:1.8970879162930507\n",
      "train loss:1.9499028918670647\n",
      "=== epoch:158, train acc:0.4166666666666667, test acc:0.3266 ===\n",
      "train loss:1.8672992364080923\n",
      "train loss:1.9090227570747609\n",
      "train loss:1.781144297187025\n",
      "=== epoch:159, train acc:0.42333333333333334, test acc:0.3279 ===\n",
      "train loss:1.8143233786154191\n",
      "train loss:1.8021043933931942\n",
      "train loss:1.7121297108746552\n",
      "=== epoch:160, train acc:0.4166666666666667, test acc:0.3258 ===\n",
      "train loss:1.8082345132882622\n",
      "train loss:1.925355297528075\n",
      "train loss:1.8105313229331514\n",
      "=== epoch:161, train acc:0.42, test acc:0.3265 ===\n",
      "train loss:1.8420991956015016\n",
      "train loss:1.7609366882073672\n",
      "train loss:1.8925694922037877\n",
      "=== epoch:162, train acc:0.4266666666666667, test acc:0.3266 ===\n",
      "train loss:1.8468623661342631\n",
      "train loss:1.844445152482215\n",
      "train loss:1.7022379530820302\n",
      "=== epoch:163, train acc:0.42333333333333334, test acc:0.3279 ===\n",
      "train loss:1.8155020261588768\n",
      "train loss:1.8878918709845496\n",
      "train loss:1.7966653278091635\n",
      "=== epoch:164, train acc:0.42333333333333334, test acc:0.3293 ===\n",
      "train loss:1.8423684393161384\n",
      "train loss:1.8808835813748963\n",
      "train loss:1.8270259221395297\n",
      "=== epoch:165, train acc:0.4266666666666667, test acc:0.3288 ===\n",
      "train loss:1.722196754655632\n",
      "train loss:1.9441270076273238\n",
      "train loss:1.7189988862645273\n",
      "=== epoch:166, train acc:0.4266666666666667, test acc:0.3288 ===\n",
      "train loss:1.673349557405879\n",
      "train loss:1.891701972974276\n",
      "train loss:1.7688974213359479\n",
      "=== epoch:167, train acc:0.4266666666666667, test acc:0.3277 ===\n",
      "train loss:1.829760388925896\n",
      "train loss:1.8663394490713676\n",
      "train loss:1.804962267149245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:168, train acc:0.42, test acc:0.329 ===\n",
      "train loss:1.7641651764703326\n",
      "train loss:1.8424435035901385\n",
      "train loss:1.796890194675882\n",
      "=== epoch:169, train acc:0.42333333333333334, test acc:0.3307 ===\n",
      "train loss:1.8207761621112744\n",
      "train loss:1.7137931024240984\n",
      "train loss:1.7027172619686874\n",
      "=== epoch:170, train acc:0.42333333333333334, test acc:0.3308 ===\n",
      "train loss:1.816264314094005\n",
      "train loss:1.7257345947853897\n",
      "train loss:1.8455503677592051\n",
      "=== epoch:171, train acc:0.4266666666666667, test acc:0.3337 ===\n",
      "train loss:1.7230223942285425\n",
      "train loss:1.7842951950386299\n",
      "train loss:1.825948147732555\n",
      "=== epoch:172, train acc:0.4266666666666667, test acc:0.3341 ===\n",
      "train loss:1.7085024109986842\n",
      "train loss:1.6986791719639336\n",
      "train loss:1.7839125773113471\n",
      "=== epoch:173, train acc:0.43, test acc:0.3345 ===\n",
      "train loss:1.8067225705108871\n",
      "train loss:1.7918011358798254\n",
      "train loss:1.7648662082880735\n",
      "=== epoch:174, train acc:0.43333333333333335, test acc:0.3359 ===\n",
      "train loss:1.775133580864636\n",
      "train loss:1.712637915141026\n",
      "train loss:1.9576217322518557\n",
      "=== epoch:175, train acc:0.43666666666666665, test acc:0.3371 ===\n",
      "train loss:1.8171444307951463\n",
      "train loss:1.8336250615876122\n",
      "train loss:1.859582019012535\n",
      "=== epoch:176, train acc:0.43666666666666665, test acc:0.3401 ===\n",
      "train loss:1.7868407715625372\n",
      "train loss:1.6804991171665045\n",
      "train loss:1.75147239946701\n",
      "=== epoch:177, train acc:0.44, test acc:0.3382 ===\n",
      "train loss:1.7668167019336432\n",
      "train loss:1.8544783642186227\n",
      "train loss:1.6964406418068014\n",
      "=== epoch:178, train acc:0.44333333333333336, test acc:0.3405 ===\n",
      "train loss:1.787641309538739\n",
      "train loss:1.7127084200542708\n",
      "train loss:1.734222584970321\n",
      "=== epoch:179, train acc:0.44333333333333336, test acc:0.3413 ===\n",
      "train loss:1.7317216598084801\n",
      "train loss:1.7435399953414714\n",
      "train loss:1.7503644932327536\n",
      "=== epoch:180, train acc:0.44, test acc:0.3421 ===\n",
      "train loss:1.8019866002920863\n",
      "train loss:1.7411960865076486\n",
      "train loss:1.675470907830582\n",
      "=== epoch:181, train acc:0.43666666666666665, test acc:0.3427 ===\n",
      "train loss:1.847726869877401\n",
      "train loss:1.8082791158743836\n",
      "train loss:1.7114712096112112\n",
      "=== epoch:182, train acc:0.43666666666666665, test acc:0.3423 ===\n",
      "train loss:1.6536960477017155\n",
      "train loss:1.7760599156616153\n",
      "train loss:1.7406554245652115\n",
      "=== epoch:183, train acc:0.44333333333333336, test acc:0.3422 ===\n",
      "train loss:1.7147060311748612\n",
      "train loss:1.7290489667896578\n",
      "train loss:1.6413423686841861\n",
      "=== epoch:184, train acc:0.44333333333333336, test acc:0.3417 ===\n",
      "train loss:1.7599513189944318\n",
      "train loss:1.7512458851995074\n",
      "train loss:1.6622842036288459\n",
      "=== epoch:185, train acc:0.44666666666666666, test acc:0.3422 ===\n",
      "train loss:1.6286967346012673\n",
      "train loss:1.70195519147137\n",
      "train loss:1.7694433833952699\n",
      "=== epoch:186, train acc:0.44666666666666666, test acc:0.3422 ===\n",
      "train loss:1.8045346242380063\n",
      "train loss:1.7643118075187088\n",
      "train loss:1.761898874723248\n",
      "=== epoch:187, train acc:0.45, test acc:0.3433 ===\n",
      "train loss:1.778895163324581\n",
      "train loss:1.8174131613641527\n",
      "train loss:1.6869085342333896\n",
      "=== epoch:188, train acc:0.45, test acc:0.3451 ===\n",
      "train loss:1.807058055084312\n",
      "train loss:1.7467111472219308\n",
      "train loss:1.7986070827988194\n",
      "=== epoch:189, train acc:0.45, test acc:0.3467 ===\n",
      "train loss:1.8409684329306015\n",
      "train loss:1.6737273507046666\n",
      "train loss:1.7210522584412593\n",
      "=== epoch:190, train acc:0.44666666666666666, test acc:0.3481 ===\n",
      "train loss:1.7879072125632876\n",
      "train loss:1.7433837966504862\n",
      "train loss:1.738494000422777\n",
      "=== epoch:191, train acc:0.43666666666666665, test acc:0.346 ===\n",
      "train loss:1.7406646088291413\n",
      "train loss:1.8243731167868074\n",
      "train loss:1.6635806712805554\n",
      "=== epoch:192, train acc:0.44, test acc:0.3482 ===\n",
      "train loss:1.7243634509413261\n",
      "train loss:1.750451804495774\n",
      "train loss:1.7810069938614275\n",
      "=== epoch:193, train acc:0.4533333333333333, test acc:0.3488 ===\n",
      "train loss:1.7788881040674698\n",
      "train loss:1.7372750143471973\n",
      "train loss:1.6711828387535044\n",
      "=== epoch:194, train acc:0.4533333333333333, test acc:0.35 ===\n",
      "train loss:1.6780358839518195\n",
      "train loss:1.7686229181762625\n",
      "train loss:1.7722032510530317\n",
      "=== epoch:195, train acc:0.46, test acc:0.3529 ===\n",
      "train loss:1.7125657723576404\n",
      "train loss:1.6434771366503755\n",
      "train loss:1.6460330481458285\n",
      "=== epoch:196, train acc:0.4533333333333333, test acc:0.352 ===\n",
      "train loss:1.7123982521121568\n",
      "train loss:1.719293920453081\n",
      "train loss:1.7451556510067305\n",
      "=== epoch:197, train acc:0.45666666666666667, test acc:0.3543 ===\n",
      "train loss:1.7209572777271618\n",
      "train loss:1.6917324717196005\n",
      "train loss:1.7692193988751743\n",
      "=== epoch:198, train acc:0.4633333333333333, test acc:0.3569 ===\n",
      "train loss:1.6119634148016215\n",
      "train loss:1.7282676806983324\n",
      "train loss:1.6821783752393455\n",
      "=== epoch:199, train acc:0.45666666666666667, test acc:0.3579 ===\n",
      "train loss:1.7909073666784052\n",
      "train loss:1.816306070008136\n",
      "train loss:1.7626318250545663\n",
      "=== epoch:200, train acc:0.4633333333333333, test acc:0.3565 ===\n",
      "train loss:1.6161406764575832\n",
      "train loss:1.6861359485441516\n",
      "train loss:1.6685328468718377\n",
      "=== epoch:201, train acc:0.47, test acc:0.358 ===\n",
      "train loss:1.6641516505709433\n",
      "train loss:1.6694224640824638\n",
      "train loss:1.833436744449275\n",
      "=== epoch:202, train acc:0.47, test acc:0.3613 ===\n",
      "train loss:1.798730842941813\n",
      "train loss:1.7692127012124206\n",
      "train loss:1.6856494590920232\n",
      "=== epoch:203, train acc:0.47333333333333333, test acc:0.3637 ===\n",
      "train loss:1.7967344677452717\n",
      "train loss:1.647105061382579\n",
      "train loss:1.753351830508295\n",
      "=== epoch:204, train acc:0.47, test acc:0.3667 ===\n",
      "train loss:1.7555030292406664\n",
      "train loss:1.6557516772173975\n",
      "train loss:1.626453726134647\n",
      "=== epoch:205, train acc:0.47, test acc:0.3647 ===\n",
      "train loss:1.7331709244166786\n",
      "train loss:1.722791731289389\n",
      "train loss:1.5803203435714241\n",
      "=== epoch:206, train acc:0.4666666666666667, test acc:0.3676 ===\n",
      "train loss:1.6599737088223687\n",
      "train loss:1.6121477909385797\n",
      "train loss:1.635142866394845\n",
      "=== epoch:207, train acc:0.47, test acc:0.3677 ===\n",
      "train loss:1.7071258662754472\n",
      "train loss:1.6941387960795347\n",
      "train loss:1.6740117647725454\n",
      "=== epoch:208, train acc:0.47333333333333333, test acc:0.3715 ===\n",
      "train loss:1.6893984517276552\n",
      "train loss:1.5702460237283173\n",
      "train loss:1.5035010687510115\n",
      "=== epoch:209, train acc:0.4766666666666667, test acc:0.3722 ===\n",
      "train loss:1.7247771978922066\n",
      "train loss:1.6737843150495249\n",
      "train loss:1.6519937641048263\n",
      "=== epoch:210, train acc:0.4766666666666667, test acc:0.3717 ===\n",
      "train loss:1.6884060638020555\n",
      "train loss:1.646511323881697\n",
      "train loss:1.5692649229137103\n",
      "=== epoch:211, train acc:0.48, test acc:0.3764 ===\n",
      "train loss:1.7484506606769903\n",
      "train loss:1.6529710372352862\n",
      "train loss:1.5141143652362856\n",
      "=== epoch:212, train acc:0.48, test acc:0.373 ===\n",
      "train loss:1.6928829564178978\n",
      "train loss:1.5792720771396207\n",
      "train loss:1.713820663385113\n",
      "=== epoch:213, train acc:0.4866666666666667, test acc:0.378 ===\n",
      "train loss:1.6060902931019518\n",
      "train loss:1.7882199999494088\n",
      "train loss:1.7183221800615527\n",
      "=== epoch:214, train acc:0.4866666666666667, test acc:0.3777 ===\n",
      "train loss:1.6681992312409002\n",
      "train loss:1.7431291139936267\n",
      "train loss:1.674701361554682\n",
      "=== epoch:215, train acc:0.49666666666666665, test acc:0.3826 ===\n",
      "train loss:1.6553786519299925\n",
      "train loss:1.7031572313731287\n",
      "train loss:1.6741984181548206\n",
      "=== epoch:216, train acc:0.49, test acc:0.3825 ===\n",
      "train loss:1.6203559300368022\n",
      "train loss:1.6592745134759739\n",
      "train loss:1.6259572757718914\n",
      "=== epoch:217, train acc:0.49, test acc:0.3854 ===\n",
      "train loss:1.5823540486942043\n",
      "train loss:1.6725152534480388\n",
      "train loss:1.6053372390109981\n",
      "=== epoch:218, train acc:0.5, test acc:0.3886 ===\n",
      "train loss:1.5805100499473042\n",
      "train loss:1.6819322249828617\n",
      "train loss:1.5995581337924252\n",
      "=== epoch:219, train acc:0.5066666666666667, test acc:0.3932 ===\n",
      "train loss:1.6343374050211132\n",
      "train loss:1.661359193946078\n",
      "train loss:1.6167738796468358\n",
      "=== epoch:220, train acc:0.51, test acc:0.3959 ===\n",
      "train loss:1.6434168000712146\n",
      "train loss:1.6476194066538596\n",
      "train loss:1.580579535876868\n",
      "=== epoch:221, train acc:0.5133333333333333, test acc:0.394 ===\n",
      "train loss:1.5844848653977364\n",
      "train loss:1.6132642266947843\n",
      "train loss:1.555325081393484\n",
      "=== epoch:222, train acc:0.5133333333333333, test acc:0.3941 ===\n",
      "train loss:1.5578021577013101\n",
      "train loss:1.5355147637969397\n",
      "train loss:1.6239748965115683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:223, train acc:0.51, test acc:0.3918 ===\n",
      "train loss:1.4674646357228591\n",
      "train loss:1.529634087162373\n",
      "train loss:1.5354930186843583\n",
      "=== epoch:224, train acc:0.5066666666666667, test acc:0.3893 ===\n",
      "train loss:1.6986118256989744\n",
      "train loss:1.4658852951628856\n",
      "train loss:1.6858361356497795\n",
      "=== epoch:225, train acc:0.51, test acc:0.3923 ===\n",
      "train loss:1.38503627980676\n",
      "train loss:1.5631879050361106\n",
      "train loss:1.4516559161896105\n",
      "=== epoch:226, train acc:0.5, test acc:0.3892 ===\n",
      "train loss:1.612591225640986\n",
      "train loss:1.5455041943331693\n",
      "train loss:1.638435893028681\n",
      "=== epoch:227, train acc:0.51, test acc:0.3943 ===\n",
      "train loss:1.6666641728487748\n",
      "train loss:1.5471775683599696\n",
      "train loss:1.6087627683128602\n",
      "=== epoch:228, train acc:0.5233333333333333, test acc:0.3999 ===\n",
      "train loss:1.4580163267046735\n",
      "train loss:1.654558029172143\n",
      "train loss:1.6588532348716571\n",
      "=== epoch:229, train acc:0.5333333333333333, test acc:0.4019 ===\n",
      "train loss:1.4278074443827415\n",
      "train loss:1.524957345507581\n",
      "train loss:1.4442307030309667\n",
      "=== epoch:230, train acc:0.5266666666666666, test acc:0.3983 ===\n",
      "train loss:1.594485978212249\n",
      "train loss:1.457439427180137\n",
      "train loss:1.5437483291237948\n",
      "=== epoch:231, train acc:0.5366666666666666, test acc:0.3998 ===\n",
      "train loss:1.5359070364186187\n",
      "train loss:1.6346384186768368\n",
      "train loss:1.4997707228103387\n",
      "=== epoch:232, train acc:0.54, test acc:0.4062 ===\n",
      "train loss:1.5918470316119484\n",
      "train loss:1.427021526429551\n",
      "train loss:1.4888131169343775\n",
      "=== epoch:233, train acc:0.54, test acc:0.4096 ===\n",
      "train loss:1.5928359000609575\n",
      "train loss:1.6026325727683397\n",
      "train loss:1.5096062741590401\n",
      "=== epoch:234, train acc:0.5333333333333333, test acc:0.4101 ===\n",
      "train loss:1.5204570914096218\n",
      "train loss:1.5581447036885945\n",
      "train loss:1.6576671171730875\n",
      "=== epoch:235, train acc:0.55, test acc:0.4141 ===\n",
      "train loss:1.5072765095912573\n",
      "train loss:1.5501572527488197\n",
      "train loss:1.5339800413307116\n",
      "=== epoch:236, train acc:0.5566666666666666, test acc:0.415 ===\n",
      "train loss:1.5135829454030933\n",
      "train loss:1.5616414796857916\n",
      "train loss:1.445764833426098\n",
      "=== epoch:237, train acc:0.5466666666666666, test acc:0.4157 ===\n",
      "train loss:1.4157547760839635\n",
      "train loss:1.5720362398673715\n",
      "train loss:1.567950110949484\n",
      "=== epoch:238, train acc:0.55, test acc:0.4161 ===\n",
      "train loss:1.5863951914903134\n",
      "train loss:1.4912424135735265\n",
      "train loss:1.5495813436537949\n",
      "=== epoch:239, train acc:0.56, test acc:0.4201 ===\n",
      "train loss:1.3263869829916926\n",
      "train loss:1.5214834403580957\n",
      "train loss:1.485773806198584\n",
      "=== epoch:240, train acc:0.5766666666666667, test acc:0.4237 ===\n",
      "train loss:1.5574259884741846\n",
      "train loss:1.3993421361500458\n",
      "train loss:1.5775892659357793\n",
      "=== epoch:241, train acc:0.5833333333333334, test acc:0.4251 ===\n",
      "train loss:1.5807556310459492\n",
      "train loss:1.4644630850148297\n",
      "train loss:1.5320414404452538\n",
      "=== epoch:242, train acc:0.5866666666666667, test acc:0.4254 ===\n",
      "train loss:1.4974711636328886\n",
      "train loss:1.566932422091422\n",
      "train loss:1.4162050268394262\n",
      "=== epoch:243, train acc:0.58, test acc:0.4251 ===\n",
      "train loss:1.5062013893150845\n",
      "train loss:1.409548302214273\n",
      "train loss:1.563203322181283\n",
      "=== epoch:244, train acc:0.59, test acc:0.4338 ===\n",
      "train loss:1.5127729092676063\n",
      "train loss:1.5447143424063847\n",
      "train loss:1.4893388997540982\n",
      "=== epoch:245, train acc:0.6, test acc:0.4365 ===\n",
      "train loss:1.5303858651470035\n",
      "train loss:1.4508115876480057\n",
      "train loss:1.3705470498953174\n",
      "=== epoch:246, train acc:0.6066666666666667, test acc:0.4399 ===\n",
      "train loss:1.4289343039939637\n",
      "train loss:1.4926010369085703\n",
      "train loss:1.38326982477312\n",
      "=== epoch:247, train acc:0.61, test acc:0.4431 ===\n",
      "train loss:1.458876624838257\n",
      "train loss:1.302120385817272\n",
      "train loss:1.4762711588591544\n",
      "=== epoch:248, train acc:0.6066666666666667, test acc:0.442 ===\n",
      "train loss:1.5468127727556507\n",
      "train loss:1.4026861583937489\n",
      "train loss:1.5216229040511804\n",
      "=== epoch:249, train acc:0.6066666666666667, test acc:0.4468 ===\n",
      "train loss:1.341096837901474\n",
      "train loss:1.4845107512326405\n",
      "train loss:1.6168579405301207\n",
      "=== epoch:250, train acc:0.6066666666666667, test acc:0.448 ===\n",
      "train loss:1.4340286654885934\n",
      "train loss:1.4523625185892153\n",
      "train loss:1.4840022447458627\n",
      "=== epoch:251, train acc:0.62, test acc:0.4522 ===\n",
      "train loss:1.3961845676694313\n",
      "train loss:1.3658325040911166\n",
      "train loss:1.4054633961961613\n",
      "=== epoch:252, train acc:0.6166666666666667, test acc:0.4545 ===\n",
      "train loss:1.426686425718723\n",
      "train loss:1.3779255246709692\n",
      "train loss:1.4520040976030832\n",
      "=== epoch:253, train acc:0.6166666666666667, test acc:0.4553 ===\n",
      "train loss:1.2840621785358695\n",
      "train loss:1.4265886747109706\n",
      "train loss:1.4493085396730414\n",
      "=== epoch:254, train acc:0.6166666666666667, test acc:0.4568 ===\n",
      "train loss:1.4315867734447778\n",
      "train loss:1.3383548324435282\n",
      "train loss:1.3847086283095174\n",
      "=== epoch:255, train acc:0.6266666666666667, test acc:0.4609 ===\n",
      "train loss:1.460028305216296\n",
      "train loss:1.5776431937761575\n",
      "train loss:1.5679679051292177\n",
      "=== epoch:256, train acc:0.6266666666666667, test acc:0.4654 ===\n",
      "train loss:1.54026451116962\n",
      "train loss:1.4714768569777192\n",
      "train loss:1.4540455079489265\n",
      "=== epoch:257, train acc:0.6233333333333333, test acc:0.4706 ===\n",
      "train loss:1.4724857695480438\n",
      "train loss:1.4291656712569745\n",
      "train loss:1.261047373256814\n",
      "=== epoch:258, train acc:0.62, test acc:0.4692 ===\n",
      "train loss:1.3507557838808495\n",
      "train loss:1.5579799238845715\n",
      "train loss:1.34601569936679\n",
      "=== epoch:259, train acc:0.6333333333333333, test acc:0.471 ===\n",
      "train loss:1.3121485372813166\n",
      "train loss:1.4754471448186708\n",
      "train loss:1.2934153727066764\n",
      "=== epoch:260, train acc:0.6266666666666667, test acc:0.4726 ===\n",
      "train loss:1.287144606350226\n",
      "train loss:1.4575695841779779\n",
      "train loss:1.2868178685817466\n",
      "=== epoch:261, train acc:0.63, test acc:0.4722 ===\n",
      "train loss:1.4529548158886092\n",
      "train loss:1.198934695985396\n",
      "train loss:1.3904883229303833\n",
      "=== epoch:262, train acc:0.6433333333333333, test acc:0.4775 ===\n",
      "train loss:1.3912077550231512\n",
      "train loss:1.4122436882777263\n",
      "train loss:1.3793445156928952\n",
      "=== epoch:263, train acc:0.6433333333333333, test acc:0.48 ===\n",
      "train loss:1.3877681308309926\n",
      "train loss:1.3277426483911803\n",
      "train loss:1.2809358044158554\n",
      "=== epoch:264, train acc:0.6366666666666667, test acc:0.4803 ===\n",
      "train loss:1.5168321283457746\n",
      "train loss:1.3625587372914367\n",
      "train loss:1.3849149390849151\n",
      "=== epoch:265, train acc:0.6433333333333333, test acc:0.4825 ===\n",
      "train loss:1.441423336794542\n",
      "train loss:1.2367523665740001\n",
      "train loss:1.2636361085626115\n",
      "=== epoch:266, train acc:0.64, test acc:0.4833 ===\n",
      "train loss:1.4561135678907424\n",
      "train loss:1.2953886131038985\n",
      "train loss:1.4067725390907437\n",
      "=== epoch:267, train acc:0.6433333333333333, test acc:0.4881 ===\n",
      "train loss:1.3506127883599286\n",
      "train loss:1.4649420642757514\n",
      "train loss:1.3182126178175546\n",
      "=== epoch:268, train acc:0.6466666666666666, test acc:0.4891 ===\n",
      "train loss:1.2707147436001986\n",
      "train loss:1.3591132757452962\n",
      "train loss:1.4397652369846043\n",
      "=== epoch:269, train acc:0.64, test acc:0.4846 ===\n",
      "train loss:1.3843798347812615\n",
      "train loss:1.226194483517088\n",
      "train loss:1.3745162354529452\n",
      "=== epoch:270, train acc:0.64, test acc:0.4843 ===\n",
      "train loss:1.3558670424455668\n",
      "train loss:1.3634374671631715\n",
      "train loss:1.3854420579377105\n",
      "=== epoch:271, train acc:0.64, test acc:0.4895 ===\n",
      "train loss:1.2814823614362654\n",
      "train loss:1.23637111273678\n",
      "train loss:1.3649606667159997\n",
      "=== epoch:272, train acc:0.6433333333333333, test acc:0.4871 ===\n",
      "train loss:1.3377388338468605\n",
      "train loss:1.2973803077665014\n",
      "train loss:1.2898469056321609\n",
      "=== epoch:273, train acc:0.64, test acc:0.4881 ===\n",
      "train loss:1.4123955534233474\n",
      "train loss:1.3673409800563805\n",
      "train loss:1.2750107966256503\n",
      "=== epoch:274, train acc:0.64, test acc:0.4905 ===\n",
      "train loss:1.3250637750151635\n",
      "train loss:1.4292431655009274\n",
      "train loss:1.5294308352795611\n",
      "=== epoch:275, train acc:0.64, test acc:0.4948 ===\n",
      "train loss:1.2327676334054332\n",
      "train loss:1.271515330127307\n",
      "train loss:1.4737764543198364\n",
      "=== epoch:276, train acc:0.64, test acc:0.4959 ===\n",
      "train loss:1.3533395081983945\n",
      "train loss:1.2971387546212072\n",
      "train loss:1.278763584989386\n",
      "=== epoch:277, train acc:0.6433333333333333, test acc:0.4961 ===\n",
      "train loss:1.2190222261460018\n",
      "train loss:1.2700236201010238\n",
      "train loss:1.290735443753324\n",
      "=== epoch:278, train acc:0.64, test acc:0.4959 ===\n",
      "train loss:1.3442207489594467\n",
      "train loss:1.1592035605962412\n",
      "train loss:1.3245380516855314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:279, train acc:0.64, test acc:0.4978 ===\n",
      "train loss:1.2541615916454285\n",
      "train loss:1.2379115836251102\n",
      "train loss:1.200599553748501\n",
      "=== epoch:280, train acc:0.6433333333333333, test acc:0.4974 ===\n",
      "train loss:1.2911466312121964\n",
      "train loss:1.2870235418404503\n",
      "train loss:1.2794616114166057\n",
      "=== epoch:281, train acc:0.6433333333333333, test acc:0.4979 ===\n",
      "train loss:1.3626399513621879\n",
      "train loss:1.334256309559863\n",
      "train loss:1.3658479204175265\n",
      "=== epoch:282, train acc:0.66, test acc:0.5027 ===\n",
      "train loss:1.25644611821099\n",
      "train loss:1.4002354058347342\n",
      "train loss:1.2171245412777394\n",
      "=== epoch:283, train acc:0.6566666666666666, test acc:0.5058 ===\n",
      "train loss:1.1516030844668783\n",
      "train loss:1.0757136238200216\n",
      "train loss:1.3874529571305265\n",
      "=== epoch:284, train acc:0.65, test acc:0.5054 ===\n",
      "train loss:1.190040060135139\n",
      "train loss:1.4524737312122433\n",
      "train loss:1.2605730400290682\n",
      "=== epoch:285, train acc:0.6533333333333333, test acc:0.5088 ===\n",
      "train loss:1.2910654371911034\n",
      "train loss:1.1464682297067637\n",
      "train loss:1.283265882423498\n",
      "=== epoch:286, train acc:0.6566666666666666, test acc:0.5082 ===\n",
      "train loss:1.2885263989505436\n",
      "train loss:1.1902435355151362\n",
      "train loss:1.1715027295793414\n",
      "=== epoch:287, train acc:0.6633333333333333, test acc:0.5077 ===\n",
      "train loss:1.004537802719701\n",
      "train loss:1.2299580346679155\n",
      "train loss:1.329044433770333\n",
      "=== epoch:288, train acc:0.6533333333333333, test acc:0.5092 ===\n",
      "train loss:1.1159793623089773\n",
      "train loss:1.1446942784347953\n",
      "train loss:1.3384113175948533\n",
      "=== epoch:289, train acc:0.6566666666666666, test acc:0.5103 ===\n",
      "train loss:1.3169576919536319\n",
      "train loss:1.3378907794436845\n",
      "train loss:1.278151316832235\n",
      "=== epoch:290, train acc:0.67, test acc:0.5169 ===\n",
      "train loss:1.308309486430509\n",
      "train loss:1.2309436566472933\n",
      "train loss:1.1619823919382466\n",
      "=== epoch:291, train acc:0.6733333333333333, test acc:0.5185 ===\n",
      "train loss:1.3843928372077559\n",
      "train loss:1.31347148761969\n",
      "train loss:1.2497927820111208\n",
      "=== epoch:292, train acc:0.6666666666666666, test acc:0.5212 ===\n",
      "train loss:1.1909582076195973\n",
      "train loss:1.2352217667898833\n",
      "train loss:1.1992437221073822\n",
      "=== epoch:293, train acc:0.66, test acc:0.5199 ===\n",
      "train loss:1.3308807423990863\n",
      "train loss:1.1220020883965554\n",
      "train loss:1.2697625914417519\n",
      "=== epoch:294, train acc:0.6566666666666666, test acc:0.519 ===\n",
      "train loss:1.207566351905782\n",
      "train loss:1.159711012799796\n",
      "train loss:1.2650129938348713\n",
      "=== epoch:295, train acc:0.6633333333333333, test acc:0.5205 ===\n",
      "train loss:1.1832255498908533\n",
      "train loss:1.1779574272621771\n",
      "train loss:1.078851845313325\n",
      "=== epoch:296, train acc:0.67, test acc:0.5218 ===\n",
      "train loss:1.197763798138123\n",
      "train loss:1.1382962804211287\n",
      "train loss:1.212275174868635\n",
      "=== epoch:297, train acc:0.6666666666666666, test acc:0.522 ===\n",
      "train loss:1.1692020045360314\n",
      "train loss:1.3896191547721253\n",
      "train loss:1.1751095364692816\n",
      "=== epoch:298, train acc:0.67, test acc:0.5243 ===\n",
      "train loss:1.1258423918187308\n",
      "train loss:1.2612796587026274\n",
      "train loss:1.1365102284670088\n",
      "=== epoch:299, train acc:0.67, test acc:0.5252 ===\n",
      "train loss:1.2826995109123982\n",
      "train loss:1.1974215646845463\n",
      "train loss:1.3117822998385078\n",
      "=== epoch:300, train acc:0.6833333333333333, test acc:0.5282 ===\n",
      "train loss:1.3786137681879642\n",
      "train loss:1.220352400559681\n",
      "train loss:1.0279031956227982\n",
      "=== epoch:301, train acc:0.6833333333333333, test acc:0.5314 ===\n",
      "train loss:1.1985955414921765\n",
      "train loss:1.1904884068013182\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5353\n"
     ]
    }
   ],
   "source": [
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], \n",
    "                              output_size=10, use_dropout=True, dropout_ration=0.2)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 hyperparameter 효율적으로 조정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data를 미리 분리하기\n",
    "\n",
    ": **Validation data**를 Training data로부터 미리 분리하여, hyperparameter를 조정할 때 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter 최적화\n",
    "\n",
    ": hyperparameter의 **최적 값**이 존재할 수 있는 **범위**를 조금씩 줄여나가는 방법. 대략적인 범위를 설정한 뒤에 무작위로 hyperparameter의 값을 **sampling**한 후, 정확도를 평가한다. 이러한 작업을 여러 번 반복하여 hyperparameter의 **최적 값**의 범위를 좁혀간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. hyperparameter의 값의 범위를 설정한다. (10의 거듭제곱 단위, **log scale**로 지정한다.)\n",
    "\n",
    "2. 설정된 범위에서 hyperparameter의 값을 무작위로 sampling한다.\n",
    "\n",
    "3. 2단계에서 sampling한 hyperparameter의 값을 사용하여 학습하고, validation data로 정확도를 평가한다.\n",
    "\n",
    "4. 2~3단계를 임의의 횟수만큼 반복하여, 그 정확도를 보고 hyperparameter의 범위를 수정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 빠르게 얻기 위해 훈련 데이터를 줄임\n",
    "x_train = x_train[:500]\n",
    "t_train = t_train[:500]\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:0.34 | lr:0.002141363879356653, weight decay:2.2490721709461877e-06\n",
      "val acc:0.14 | lr:1.0119319130575426e-06, weight decay:2.4945805735901177e-08\n",
      "val acc:0.11 | lr:9.011177375422073e-05, weight decay:2.2517081693222797e-08\n",
      "val acc:0.16 | lr:7.758060058145069e-05, weight decay:9.868714967512577e-05\n",
      "val acc:0.12 | lr:1.4730512991225995e-06, weight decay:2.4427424009176462e-05\n",
      "val acc:0.11 | lr:2.0236860593784194e-05, weight decay:1.3941279338956343e-08\n",
      "val acc:0.13 | lr:2.8549150966122274e-05, weight decay:1.272283259657272e-05\n",
      "val acc:0.14 | lr:0.001227825818158986, weight decay:9.472718930339805e-07\n",
      "val acc:0.64 | lr:0.003000752027899676, weight decay:2.921527565735996e-05\n",
      "val acc:0.39 | lr:0.003025156271310908, weight decay:2.4217396742361944e-05\n",
      "val acc:0.15 | lr:8.617549499567187e-05, weight decay:1.7104837515173504e-06\n",
      "val acc:0.87 | lr:0.008377781813198386, weight decay:2.065295037992613e-05\n",
      "val acc:0.09 | lr:6.575117454056138e-05, weight decay:2.0634527215246604e-08\n",
      "val acc:0.14 | lr:0.0003090899682766819, weight decay:5.940895603572273e-06\n",
      "val acc:0.13 | lr:2.9089278921005146e-06, weight decay:8.304078601866769e-08\n",
      "val acc:0.14 | lr:0.0002414978046972772, weight decay:1.361689066593439e-06\n",
      "val acc:0.1 | lr:4.016207523968447e-05, weight decay:1.5572312335883658e-08\n",
      "val acc:0.14 | lr:8.074864340207239e-05, weight decay:2.814673636080557e-07\n",
      "val acc:0.5 | lr:0.003397822323175631, weight decay:2.4805237058029227e-08\n",
      "val acc:0.08 | lr:7.11193798179207e-05, weight decay:2.8762429807574738e-06\n",
      "val acc:0.07 | lr:9.987481210864893e-06, weight decay:3.332697712843956e-06\n",
      "val acc:0.15 | lr:2.1641908082937176e-06, weight decay:1.3324944002397426e-05\n",
      "val acc:0.19 | lr:0.0001032198786308658, weight decay:6.370229552782159e-08\n",
      "val acc:0.12 | lr:1.4554950647975431e-06, weight decay:1.3953664056630148e-07\n",
      "val acc:0.08 | lr:1.499512222891676e-05, weight decay:1.0661464652781934e-05\n",
      "val acc:0.73 | lr:0.004473138257902729, weight decay:1.0802765626890522e-08\n",
      "val acc:0.2 | lr:3.5072877585661163e-06, weight decay:6.062425136119396e-07\n",
      "val acc:0.12 | lr:7.543442598837227e-05, weight decay:1.511046617130972e-07\n",
      "val acc:0.07 | lr:0.00012884281314840858, weight decay:3.536413623354262e-08\n",
      "val acc:0.11 | lr:3.577124309308393e-05, weight decay:1.0228598141663132e-05\n",
      "val acc:0.08 | lr:5.755559589099014e-06, weight decay:2.400512042929533e-05\n",
      "val acc:0.53 | lr:0.003396174109790688, weight decay:1.1897874956217943e-07\n",
      "val acc:0.13 | lr:0.00022890321181403745, weight decay:5.093840898153205e-08\n",
      "val acc:0.2 | lr:3.988903606045181e-06, weight decay:4.8615913631241335e-05\n",
      "val acc:0.13 | lr:4.725993770805692e-05, weight decay:4.155263170845988e-05\n",
      "val acc:0.83 | lr:0.009840669229140177, weight decay:5.708318782546727e-08\n",
      "val acc:0.54 | lr:0.002904452799843577, weight decay:7.833934695822442e-05\n",
      "val acc:0.11 | lr:1.3002221026144656e-06, weight decay:2.0405872798798904e-05\n",
      "val acc:0.21 | lr:0.000556154736271228, weight decay:7.5714090935438205e-06\n",
      "val acc:0.84 | lr:0.00944882482522704, weight decay:1.4337544797710936e-06\n",
      "val acc:0.11 | lr:3.7980001053962384e-05, weight decay:5.147984734640922e-06\n",
      "val acc:0.03 | lr:1.0030362969351347e-06, weight decay:2.023831058379368e-05\n",
      "val acc:0.12 | lr:4.611771610399546e-06, weight decay:2.617726586979123e-07\n",
      "val acc:0.11 | lr:2.9248338845496533e-06, weight decay:3.9551736763109615e-08\n",
      "val acc:0.13 | lr:5.233365729703904e-06, weight decay:1.8256705454187263e-08\n",
      "val acc:0.62 | lr:0.002570391330922367, weight decay:1.464944021741112e-05\n",
      "val acc:0.1 | lr:1.3140413565669522e-05, weight decay:4.3135350210215504e-07\n",
      "val acc:0.03 | lr:4.237691490148059e-05, weight decay:2.25607681299028e-05\n",
      "val acc:0.09 | lr:2.2735394753235032e-05, weight decay:1.8760816915341308e-05\n",
      "val acc:0.08 | lr:6.3785383884229014e-06, weight decay:2.3441081348952955e-08\n",
      "val acc:0.09 | lr:3.7140441947745104e-06, weight decay:1.8831742922399388e-06\n",
      "val acc:0.07 | lr:7.064251043326586e-05, weight decay:4.110105893189833e-08\n",
      "val acc:0.11 | lr:6.964227866966892e-06, weight decay:9.83858568075524e-07\n",
      "val acc:0.08 | lr:0.00014571888261918485, weight decay:4.050986223479349e-05\n",
      "val acc:0.11 | lr:0.00044513988851128966, weight decay:1.775775283086883e-06\n",
      "val acc:0.17 | lr:0.0010651242692714033, weight decay:4.156964588407309e-06\n",
      "val acc:0.64 | lr:0.004922238624944779, weight decay:2.940409293458591e-06\n",
      "val acc:0.11 | lr:4.3653135297283185e-06, weight decay:1.1689658204440017e-05\n",
      "val acc:0.77 | lr:0.0075899194533398835, weight decay:1.0077033280053093e-07\n",
      "val acc:0.11 | lr:8.328602139217021e-05, weight decay:7.415981761473327e-06\n",
      "val acc:0.6 | lr:0.004165565178688089, weight decay:1.7163633438862806e-05\n",
      "val acc:0.07 | lr:6.534229901179145e-06, weight decay:3.352374481114417e-06\n",
      "val acc:0.1 | lr:4.544989530567534e-06, weight decay:8.269703043522422e-05\n",
      "val acc:0.55 | lr:0.004556697120016462, weight decay:2.2187481263005907e-06\n",
      "val acc:0.13 | lr:4.2009199865446025e-06, weight decay:9.881819500406631e-08\n",
      "val acc:0.39 | lr:0.0024580244521379036, weight decay:3.679094176863681e-06\n",
      "val acc:0.39 | lr:0.002227415412075055, weight decay:1.4859801365713589e-08\n",
      "val acc:0.16 | lr:2.760930446548241e-05, weight decay:2.917836019850799e-08\n",
      "val acc:0.1 | lr:7.6253608616432725e-06, weight decay:1.5428660782395266e-06\n",
      "val acc:0.19 | lr:6.258132043278673e-05, weight decay:5.9888987840233265e-05\n",
      "val acc:0.11 | lr:0.0009673984037194738, weight decay:2.0723876617524687e-07\n",
      "val acc:0.09 | lr:2.1641563888227573e-05, weight decay:3.191562823203081e-05\n",
      "val acc:0.06 | lr:7.159201731674621e-06, weight decay:2.04275412086761e-05\n",
      "val acc:0.1 | lr:5.595457666089146e-05, weight decay:2.3999204323222695e-08\n",
      "val acc:0.48 | lr:0.002646076748360768, weight decay:1.4458644705773482e-08\n",
      "val acc:0.07 | lr:3.0003093604092522e-05, weight decay:1.0199808290139422e-05\n",
      "val acc:0.12 | lr:6.991402166140306e-06, weight decay:2.0512647239763784e-07\n",
      "val acc:0.15 | lr:1.9694210044204613e-05, weight decay:3.358201690918945e-05\n",
      "val acc:0.08 | lr:1.9047788475902816e-06, weight decay:3.7125252257853947e-07\n",
      "val acc:0.11 | lr:4.1449519135417856e-05, weight decay:1.8098066044285655e-06\n",
      "val acc:0.05 | lr:8.284535473680928e-06, weight decay:6.198381469350925e-08\n",
      "val acc:0.8 | lr:0.0059085283983120115, weight decay:1.6250459879486102e-08\n",
      "val acc:0.04 | lr:0.0004201942741889595, weight decay:1.2166905101401437e-07\n",
      "val acc:0.08 | lr:9.650902118247168e-05, weight decay:1.2526783559994327e-05\n",
      "val acc:0.18 | lr:0.0007146697667943329, weight decay:2.591022865670294e-06\n",
      "val acc:0.07 | lr:8.17648533775572e-05, weight decay:1.585934989115533e-08\n",
      "val acc:0.11 | lr:1.3144417196214786e-05, weight decay:6.696549272987374e-05\n",
      "val acc:0.06 | lr:1.1387257199239315e-06, weight decay:1.961591023180203e-06\n",
      "val acc:0.08 | lr:8.466885479585072e-06, weight decay:1.6980293265948984e-05\n",
      "val acc:0.14 | lr:5.132448120232386e-06, weight decay:1.3263670541301985e-08\n",
      "val acc:0.11 | lr:5.9428360622578106e-05, weight decay:1.1484562237971185e-05\n",
      "val acc:0.11 | lr:0.0002769989764081566, weight decay:3.241838100663942e-05\n",
      "val acc:0.33 | lr:0.001194620105068132, weight decay:1.1467789128929214e-07\n",
      "val acc:0.17 | lr:5.2777516081999035e-05, weight decay:2.7199851849188975e-08\n",
      "val acc:0.69 | lr:0.007363907657025434, weight decay:4.113914192615551e-06\n",
      "val acc:0.6 | lr:0.003065474427247241, weight decay:8.01360282879633e-08\n",
      "val acc:0.15 | lr:0.0003044142587442278, weight decay:1.0748776101663653e-08\n",
      "val acc:0.1 | lr:8.950032035675227e-06, weight decay:3.420751920630735e-07\n",
      "val acc:0.81 | lr:0.006015271767676504, weight decay:1.2437297089774085e-06\n",
      "val acc:0.07 | lr:8.203895127090623e-06, weight decay:9.4122464309503e-08\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter 무작위 탐색\n",
    "optimization_trial = 100 # 100번 시도\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "\n",
    "for _ in range(optimization_trial):\n",
    "    # 탐색한 hyperparameter 범위 지정\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4) # weight decay의 값 범위 지정\n",
    "    lr = 10 ** np.random.uniform(-6, -2) # learning rate의 값 범위 지정\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Hyper-Parameter Optimization Result ===========\n",
      "Best-1(val acc:0.21666666666666667) | lr:0.002136607345722235, weight decay:6.989964703719477e-05\n",
      "Best-2(val acc:0.2) | lr:0.00026697338402540484, weight decay:2.8231344268370967e-06\n",
      "Best-3(val acc:0.18333333333333332) | lr:2.0748334394750932e-05, weight decay:1.3267261790301825e-07\n",
      "Best-4(val acc:0.16666666666666666) | lr:0.000763347555854404, weight decay:2.992099694019752e-08\n",
      "Best-5(val acc:0.16666666666666666) | lr:0.0007082352007704818, weight decay:1.3016784006933229e-05\n",
      "Best-6(val acc:0.16666666666666666) | lr:0.001919702618873146, weight decay:2.1077937325467582e-07\n",
      "Best-7(val acc:0.16666666666666666) | lr:0.006374972469953048, weight decay:3.235118657737735e-06\n",
      "Best-8(val acc:0.16666666666666666) | lr:2.069845031275795e-06, weight decay:2.8428444489931127e-08\n",
      "Best-9(val acc:0.15) | lr:2.5844078528463455e-06, weight decay:8.168184509744596e-06\n",
      "Best-10(val acc:0.15) | lr:7.915701095744678e-06, weight decay:4.596106875750513e-08\n",
      "Best-11(val acc:0.15) | lr:0.007233390383054609, weight decay:2.1829075515483148e-06\n",
      "Best-12(val acc:0.15) | lr:0.0006253863296376102, weight decay:2.0799417459903605e-07\n",
      "Best-13(val acc:0.15) | lr:0.006103261674195909, weight decay:7.228066846859633e-05\n",
      "Best-14(val acc:0.15) | lr:6.878127408911018e-05, weight decay:9.621033948037406e-05\n",
      "Best-15(val acc:0.15) | lr:2.1794023594083516e-06, weight decay:7.114974896301335e-06\n",
      "Best-16(val acc:0.13333333333333333) | lr:1.6763939305854842e-06, weight decay:1.1640056365702936e-07\n",
      "Best-17(val acc:0.13333333333333333) | lr:2.761286201158887e-05, weight decay:6.514853073707674e-07\n",
      "Best-18(val acc:0.13333333333333333) | lr:5.339643477789561e-06, weight decay:4.515082651856349e-06\n",
      "Best-19(val acc:0.13333333333333333) | lr:6.364420476476964e-05, weight decay:5.259131981652034e-07\n",
      "Best-20(val acc:0.13333333333333333) | lr:7.59948271038311e-05, weight decay:9.623570248670122e-08\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD9CAYAAABHnDf0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X2QFPW97/H31wWzsIs8LSCPrgaUSMSH5XIQNXIPGlGETQxRQYykJGgd0ejREjAaE6qih9x4PDdK5EKOEalKhINV54APoeQaribGhOUECYsBVxRZRFxleVhACPC9f3SD4+zsTu9u787QfF5VXUz37ze/hy873+7p6Z4xd0dERJLllFwPQERE4qfkLiKSQEruIiIJpOQuIpJASu4iIgmk5C4ikkBK7iIiCXRCJHcze9/MDphZnZnVmtmLZtY/hjavyFLnejN7w8z2m9mqlvQXtxzG5Gdm9o6Z7TWzv5nZd1rSZ5xyGJOfmtlWM9tjZlvM7Act6TNuuYpLSt1uZlZjZr9vSZ9xyuHfyjNmdijs99hS0JJ+G3JCJPfQOHcvBnoDO4An2qDPncC/Af/SBn01Ry5isg8YB3QGbgH+t5mNbIN+o8pFTP4dGOzupwEjgUlmdl0b9NsUuYjLMXOAt9uwv6hyFZOfuntxynKkNTo5kZI7AO7+GbAUOBfAzL4UHk1+YGY7zGyemXUIy0rM7AUz22VmO83sdTM7xcwWAQOA5eGe8/4G+lrp7kuAD9toes3SxjF52N3/5u5H3f1PwOvAxW0z0+jaOCYb3X1fyqajwMDWnWHztGVcwjYuBr4K/Kr1Z9c8bR2TtnLCJXcz6wjcALwZbpoDnA1cQPCC6gv8MCy7F6gGegC9gAcAd/ebgQ8I99zu/tO2m0H8chWT8A/+fwCV8c0mHm0dEzObaWZ1YTtFwK9jn1QM2jIu4emGucB0IG+/5yQHr59/CncMa8zsW7FP6Bh3z/sFeB+oA3YBhwmOpM8DjOA0wZdT6l4MvBc+ng38FzCwgTaviNj/VGBVruOQTzEJ6y8EfgtYruORDzEJ+7kQ+DHQKdfxyHVcgHuAp8LHU4Df5zoWeRCTi4DuQDvgGmAvcEmrzDHXQW7Cf8QV4eMC4DqC8+EDCI4IdqUsu4G6sG4n4DFgc7jMbOg/ApgX/mfXAQ+k9Z+vyT2XMflfwBrgtFzHIl9iklJnJvCvuY5HLuMC9AHeA7qF5VPIv+SeD38r84DHWmWOuQ5yU/8jUrbVANcD+4G+EdoYAnwMjA7X30tvs5Hn5nVyb+uYEByZrge65zoO+RKTtDYeBP4r1/HIZVyAbwCfAR+Fy27gUPi44GSMSQNtPEUrHQiciOfczczKga4E53oXAI+bWc+wvK+ZXRU+vtbMBpqZAXuAI+ECwafjZ2Xpq8DMCgneQp1iZoVm1r5VJtYCbRyTWcAk4Ep3/7RVJhSDtopJ+GHabWbWNexzOHAH8H9bbXIt0IZ/Ky8DpQTnrS8gOGf9F+ACb6WrQ5qrjV8/E8ysOPy7+TowGVjWKhPL9R60CXvZAwRvb/YSHDXeFJYVAo8QvEXaQ3DJ1V1h2T3hc/cRfAjyUEqb5QQfgOwC7mug3ykEb9FSl2dyHY8cx8SBg3z+drPBt5wnQ0wILkr4LcFb+jpgE8Fpibz4HCKXfytpY5hC/p2WycXr53WCdzF7gLeAG1trjhZ2KCIiCXLCnZYREZHssiZ3M3vazD42s/UNlJuZ/dzMqsxsnZldFP8wRUSkKaIcuT8DjGmk/GpgULhMI/j0V0REcihrcnf31wg+LGpIOfCsB94EuphZ77gGKCIiTdcuhjb6AltT1qvDbdvTK5rZNIKje4qKisoGDx4cQ/f5bc2aNZ+4e48odUtKSry0tLSVR5R7TYkJnBxxUUwy0+unvqgxiSO5W4ZtGS/Bcff5wHyAYcOGeUVFRQzd5zcz2xK1bmlpKYpJfSdDXBSTzPT6qS9qTOK4WqYaSP0e5H7k+bcoiogkXRzJfRnwnfCqmRHAbnevd0pGRETaTtbTMmb2G2AUUGJm1cDDQHsAd58HvETw7WZVBN/J8N3WGqyIiESTNbm7+8Qs5U7wXRoiIpIndIeqiEgCKbmLiCSQkruISAIpuYuIJJCSu4hIAim5i4gkkJK7iEgCKbmLiCSQkruISAIpuYuIJJCSu4hIAim5i4gkkJK7iEgCKbmLiCSQkruISAJFSu5mNsbMNppZlZnNzFA+xcxqzGxtuEyNf6giIhJVlF9iKgDmAlcS/F7qajNb5u4b0qoudvfprTBGERFpoihH7sOBKnff7O6HgOeA8tYdloiItESU5N4X2JqyXh1uS/ctM1tnZkvNrH+mhsxsmplVmFlFTU1NM4YrIiJRREnulmGbp60vB0rdfSiwEliYqSF3n+/uw9x9WI8ePZo2UhERiSxKcq8GUo/E+wEfplZw90/d/WC4ugAoi2d4IiLSHFGS+2pgkJmdaWanAjcCy1IrmFnvlNXxwNvxDVFERJoq69Uy7n7YzKYDK4AC4Gl3rzSz2UCFuy8D7jKz8cBhYCcwpRXHLCIiWWRN7gDu/hLwUtq2H6Y8ngXMindoIiLSXLpDVUQkgZTcRUQSSMldRCSBlNxFRBJIyV1EJIGU3EVEEkjJXUQkgZTcRUQSSMldRCSBlNxFRBJIyV1EJIGU3EVEEkjJXUQkgU6I5F5aWkqHDh0oLi6ma9eujB07lq1bt2Z/YpY2V65cmbXeypUrueiiiygqKqJ///4sWbKkRf3GJVcxGTJkCMXFxceXdu3aMW7cuBb1G5dcxWTnzp3ccMMNlJSUUFJSwk033cSePXta1G9cchWTbdu2UV5eTrdu3ejXrx/z5s1rUZ8tlas4LFmyhJEjR9KxY0dGjRpVr3zt2rWUlZXRsWNHysrKWLt2bYvGlCpScjezMWa20cyqzGxmhvIvmdnisPxPZlYa2whDy5cvp66uju3bt9OrVy/uvPPOuLuoZ8OGDUyaNImf/OQn7N69+/h/RL7IRUwqKyupq6ujrq6OvXv3MmDAAL797W+3er9R5SImDz74ILW1tWzevJl3332XHTt28KMf/ajV+40qFzGZPHkyZ555Jjt27ODFF1/kgQce4He/+12r99uYXMShW7du3H333cycWS9tcujQIcrLy5k8eTK1tbXccsstlJeXc+jQoXg6d/dGF4If6HgXOAs4FXgLODetzj8B88LHNwKLs7VbVlbmUZ1xxhn+yiuvHF9/8cUXfdCgQe7u/tlnn/m9997r/fv39549e/ptt93m+/fvd3f3mpoaHzt2rHfu3Nm7du3ql156qR85csQnT57sZuaFhYVeVFTkc+bMydjvxIkT/cEHH4w8zkwIftAka5z9BIlJqlWrVnlRUZHX1dVFHrd702LiTYhLrmIyZswYnzt37vH1J5980r/+9a+ftDHZu3evA/7xxx8f3/a9733PJ0+e3KSYNDUujcUk16+XBQsW+OWXX/6FbStWrPA+ffr40aNHj2/r37+/v/zyy7HEJMqR+3Cgyt03u/sh4DmgPK1OOZ//KPZSYLSZZfph7Rbbv38/ixcvZsSIEQDMmDGDTZs2sXbtWqqqqti2bRuzZ88G4LHHHqNfv37U1NSwY8cOHnnkEcyMRYsWMWDAgON78vvvvz9jX2+++SYA5513Hr1792by5Mns3LmzNabVIm0Zk1QLFy5kwoQJFBUVter8mqMtY3LHHXfwwgsvUFtbS21tLc8//zxXX311m801qraKSZB/Pv/32OP169e3wSyzy9XrJV1lZSVDhw4lNVUOHTqUysrKeCaaLfsDE4BfpqzfDDyZVmc90C9l/V2gpLF2m3qUWlRU5J07d/aCggLv3bu3r1u3zo8ePeodO3b0qqqq43XfeOMNLy0tdXf3hx56yMePH+/vvPNOxjZT9+SZtG/f3s844wzfuHGj792716+77jqfNGlS5HG7t+6Rey5icsy+ffu8U6dO/rvf/S7ymI9pSky8iUepuYjJtm3bfPTo0W5mbmZ+xRVX+MGDByON+ZikxeSSSy7x6dOn+4EDB3zNmjXetWtXP/vssyONOVVcr59cv14yHbnPnj3bb7jhhi9smzRpkj/88MONthU1JuYpe9dMzOzbwFXuPjVcvxkY7u53ptSpDOtUh+vvhnU+TWtrGjAtXD0H2BhxH3Qe8D6wN1zvApQCG8KyI+nDBv5C8JlCH6BruL0G+KiBNgcA3cPH28N6FwA7wnWAjsDZQFM+9TjD3XtEqWhmNcCWiO3mKibHdAP6An+NON5UkWMCTYpLrmIyGNgPVIfb+xH8hOXmCGM+JmkxOTXcXgQcBPYBHYBNEcacKq7XT65fLyVhWWrO6wmcBlSlbBsYtrejgXlA1Jhky/7AxcCKlPVZwKy0OiuAi8PH7YBPINhxxLGEAbwibVsNcD3Bi6pvhDaGAB8Do8P199LbzPCc14EfpqyXAbVxzetEjEnKc18BZuc6DvkQE6AOOD9l/QKgLtfxyIe/k5Q2fg08erLGAZgKrErb9nWCAwJL2bYFGBPHnKOcc18NDDKzM83sVIIPTJel1VkG3BI+ngC86uFI42aBcoI9aSWwAHjczHqG5X3N7Krw8bVmNjA8/7+HYO98bA+9g+BD4sb8CviumZ1lZh2BGcALsU+qhdo4JphZP+B/8vnnLHmnjWOyGphqZh3MrAPBu9O3Yp9UC7VlTMzsK2bWycxONbPJBInsX1tlYk3UxnEoMLNCgoPeU8ys0Mzah8WrwrbuCq84nB5ufzWWiUbc61xD8HbqXeAH4bbZwPjwcSHwHwRvL/4MnNUKe90DBEdIewnO8d+U0vcjBG+B9wBvA3eFZfeEz91HsId8KKXNcuADYBdwXyN9/5hgD18DLAK65uroI49iMgt4PdcxyJeYAGcCy4FPgZ3Ab4FBuY5HjmNyd/ia2Qf8Hhh2ksZhCuBpyzMp5RcCa8Kx/TdwYVxzznrOXURETjwnxB2qIiLSNFmTu5k9bWYfm1nGi1TD81c/D+9OXWdmF8U/TBERaYooR+7PAGMaKb8aGBQu04CnWj4sERFpiazJ3d1fI/iQqCHlwLMeeBPoYma94xqgiIg0XbsY2ugLpH69WnW4bXt6xdSbmIqKisoGDx4cQ/f5bc2aNZ94xJswSkpKvLS0tJVHlHtNiQmcHHFRTDLT66e+qDGJI7ln+g6ZjJfguPt8YD7AsGHDvKKiIobu85uZRb3jlNLSUhST+k6GuCgmmen1U1/UmMRxtUw10D9lvR/wYQztiohIM8WR3JcB3wmvmhkB7Hb3eqdkRESk7WQ9LWNmvwFGASVmVg08DLQHcPd5wEsEd7BWEXxHw3dba7AiIhJN1uTu7hOzlDtwR2wjEhGRFtMdqiIiCaTkLiKSQEruIiIJpOQuIpJASu4iIgmk5C4ikkBK7iIiCaTkLiKSQEruIiIJpOQuIpJASu4iIgmk5C4ikkBK7iIiCaTkLiKSQEruIiIJFCm5m9kYM9toZlVmNjND+RQzqzGzteEyNf6hiohIVFF+iakAmAtcSfB7qavNbJm7b0irutjdp7fCGEVEpImiHLkPB6rcfbO7HwKeA8pbd1giItISUZJ7X2Brynp1uC3dt8xsnZktNbP+mRoys2lmVmFmFTU1Nc0YroiIRBEluVuGbZ62vhwodfehwEpgYaaG3H2+uw9z92E9evRo2khFRCSyKMm9Gkg9Eu8HfJhawd0/dfeD4eoCoCye4YmISHNESe6rgUFmdqaZnQrcCCxLrWBmvVNWxwNvxzdEERFpqqxXy7j7YTObDqwACoCn3b3SzGYDFe6+DLjLzMYDh4GdwJRWHLOIiGSRNbkDuPtLwEtp236Y8ngWMCveoYmISHPpDlURkQRSchcRSSAldxGRBFJyFxFJICV3EZEEUnIXEUkgJXcRkQRSchcRSSAldxGRBFJyFxFJICV3EZEEUnIXEUkgJXcRkQTKy+ReWlpKhw4dKC4upmvXrowdO5atW7dmf2KWNleuXNlonSVLljBy5Eg6duzIqFGj6pVPmzaNc845h1NOOYVnnnmmReNpqnyMyaZNmygvL6dHjx5069aNq666io0bN7ZoTE2RjzH55JNPuOSSS+jevTtdunTh4osv5g9/+EOLxtRU+RiXVAsXLsTM+OUvf9miMTVFvsbEzCgqKqK4uJji4mKmTp3aojGlipTczWyMmW00syozm5mh/Etmtjgs/5OZlbZ0YMuXL6euro7t27fTq1cv7rzzzpY2mVW3bt24++67mTmz3hQBOP/88/nFL37BRRdd1OpjySTfYrJr1y7Gjx/Pxo0b2bFjB8OHD6e8vG1/Oz3fYlJcXMzTTz9NTU0NtbW1zJgxg3HjxnH48OFWH1eqfIvLMbW1tTz66KMMGTKk1ceTLl9j8tZbb1FXV0ddXV2sO7ysyd3MCoC5wNXAucBEMzs3rdqtQK27DwQeB+bENcDCwkImTJjAhg0bADh48CD33XcfAwYMoFevXtx+++0cOHAACI6arr32Wrp06UK3bt247LLLOHr0KDfffDMffPAB48aNo7i4mJ/+9KcZ+7riiiu4/vrr6dOnT8byO+64g9GjR1NYWBjX9JolX2IyfPhwbr31Vrp160b79u2555572LhxI59++mnrTb4B+RKTwsLC4+/u3J2CggJqa2vZuXNn602+EfkSl2NmzZrFXXfdRUlJSfyTjSjfYtJaohy5Dweq3H2zux8CngPSD8/K+fxHsZcCo80s0w9rN9n+/ftZvHgxI0aMAGDGjBls2rSJtWvXUlVVxbZt25g9ezYAjz32GP369aOmpoYdO3bwyCOPYGYsWrSIAQMGHN9z33///XEMLWfyNSavvfYap59+Ot27d29xW02VbzEZOnQohYWFjB8/nqlTp9KzZ89Y5tlU+RSXP//5z1RUVHD77bfHNr/myKeYAHzta1/j9NNP57rrruP999+PY4oAmLs3XsFsAjDG3aeG6zcD/+Du01PqrA/rVIfr74Z1PklraxowLVw9B2joBO15BL8S5QQ/7fd34B3gAHAhsAE49oPcRcBZwF+BPkAHgh/1PvjFJjkPeB/Y2+iEAyVA90bGdw7wCRDlEPUMd+8RoR5mVgNsaaA432PSHvhK2E+2w9TIMYFG45LvMTGga/hvtr+VuGIC+RuXrwAfAPsIXkOfEryOGpP0108xQTxOAfoCnYDKLG1Fi4m7N7oA3wZ+mbJ+M/BEWp1KoF/K+rtA92xtN9Ln+8AV4eMC4DqChDGA4D9nV8qyG6gL63YCHgM2h8vMTG2G6/OAunB5IK3/qcCqRsb3e2BKc+eXtJgAPQheHD9QTDKO823g/JM5LsCdBL+/fGx9FTD1ZI5JhjEWECT682KZc4SgXAysSFmfBcxKq7MCuDh83I5gb2xx/EekbKsBrgf2A30jtDEE+BgYHa6/l95mI8/N6+SeTzEhODL9C/AvbRmPfI5JhnpVwDdP5rgA/wnUAh+FyyGCJPrkyRqTDHUKCHYMQ+OYc5Rz7quBQWZ2ppmdCtwILEurswy4JXw8AXjVw9G2lAXKCZJIJbAAeNzMeoblfc3sqvDxtWY2MDzfvwc4Ei4AOwjeajXWV4GZFRLsoE4xs0Iza59SfmpYbkD7sLzNLyfNl5iY2WkEO/Y/uHvDlwO0gTyKyQgzuzT8W+lgZjOAXsCfYp90BPkSF2AKwWmZC8KlAvgx8IPYJhtRvsTEzIaY2QVhnWKCdwjbCN7ptVzEvc41wCaC0y0/CLfNBsaHjwuB/yA4QvkzcFYMe9kDBHuxvcB64KaUvh4heIu0JwzEXWHZPeFz9xGcI3sopc1ygvN9u4D7Guh3CsFbtNTlmZTyVRnKR7XhkUdexYRgh+5h23Upy4CTOCaXA2+F49kJ/D/ga20Rj3yOS4a6q2j70zJ5FRPgHwnOwe8jeEfwn8CguOac9QNVERE58eTlHaoiItIyUW5ietrMPg4vd8xUbmb28/Du1HVmlpvbN0VE5LgoR+7PAGMaKb8aGBQu04CnWj4sERFpiazJ3d1fo/GbUsqBZz3wJtDFzHrHNUAREWm6djG00RdI/Xq16nDb9vSKqXeoFhUVlQ0ePDiG7vPbmjVrPvGId9iVlJR4aWlpK48o95oSEzg54qKYZKbXT31RYxJHcs/0HTIZL8Fx9/nAfIBhw4Z5RUVFDN3nNzNr6HboekpLS1FM6jsZ4qKYZKbXT31RYxLH1TLVQP+U9X7AhzG0KyIizRRHcl8GfCe8amYEsNvd652SERGRtpP1tIyZ/QYYBZSYWTXwMME3AOLu84CXCO5grSL4jobvttZgRUQkmqzJ3d0nZil34I7YRiQiIi2mO1RFRBJIyV1EJIGU3EVEEkjJXUQkgZTcRUQSSMldRCSBlNxFRBJIyV1EJIGU3EVEEkjJXUQkgZTcRUQSSMldRCSBlNxFRBJIyV1EJIEiJXczG2NmG82sysxmZiifYmY1ZrY2XKbGP1QREYkqyo91FABzgSsJflJvtZktc/cNaVUXu/v0VhijiIg0UZQj9+FAlbtvdvdDwHNAeesOS0REWiJKcu8LbE1Zrw63pfuWma0zs6Vm1j9DOWY2zcwqzKyipqamGcMVEZEooiR3y7DN09aXA6XuPhRYCSzM1JC7z3f3Ye4+rEePHk0bqYiIRBYluVcDqUfi/YAPUyu4+6fufjBcXQCUxTM8ERFpjijJfTUwyMzONLNTgRuBZakVzKx3yup44O34higiIk2V9WoZdz9sZtOBFUAB8LS7V5rZbKDC3ZcBd5nZeOAwsBOY0opjFhGRLLImdwB3fwl4KW3bD1MezwJmxTs0ERFpLt2hKiKSQEruIiIJpOQuIpJASu4iIgmk5C4ikkBK7iIiCaTkLiKSQEruIiIJpOQuIpJASu4iIgmk5C4ikkBK7iIiCaTkLiKSQHmZ3EtLS+nQoQPFxcV07dqVsWPHsnXr1uxPzNLmypUrG62zZMkSRo4cSceOHRk1alS98iNHjvDggw/Sp08fOnXqxIUXXsiuXbtaNK6o8jEmr7/+OsXFxV9YzIznn3++ReOKKh9jAvDqq69y0UUXcdppp3HWWWcxf/78Fo2pqfI1LsuXL+erX/0qxcXFjBw5kg0bNrRoTE2Rq5jcd999DBo0iE6dOjF48GCeffbZL5SvXbuWsrIyOnbsSFlZGWvXrm3RmFLlZXKH4A+hrq6O7du306tXL+68885W77Nbt27cfffdzJw5M2P5ww8/zBtvvMEf//hH9uzZw6JFiygsLGz1cR2TbzG57LLLqKurO7688MILFBcXM2bMmFYf1zH5FpO///3vfPOb3+S2225j9+7dLF68mH/+53/mrbfeavVxpcq3uLzzzjvcdNNNzJs3j127djFu3DjGjx/P4cOHW31cx+QiJkVFRSxfvpzdu3ezcOFCvv/97/PGG28AcOjQIcrLy5k8eTK1tbXccsstlJeXc+jQoXg6d/esCzAG2AhUATMzlH8JWByW/4ng91QbbbOsrMwbcsYZZ/grr7xyfP3FF1/0QYMGubv7Z5995vfee6/379/fe/bs6bfddpvv37/f3d1ramp87Nix3rlzZ+/atatfeumlfuTIEZ88ebKbmRcWFnpRUZHPmTOnwb7d3RcsWOCXX375F7bt3LnTi4qKvKqqqtHnpiP4QZNIcT7RYpJuypQpPmXKlEbruDctJt5IXPIxJh999JEDvm/fvuPbhg0b5r/+9a/bJCb5GpcnnnjCr7nmmuPrR44c8cLCQl+5cmVsccnnmBwzbtw4/9nPfubu7itWrPA+ffr40aNHj5f379/fX3755VhikvXI3cwKgLnA1cC5wEQzOzet2q1ArbsPBB4H5rRwn3Pc/v37Wbx4MSNGjABgxowZbNq0ibVr11JVVcW2bduYPXs2AI899hj9+vWjpqaGHTt28Mgjj2BmLFq0iAEDBhzfc99///1NHsdf//pX2rVrx9KlSzn99NM5++yzmTt3blzTbJJ8iUn6mJYuXcott9zS4vk1t/98iEmvXr2YOHEiv/rVrzhy5Ah//OMf2bJlC5deemms840qX+JyLOGkr69fvz6eiTZBrmJy4MABVq9ezZAhQwCorKxk6NChmNnxOkOHDqWysjKWeUb5JabhQJW7bwYws+eAciD1hFk58KPw8VLgSTMzT/3fbKJvfOMbtGvXjrq6Onr27MmKFStwdxYsWMC6devo1q0bAA888ACTJk3i0UcfpX379mzfvp0tW7YwcOBALrvssuZ2X091dTW7d+9m06ZNvPfee7zzzjuMHj2as88+myuvvDK2fhqTbzFJ9fzzz1NSUsLll1/eKu03JB9jMnHiRKZOncr3v/99AJ566in69++f5Vnxyre4XHnllcycOZNVq1YxcuRI5syZw6FDh9i/f39sfWST65jcfvvtnH/++Vx11VUA1NXV0blz5y/U6dy5M3v37m3+JFNYtvxrZhOAMe4+NVy/GfgHd5+eUmd9WKc6XH83rPNJWlvTgGnh6jkEp3oyOQ94Hzg2yy5AKcEO5TzgSPowgb8QfIbQB+gabq8BPmqgzQFA9/Dx9pR6ACVhWer4ugBfBv4KHDspduwV29gnM2e4e49Gyj+fhFkNsKWB4nyMSaqzgTrgwwbKU0WOCTQal3yMSSHwFeBdYA/BKctBBH8juxuZZlwxyTSHfIgLYbt9gPbAp8Bp4XN3NjAPSM7rpx/QiSAmR8NtPQliUJVSb2DY3o4G5gFRY5LtvA3wbeCXKes3A0+k1akE+qWsvwt0j3JeqIE+3weuSNtWA1wP7Af6RmhjCPAxMDpcfy+9zUaeOxVYlbbty4ADA1K2PQE83tx5nugxSSnrT/Dj6F9ui1jkc0yACcBf0rb9G/DkyRyXDHW6ECSxwUmPCfBjYH16TgS+DlQTHmSH27YQHCi3eM5Rrpap5vMjVAj2QOlHZ8frmFk7oDON740js0A5wZ6zElgAPG5mPcPyvmZ2Vfj4WjMbaMFJrD0Ee+Nje+QdwFlZ+iows0KC01WnmFmhmbUHcPd3gdeBH5jZl8zsK8ANwAtxzLMp8iUmKW4G3ghjlBN5FJO/AIPM7B/DMX0ZuBZo28tlPh9rvsQFMysL6/Q9jubjAAAFMUlEQVQA/g+w3N3/FuuEI2jjmMwCJgFXuvunacWrwrbuCnPKsbMhr7Z0jkCkI/d2wGbgTOBUgj/SIWl17gDmhY9vBJbEsJc9QPA2fy/BXu+msKwQeCQc0x7gbeCusOye8Ln7CHY4D6W0WQ58AOwC7mug3ykER+epyzMp5X2B34bj2gzc1hZHHfkck7DO34Bb2yoW+R4TgqPB9eGYqgkuMDhFceH34Xh2EiT3opMgJg4cDPs9tjyQUn4hsCYc238DF8Y156zn3AHM7BqCt5YFwNPu/hMzm01wSc6ycG+9KBzoTuBGDz+AFRGRthcpuYuIyIklynXuT5vZx+EVMZnKzcx+bmZVZrbOzC6Kf5giItIUUT5QfYbgDtWGXE1wqdcggsscn2r5sEREpCWyJnd3f43Gr3wpB571wJtAFzPrHdcARUSk6eL44rC+fPEmnupwm4iI5EiUrx/IxjJsy/gpbeodqkVFRWWDBw+Oofv8tmbNmk884h12JSUlXlpa2sojyr2mxAROjrgoJpnp9VNf1JjEkdyj3OQEgLvPB+YDDBs2zCsqKmLoPr+ZWUO3Q9dTWlqKYlLfyRAXxSQzvX7qixqTOE7LLAO+E141MwLY7e7bY2hXRESaKeuRu5n9BhgFlJhZNfAwwRf/4O7zgJeAawi+/GY/8N3WGqyIiESTNbm7+8Qs5U7w9QMiIpIn8vZn9kREpPmU3EVEEkjJXUQkgZTcRUQSSMldRCSBlNxFRBJIyV1EJIGU3EVEEkjJXUQkgZTcRUQSSMldRCSBlNxFRBJIyV1EJIGU3EVEEkjJXUQkgSIldzMbY2YbzazKzGZmKJ9iZjVmtjZcpsY/VBERiSrKLzEVAHOBKwl+L3W1mS1z9w1pVRe7+/RWGKOIiDRRlCP34UCVu29290PAc0B56w5LRERaIkpy7wtsTVmvDrel+5aZrTOzpWbWP1NDZjbNzCrMrKKmpqYZwxURkSiiJHfLsM3T1pcDpe4+FFgJLMzUkLvPd/dh7j6sR48eTRupiIhEFiW5VwOpR+L9gA9TK7j7p+5+MFxdAJTFMzwREWmOKMl9NTDIzM40s1OBG4FlqRXMrHfK6njg7fiGKCIiTZX1ahl3P2xm04EVQAHwtLtXmtlsoMLdlwF3mdl44DCwE5jSimMWEZEssiZ3AHd/CXgpbdsPUx7PAmbFOzQREWku3aEqIpJASu4iIgmk5C4ikkBK7iIiCaTkLiKSQEruIiIJpOQuIpJASu4iIgmk5C4ikkBK7iIiCaTkLiKSQEruIiIJpOQuIpJASu4iIgmk5C4ikkCRkruZjTGzjWZWZWYzM5R/ycwWh+V/MrPSuAcqIiLRZU3uZlYAzAWuBs4FJprZuWnVbgVq3X0g8DgwJ+6BiohIdFGO3IcDVe6+2d0PAc8B5Wl1yoGF4eOlwGgzs/iGKSIiTRHlZ/b6AltT1quBf2ioTvibq7uB7sAnqZXMbBowLVytM7ONzRn0CeaMqBXXrFnziZltac3B5InIMYGTJi6KSWZ6/dQXKSZRknumI3BvRh3cfT4wP0KfJyV375HrMeQjxaU+xaQ+xeSLopyWqQb6p6z3Az5sqI6ZtQM6AzvjGKCIiDRdlOS+GhhkZmea2anAjcCytDrLgFvCxxOAV9293pG7iIi0jaynZcJz6NOBFUAB8LS7V5rZbKDC3ZcB/w4sMrMqgiP2G1tz0CIi0jjTAbaISPLoDlURkQRSchcRSSAldxGRBFJyFxFJICV3EZEEUnIXEUkgJXcRkQT6/29u77jsJIKZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
