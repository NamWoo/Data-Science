<!DOCTYPE html>
  <html>
    <head>
      <title>04_course1_week3</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"]}});
        </script>
        <script type="text/javascript" async src="file:///C:\Users\HJY\.atom\packages\markdown-preview-enhanced\node_modules\@shd101wyy\mume\dependencies\mathjax\MathJax.js"></script>
        
      
      
      
      
      
      
      
      
      
      

      <style> 
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
 
      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview   ">
      <h2 class="mume-header" id="neural-network-representation">Neural Network Representation</h2>

<p>이제부터 다룰 neural network의 기본적인 구조는 다음과 같다..<br>
<img src="./week3/1.png" alt="image"></p>
<p>다음의 그림에서 각 열을 레이어라고 부르며 구체적으로,</p>
<ul>
<li><span class="mathjax-exps">$x1, x2, x3$</span> : input layer</li>
<li>중간 레이어 : hidden layer, 각각은 hidden unit에 해당.</li>
<li><span class="mathjax-exps">$&#x5C;hat{y}$</span> : output layer</li>
</ul>
<p>라고 부른다. 또한 위의 그림과 같은 경우를 2 layer neural network라고 부르는데, 일반적으로 input layer는 layer의 수에 포함시키지 않기 때문이다.</p>
<p>neural network는 그림에서 직관적으로 파악할 수 있듯, 화살표가 가리키는 방향으로 값을 전달한다. 따라서 input layer에서는 <span class="mathjax-exps">$x_n$</span>을 hidden layer의 입력값으로 전달한다. 다음으로 이를 전달받아 hidden layer에서 어떠한 함수를 통해 최종적으로 값을 산출하며, 이 값을 <span class="mathjax-exps">$a^&#x5C;text{[1]}$</span>로 표기한다.<br>
<span class="mathjax-exps">$a^&#x5C;text{[1]}$</span>는 다시 output layer의 입력값이 되며, output layer에서 또다른 함수를 통해 최종적으로 <span class="mathjax-exps">$&#x5C;hat{y} = a^&#x5C;text{[2]}$</span>를 산출한다.</p>
<p>또한 hidden layer는 두 개의 파라미터를 가지는데, 가중치 행렬인 <span class="mathjax-exps">$w^&#x5C;text{[1]}$</span> 과 bias <span class="mathjax-exps">$b^&#x5C;text{[1]}$</span>이 있다. 후술하겠지만, 위의 예제에서 가중치 행렬은 (4, 3), bias는 (4, 1) 벡터가 된다.</p>
<p>정리하자면 neural network는 input, hidden, output의 세 종류의 layer로 구분될 수 있으며, 각각의 레이어에서 사용되는 표기법은 다음과 같은 것들이 있다.</p>
<table>
<thead>
<tr>
<th></th>
<th>input layer</th>
<th>hidden layer</th>
<th>output layer</th>
</tr>
</thead>
<tbody>
<tr>
<td>output value</td>
<td><span class="mathjax-exps">$X = a^&#x5C;text{[0]}$</span></td>
<td><span class="mathjax-exps">$a^&#x5C;text{[1]} = &#x5C;begin{bmatrix} a_1^&#x5C;text{[1]}&#x5C;&#x5C;a_2^&#x5C;text{[1]}&#x5C;&#x5C;a_3^&#x5C;text{[1]}&#x5C;&#x5C;a_4^&#x5C;text{[1]} &#x5C;end{bmatrix}$</span></td>
<td><span class="mathjax-exps">$&#x5C;hat{y} = a^&#x5C;text{[2]}$</span></td>
</tr>
<tr>
<td>parameters</td>
<td></td>
<td>weight <span class="mathjax-exps">$w^&#x5C;text{[1]}$</span> 과 bias <span class="mathjax-exps">$b^&#x5C;text{[1]}$</span></td>
<td>weight <span class="mathjax-exps">$w^&#x5C;text{[2]}$</span> 과 bias <span class="mathjax-exps">$b^&#x5C;text{[2]}$</span></td>
</tr>
<tr>
<td>dimensions</td>
<td></td>
<td><span class="mathjax-exps">$w$</span>는 (4, 3), <span class="mathjax-exps">$b$</span>는 (4, 1)</td>
<td><span class="mathjax-exps">$w$</span>는 (1, 4), <span class="mathjax-exps">$b$</span>는 (1, 1)</td>
</tr>
</tbody>
</table>
<h2 class="mume-header" id="computing-a-neural-networks-output">Computing a Neural Network's output</h2>

<p>이제 하나의 unit 별로 어떠한 연산이 이루어지는지를 알아보자! 다음의 예시는 hidden layer에 하나의 unit만이 존재하는 경우 어떻게 결과값을 산출하는지를 나타내는 그래프이다.</p>
<p><img src="./week3/2.png" alt="image"></p>
<p>입력값 <span class="mathjax-exps">$x$</span>는 <span class="mathjax-exps">$z = W^{&#x5C;rm T}x + b$</span>를 거쳐 최종적으로 <span class="mathjax-exps">$a = &#x5C;sigma{(z)} = &#x5C;hat{y}$</span>를 산출한다.</p>
<p>위에서 살펴본 2 layer neural network의 hidden layer의 경우 이를 여러 unit으로 확장한 것에 불과하므로, 다음과 같은 식을 통해 <span class="mathjax-exps">$z_n^{[1]}$</span>와 <span class="mathjax-exps">$a_n^{[1]}$</span>를 산출한다.</p>
<p><img src="./week3/3.jpg" alt="image"></p>
<p>실제 신경망 구현에서는 위의 연산을 반복문을 통해 구현하지 않고, 행렬로 표현하여 다음과 같이 계산한다.<br>
<div class="mathjax-exps">$$z^{[1]} = &#x5C;begin{bmatrix} z_1^{[1]}&#x5C;&#x5C;z_2^{[1]}&#x5C;&#x5C;z_3^{[1]}&#x5C;&#x5C;z_4^{[1]}&#x5C;end{bmatrix}= &#x5C;begin{bmatrix} w_1^{[1]&#x5C;rm T}&#x5C;&#x5C;w_2^{[1]&#x5C;rm T}&#x5C;&#x5C;w_3^{[1]&#x5C;rm T}&#x5C;&#x5C;w_4^{[1]&#x5C;rm T} &#x5C;end{bmatrix} &#x5C;begin{bmatrix} x_1&#x5C;&#x5C;x_2&#x5C;&#x5C;x_3 &#x5C;end{bmatrix} + &#x5C;begin{bmatrix} b_1^{[1]}&#x5C;&#x5C;b_2^{[1]}&#x5C;&#x5C;b_3^{[1]}&#x5C;&#x5C;b_4^{[1]}&#x5C;end{bmatrix}$$</div><br>
<div class="mathjax-exps">$$a^{[1]}=  &#x5C;begin{bmatrix} a_1^{[1]}&#x5C;&#x5C;a_2^{[1]}&#x5C;&#x5C;a_3^{[1]}&#x5C;&#x5C;a_4^{[1]}&#x5C;end{bmatrix} = &#x5C;sigma{(z)}$$</div></p>
<h2 class="mume-header" id="vectorizing-across-multiple-examples">Vectorizing across multiple examples</h2>

<p>i개의 training data가 있을 때의 학습은 다음과 같이 이루어진다.<br>
<div class="mathjax-exps">$$&#x5C;text{for i = 1 to m: }&#x5C;&#x5C; z^{[1](i)} = w^{[1]}x^{(i)} + b^{[1]}&#x5C;&#x5C; a^{[1](i)} = &#x5C;sigma{(z^{[1](i)})}&#x5C;&#x5C; z^{[2](i)} = w^{[2]}a^{[1](i)} + b^{[2]}&#x5C;&#x5C; a^{[2](i)} = &#x5C;sigma{(z^{[2](i)})}$$</div></p>
<p>실제로 구현할 때에는 반복문을 사용하지 않고 다음과 같이 각 training data를 열벡터로 두는 행렬을 만들어 계산하게 된다.<br>
<img src="./week3/4.png" alt="image"></p>
<h2 class="mume-header" id="activation-function">Activation function</h2>

<p>지금까지 다루어왔던 예제에서 회귀를 통해 예측된 <span class="mathjax-exps">$z$</span>는 activation function(활성함수)을 통해 변환된다. 이전의 강의에서는 이 activation function으로 시그모이드 함수를 사용해 왔다. 이번 강의에서는 여러 activation function을 소개하고 있다.</p>
<ol>
<li>tanh</li>
</ol>
<ul>
<li>-1과 +1 사이의 값을 가짐</li>
<li>데이터를 centering하는 역할을 수행함으로써 데이터들의 평균이 0에 근접하도록 한다. 이 경우 다음 레이어에서의 학습이 보다 수월하게 이루어질 수 있다.</li>
</ul>
<ol start="2">
<li>RELU</li>
</ol>
<ul>
<li>z값이 너무 크거나 너무 작으면 시그모이드와 tanh의 값이 1과 -1에 가까워질 수 있다. 즉 기울기가 0에 가까워지며, gradient descent가 느려질 수 있다. 이 단점을 보완하는 활성함수</li>
<li>z과 0보다 작으면 0으로 만들지만, 그렇지 않으면 계속 증가함( = <span class="mathjax-exps">$max(0, z)$</span>)</li>
<li>기울기가 0에 가까워지지 않고 z에 따라 달라지기 때문에, tanh에 비해 학습이 빠르다.</li>
</ul>
<ol start="3">
<li>Leaky ReLU</li>
</ol>
<ul>
<li><span class="mathjax-exps">$z$</span>가 양수인 경우에는 ReLU와 동일하지만</li>
<li><span class="mathjax-exps">$z &lt;0$</span>인 경우</li>
</ul>
<p>강의에서는 결과값이 binary classification인 경우에는 시그모이드 함수를 사용하고, 다른 경우에는 ReLU나 tanh를 사용하라고 권유한다.</p>
<table>
<thead>
<tr>
<th></th>
<th>수식</th>
<th>특징</th>
</tr>
</thead>
<tbody>
<tr>
<td>sigmoid</td>
<td><img src="../week3/5.jpg" width="50%"></td>
<td>* 데이터가 <span class="mathjax-exps">$0&#x5C;le a &#x5C;le 1$</span> 사이의 값을 가짐<br>* binary classification시 사용</td>
</tr>
<tr>
<td>tanh</td>
<td><img src="../week3/6.jpg" width="50%"></td>
<td>* 데이터가 <span class="mathjax-exps">$-1&#x5C;le a &#x5C;le 1$</span> 사이의 값을 가짐<br>*데이터의 평균이 0이 되도록 centering</td>
</tr>
<tr>
<td>ReLU</td>
<td><img src="../week3/7.jpg" width="50%"></td>
<td>* sigmoid와 tanh에 비해 학습이 빠름</td>
</tr>
<tr>
<td>leaky ReLU</td>
<td><img src="../week3/8.jpg" width="50%"></td>
<td>* ReLU와 유사!</td>
</tr>
</tbody>
</table>
<h2 class="mume-header" id="why-do-you-need-non-linear-activation-functions">Why do you need non-linear activation functions?</h2>

<p>활성함수에는 선형 활성함수와 비선형 활성함수가 존재한다. 비선형 활성함수에는 위에서 다룬 sigmoid, ReLU 등이 있으며, 선형 활성함수는 다음과 같이 정의된다. <div class="mathjax-exps">$$g{(z)} = z$$</div><br>
즉 input과 output이 동일한 함수를 선형 활성함수라고 하는데, 둘이 동일하기 때문에 이를 identity function이라고 부른다.</p>
<p>hidden layer에서는 선형 활성함수를 사용해서는 안된다. 간단한 예시를 통해 직관적으로 이해할 수 있다. 만약 각 unit의 활성함수가 선형이라고 하면 다음과 같이 계산될 것이다.</p>
<p>a[1] = w[1] * x[0] + b[1]<br><br>
a[2] = w[2] * a[1] + b[2] = w[2] * ( w[1] * x[0] + b[1] ) + b[2]<br><br>
a[3] = w[3] * a[2] + b[3] = ...<br></p>
<p>이므로 각각의 레이어를 합쳐서 하나의 선형 함수로 표현할 수 있다. 즉 레이어를 쌓는 의미가 없어지기 때문에, 활성함수는 비선형적이어야 한다.</p>
<h2 class="mume-header" id="derivatives-of-activation-functions">Derivatives of activation functions</h2>

<p>역전파 알고리즘을 이용할 때에는 각 활성 함수의 기울기를 구해야 한다. 강의에서는 미분에 대한 자세한 설명은 생략하고, 각각의 도함수가 어떻게 계산될 수 있는지만 설명하고 있기 때문에 이를 표로 정리해보았다.</p>
<table>
<thead>
<tr>
<th></th>
<th>수식</th>
<th>특징</th>
<th>도함수</th>
</tr>
</thead>
<tbody>
<tr>
<td>sigmoid</td>
<td><img src="../week3/5.jpg" width="50%"></td>
<td>* 데이터가 <span class="mathjax-exps">$0&#x5C;le a &#x5C;le 1$</span> 사이의 값을 가짐<br>* binary classification시 사용</td>
<td><span class="mathjax-exps">$g&#x27;{(z)} = g{(z)}(1-g{(z)})$</span></td>
</tr>
<tr>
<td>tanh</td>
<td><img src="../week3/6.jpg" width="50%"></td>
<td>* 데이터가 <span class="mathjax-exps">$-1&#x5C;le a &#x5C;le 1$</span> 사이의 값을 가짐<br>*데이터의 평균이 0이 되도록 centering</td>
<td><span class="mathjax-exps">$g&#x27;{(z)} = 1 - tanh{(z)}^2$</span></td>
</tr>
<tr>
<td>ReLU</td>
<td><img src="../week3/7.jpg" width="50%"></td>
<td>* sigmoid와 tanh에 비해 학습이 빠름</td>
<td><span class="mathjax-exps">$&#x5C;begin{equation} g&#x27;{(z)}  =&#x5C;begin{cases} 0 &#x5C;quad &#x5C;text{if}&#x5C;,&#x5C;, z &lt; 0 &#x5C;&#x5C; 1 &#x5C;quad &#x5C;text{if}&#x5C;,&#x5C;, z &gt; 0 &#x5C;&#x5C; &#x5C;text{undef} &#x5C;quad &#x5C;text{if}&#x5C;,&#x5C;, z == 0 &#x5C;&#x5C; &#x5C;end{cases} &#x5C;end{equation}$</span></td>
</tr>
<tr>
<td>leaky ReLU</td>
<td><img src="../week3/8.jpg" width="50%"></td>
<td>* ReLU와 유사!</td>
<td><span class="mathjax-exps">$&#x5C;begin{equation} g&#x27;{(z)}  =&#x5C;begin{cases} 0 &#x5C;quad &#x5C;text{if}&#x5C;,&#x5C;, z &lt; 0 &#x5C;&#x5C; 1 &#x5C;quad &#x5C;text{if}&#x5C;,&#x5C;, z &#x5C;ge 0  &#x5C;end{cases} &#x5C;end{equation}$</span></td>
</tr>
</tbody>
</table>
<h2 class="mume-header" id="gradient-descent-for-neural-networks">Gradient descent for Neural Networks</h2>

<p>아래의 이미지처럼 로지스틱 회귀의 값을 구했다고 가정하자.</p>
<p><img src="./week3/9.jpg" alt="image"></p>
<p>다음으로 해야할 일은 gradient descent를 이용하여 parameter인 W와 b를 업데이트하는 것이다. 업데이트는 다음을 반복함으로써 이루어진다.</p>
<p><div class="mathjax-exps">$$W := W - &#x5C;alpha * dW &#x5C;&#x5C; b := b - &#x5C;alpha * db$$</div></p>
<p>위에서 a와 z에 대한 도함수를 계산하는 공식이 주어졌기 때문에 실제로 구현할 때에는 이를 식에 대입하기만 하면 된다! 신경망이 확장되더라도 이러한 계산을 반복하기만 하면 된다.</p>
<p><img src="./week3/10.png" alt="image"></p>
<p>그리고 다음의 강의노트는 이를 실제로 구현할 때 어떻게 구현해야하는지를 설명하고 있다(구현할 때 참고하면 되는 내용같아서 따로 설명하지는 않을게요)</p>
<p><img src="./week3/11.png" alt="image"></p>
<h2 class="mume-header" id="random-initialization">Random Initialization</h2>

<p>드디어! shallow neural network의 마지막 강의이다!ㅠㅠ 이번에는 앞서 이용했던 W와 b를 학습할 때 가장 먼저 어떤 값으로 초기화해야 하는가에 대해 논한다.<br>
가장 간단한 초기화 값은 모든 값을 0으로 두는 것이다. 하지만 이 경우 gradient descent를 이용한 학습이 불가능하다.</p>
<p><img src="./week3/12.png" alt="image"></p>
<p>위의 예제는 가중치를 0으로 두었을 때 어떤 결과가 나타나는지를 보여준다. 가중치와 bias가 0인 경우 모든 unit이 동일한 값을 산출하기 때문에, 하나의 unit을 두는 것과 다를바 없는 결과를 낳게 된다.<br>
따라서 가중치를 초기화할 때에는 임의의 값으로 초기화하는 것이 좋은데, 강의에서는 아래의 연산을 통해 가중치를 초기화할 것을 권장한다.</p>
<pre data-role="codeBlock" data-info="r" class="language-r">W_1 <span class="token operator">=</span> np.random.randn<span class="token punctuation">(</span>dim<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>
b_2 <span class="token operator">=</span> np.zeros<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>
</pre><p>가중치에 0.01을 곱하는 이유는 tanh나 시그모이드 함수가 output layer의 활성함수인 경우, z값이 너무 크거나 너무 작은 경우 해당 지점에서의 기울기가 아주 작은 값을 가져 학습이 느려지기 때문이다.<br>
한편 b에 대해서는 앞서 설명한 문제가 발생하지 않기 때문에 0으로 초기화해도 문제가 발생하지 않는다.</p>

      </div>
      
      
    </body>
    
    
    
    
    
    
    
  </html>