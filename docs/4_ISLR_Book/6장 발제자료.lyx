#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% LaTeX에서 한글사용을 위해 사용해야하는 패키지
\usepackage[hangul]{kotex}
\setmainhangulfont{함초롬바탕} % 메인 한글폰트를 지정해줌


% 페이지 스타일 'fancy'를 사용하게 되면, 페이지 윗부분에 줄을 하나 그어줌.
% fancy 스타일을 사용할때 사용할수 있는 옵션은 아래와 같다.
%\lhead{} % 줄 왼쪽 부분에 적을 내용
%\chead{} % 가운데 부분
%\rhead{} % 오른쪽 부분
%=====================================%

%\renewcommand\footrulewidth{0.4pt}  % 페이지번호 위에 직선을그어주는 명령어




%=== 문서의 줄간격을 수정해주는 패키지 ===%
\usepackage{setspace} 
\setstretch{1.5} % 간격 임의로 지정 
% \onehalfspacing  한칸반 간격인것 같음
%=====================================%

\usepackage{tikz} % LaTeX으로 Graphical한 작업을 할때 사용하는 패키지



\usepackage{hyperref} % 하이퍼링크 색깔 바꿔주기 위해

% 아래 옵션을 활성화시키면 하이퍼링크걸린 문자들의 색깔이 red로 변함
%\hypersetup{         
%    colorlinks,
%    citecolor=red,
%    filecolor=red,
%    linkcolor=red,
%    urlcolor=red
%}
\end_preamble
\use_default_options true
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 4cm
\topmargin 3cm
\rightmargin 4cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip bigskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\b}[1]{\beta_{#1}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\a}[1]{\alpha_{#1}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ddd}{\cdots}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conpr}[4]{Pr(#1=#2\sumset#3=#4)}
\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}[2]{Pr(#1=#2)}
\end_inset


\begin_inset FormulaMacro
\newcommand{\shat}[1]{\hat{#1}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\lhat}[1]{\widehat{#1}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\inf}{\infty}
\end_inset


\begin_inset FormulaMacro
\newcommand{\lam}{\lambda}
\end_inset


\begin_inset FormulaMacro
\newcommand{\df}[2]{\dfrac{#1}{#2}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ss}[1]{\left(#1\right)}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mm}[1]{\left\{  #1\right\}  }
\end_inset


\begin_inset FormulaMacro
\newcommand{\ll}[1]{\left[#1\right]}
\end_inset


\begin_inset FormulaMacro
\newcommand{\si}{\sigma}
\end_inset


\begin_inset FormulaMacro
\newcommand{\rr}[1]{\sqrt{#1}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\pdf}[1]{f_{#1}(x)}
\end_inset


\begin_inset FormulaMacro
\newcommand{\pmf}[1]{p_{#1}(x)}
\end_inset


\begin_inset FormulaMacro
\newcommand{\d}{\cdot}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ones}{\,}
\end_inset


\begin_inset FormulaMacro
\newcommand{\twos}{\:}
\end_inset


\begin_inset FormulaMacro
\newcommand{\threes}{\;}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\d}{\delta}
\end_inset


\begin_inset FormulaMacro
\newcommand{\app}{\thickapprox}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ep}{\epsilon}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\lam}{\lambda}
\end_inset


\begin_inset FormulaMacro
\newcommand{\pp}{\propto}
\end_inset


\begin_inset FormulaMacro
\newcommand{\dltkd}{\geq}
\end_inset


\begin_inset FormulaMacro
\newcommand{\dlgk}{\leq}
\end_inset


\begin_inset FormulaMacro
\newcommand{\pro}{\times}
\end_inset


\end_layout

\begin_layout Title

\series bold
\shape smallcaps
\size normal
ybigta science team
\series default

\begin_inset VSpace 0.1cm
\end_inset


\shape default
\size default
 
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "0.5pt"

\end_inset


\begin_inset VSpace 0.4cm
\end_inset


\series bold
\size giant
6.
 Linear Model Selection and Regularization
\series default
\size default

\begin_inset VSpace 0.5cm
\end_inset


\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "2pt"

\end_inset


\begin_inset VSpace 4in
\end_inset


\end_layout

\begin_layout Author
ISL 스터디 신보현
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
전통적인 선형 회귀 모델은 
\begin_inset Formula $Y=\b 0+\b 1X_{1}+\ddd+\b pX_{p}+\ep$
\end_inset

의 관계를 가정하고 Least Sqaures Estimation을 통해서 
\begin_inset Formula $RSS=\sum_{i=1}^{n}(y_{i}-\b 0-\sum_{i=1}^{p}\b jx_{ij})^{2}$
\end_inset

를 최소로 만드는 
\begin_inset Formula $\b 0,\ddd,\b p$
\end_inset

에 대한 estimation인 
\begin_inset Formula $\hat{\b 0},\ddd,\hat{\b p}$
\end_inset

을 찾는다.
 하지만 이러한 선형회귀는 
\begin_inset Formula $Y$
\end_inset

와 
\begin_inset Formula $X$
\end_inset

가 선형이라고 가정을 하기 때문에 실제 
\begin_inset Formula $Y$
\end_inset

와 
\begin_inset Formula $X$
\end_inset

의 관계가 비선형(non-linear)이거나 
\begin_inset Formula $Y$
\end_inset

와 상관 없는 예측변수들이 많을 경우, 
\begin_inset Formula $n\thickapprox p$
\end_inset

 또는 
\begin_inset Formula $n<p$
\end_inset

일 경우, 모델의 성능이 상당히 떨어질 수 있다.
 이러한 LSE의 단점에 대한 대안으로, 이번 6장에서는 크게 3가지 방법(Subset Selection, Shrinkage, Dimension
 Reduction)을 소개한다.
\end_layout

\begin_layout Standard

\series bold
\size larger
6.1 Subset Selection
\end_layout

\begin_layout Standard
변수가 여러 개 있을 때, 그 중 일부만을 예측변수로 사용하기 위해 쓰이는 방법이다.
\end_layout

\begin_layout Standard
\noindent

\series bold
6.1.1 Best Subset Selection
\end_layout

\begin_layout Standard
쉽게 생각하면, 
\begin_inset Formula $p$
\end_inset

개의 변수가 있을 때, 
\begin_inset Formula $2^{p}$
\end_inset

개의 가능한 모델들 중 가장 좋은 모델을 택하는 것이다.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/best subset.PNG
	lyxscale 70
	scale 40

\end_inset


\end_layout

\begin_layout Standard
하지만 계산상의 문제가 있다.
 만약 
\begin_inset Formula $p=10$
\end_inset

, 즉 예측 변수가 10개만 되더라도 고려해야 하는 모델이 
\begin_inset Formula $2^{10}=1024$
\end_inset

개나 된다.
 또한 고려해야하는 모이 많아질수록 test data가 아니라 training data에 더 맞는 모델을 찾을 수가 있어서, 과적합의
 위험도 있다.
\end_layout

\begin_layout Standard

\series bold
6.1.2 Stepwise Selection
\end_layout

\begin_layout Standard

\series bold
Forward Stepwise Selection
\end_layout

\begin_layout Standard
예측 변수들이 없는 null model에서 시작해, 변수를 한 개씩 추가해보면서 가장 큰 추가적인 향상을 보이는 변수를 선택한다.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/forward.PNG
	lyxscale 70
	scale 40

\end_inset


\end_layout

\begin_layout Standard

\series bold
Backward Stepwise Selection
\end_layout

\begin_layout Standard
모든 예측 변수들이 있는 full model에서 시작해, 변수를 한 개씩 제거해보면서 가장 작은 향상을 보이는 변수를 제거한다.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/backward.PNG
	lyxscale 70
	scale 40

\end_inset


\end_layout

\begin_layout Standard
하지만, forward selection과 backward selection는 가능한 모든 
\begin_inset Formula $2^{p}$
\end_inset

개의 모델들 중, 가장 좋은 
\begin_inset Formula $p$
\end_inset

개 중 일부 변수를 포함하는 모델을 만들것이라는 보장은 없다.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/비교.PNG
	lyxscale 70
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
Hybrid Approaches
\end_layout

\begin_layout Standard
forward selection 하면서 backward selection도 동시에 하는 방법이다.
 
\end_layout

\begin_layout Standard

\series bold
6.1.3 Choosing the Optimal Model
\end_layout

\begin_layout Standard
항상 주의해야할 점은 training error가 아니라 test error을 고려해야한다는 것이다.
 왜냐하면 training data에 대한 RSS은 변수가 추가 됨에 따라서 일정하게 작아지고, 결정계수(
\begin_inset Formula $R^{2}$
\end_inset

)도 일정하게 증가하기 때문이다.
 따라서 forward, backward의 (c)단계에서 RSS나 
\begin_inset Formula $R^{2}$
\end_inset

가 아닌 여러 다른 지표들을 이용해서 가장 최적의 모델을 선택한다.
\begin_inset Formula 
\[
RSS=\sum_{i=1}^{n}(y_{i}-\shat y_{i})^{2}
\]

\end_inset


\begin_inset Formula 
\[
R^{2}=1-RSS/TSS
\]

\end_inset


\begin_inset Formula 
\[
C_{p}=\df 1n(RSS+2d\shat{\sigma}^{2})
\]

\end_inset


\begin_inset Formula 
\[
AIC=\df 1{n\shat{\sigma}^{2}}(RSS+2d\shat{\sigma}^{2})
\]

\end_inset


\begin_inset Formula 
\[
BIC=\df 1{n\shat{\sigma}^{2}}(RSS+log(n)d\shat{\sigma}^{2})
\]

\end_inset


\begin_inset Formula 
\[
Adjusted\text{ \ensuremath{R^{2}}}=1-\df{RSS/(n-d-1)}{TSS/(n-1)}
\]

\end_inset

위의 지표들은 어느정도 가정을 필요로하기도 하고, 실제로 
\begin_inset Formula $\shat{\sigma}^{2}$
\end_inset

(오차항 
\begin_inset Formula $\ep$
\end_inset

의 분산에 대한 estimate)을 구하기 힘든 경우도 있을 수 있다.
 이럴 때는 
\begin_inset Formula $Cross\text{ \ensuremath{Validation}}$
\end_inset

을 통해서 test error에 대한 estimate을 구하자!!
\end_layout

\begin_layout Standard

\series bold
\size larger
6.2 Shrinkage Methods
\end_layout

\begin_layout Standard

\series bold
6.2.1 Ridge Regression
\end_layout

\begin_layout Standard
Least Squares에서는 
\begin_inset Formula $\b 0,\ddd,\b p$
\end_inset

에 대한 estimates인 
\begin_inset Formula $\shat{\b 0},\ddd,\shat{\b p}$
\end_inset

를 
\begin_inset Formula $RSS=\sum_{i=1}^{n}(y_{i}-\b 0-\sum_{i=1}^{p}\b jx_{ij})^{2}$
\end_inset

를 최소화하는 방법에 의해 찾았다.
 Ridge regression은 이와 아주 유사한데, 
\begin_inset Formula $RSS$
\end_inset

에 penalty을 더한 
\begin_inset Formula 
\[
RSS_{ridge}=RSS+\lam\sum_{j=1}^{p}\b j^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
을 최소화하며 계수들에 대한 estimates을 찾는다.
\end_layout

\begin_layout Standard
여기서 
\begin_inset Formula $\lam$
\end_inset

은 tuning parameter라고 불리며 계수들을 수축(shrinking)하는 효과를 가져온다.
 만약 
\begin_inset Formula $\lam$
\end_inset

가 크다면, Ridge의 penalty항의 영향력이 커질 것이고, 
\begin_inset Formula $RSS_{ridge}$
\end_inset

를 최소화할때, 
\begin_inset Formula $\lam$
\end_inset

와 곱해져있는 
\begin_inset Formula $\sum_{j=1}^{p}\b j^{2}$
\end_inset

때문에 
\begin_inset Formula $\b j$
\end_inset

을 작게 만들 수밖에 없다.
 반대로, 
\begin_inset Formula $\lam$
\end_inset

가 작다면, Ridge의 penalty항의 영향력이 작아질 것이고, 마침내 
\begin_inset Formula $\lam=0$
\end_inset

이 된다면 원래의 LSE와 동일한 결과를 낼 것이다.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/ridge.PNG
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard

\series bold
6.2.2 The Lasso
\end_layout

\begin_layout Standard
Ridge는 의미가 적은 계수를 정확히 0으로 만드는 것이 아니라 0에 가깝게 만든다.
 그에 따라서 Ridge는 엄밀하게 variable selection을 하지는 않는다.
 이에 대한 대안으로, Lasso는 계수를 정확히 0으로 만듦으로써 variable selection을 한다.
\begin_inset Formula 
\[
RSS_{lasso}=RSS+\lam\sum_{j=1}^{p}|\b j|
\]

\end_inset

Ridge와 Lasso의 형태는 penalty에서만 다르고, 그 이외는 동일하다.
\end_layout

\begin_layout Standard
기억해야할 것!!! Ridge나 Lasso를 시행하기 전에 예측변수들을 표준화하자!! 일반적인 선형회귀는 굳이 표준화를 안 해도
 되지만(scale equivariant하기 때문에) Ridge나 Lasso에서는 penalty의 존재 때문에 scale이 다름으로
 인해서 영향을 받기 때문에 꼭 표준화를 해야 한다!
\begin_inset Formula 
\[
\widetilde{x_{ij}}=\df{x_{ij}}{\rr{\df 1n\sum_{i=1}^{n}(x_{ij}-\bar{x}_{j})^{2}}}
\]

\end_inset


\series bold
Ridge vs Lasso
\end_layout

\begin_layout Standard
아무래도 Lasso는 변수 선택을 하므로 만약 p개의 예측변수가 골고루 반응변수와 관련이 되어 있으면 Lasso보다는 Ridge가
 더 나은 성능을 보이고 p개의 예측변수 중 일부만 반응변수와 관련이 있으면 Lasso가 더 좋은 성능을 보일 것이다.
\end_layout

\begin_layout Standard

\series bold
6.2.3 Selecting the Tuning Parameter
\end_layout

\begin_layout Standard
\noindent
Ridge와 Lasso을 할 때, 적절한 
\begin_inset Formula $\lam$
\end_inset

을 정하는 것이 매우 중요하다.
 적절한 
\begin_inset Formula $\lam$
\end_inset

는 Cross-Validation을 통해서 결정한다.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/람다.PNG
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
\size larger
6.3 Dimension Reduction Methods
\end_layout

\begin_layout Standard
p개의 변수를 가지는 모형을 M( < p )개의 변수를 가지는 모형으로 차원을 축소시키는 방법.
 
\begin_inset Formula 
\[
Y=\b 0+\b 1X_{1}+\b 2X_{2}+\ddd+\b pX_{p}+\ep
\]

\end_inset


\begin_inset Formula 
\[
Transformation\text{ \ensuremath{Z_{m}=\sum_{j=1}^{p}\phi_{jm}X_{j}\text{\ensuremath{} \ensuremath{where\text{ \ensuremath{\phi_{jm}\text{ }are\text{ \ensuremath{constant}}}}}}}}\text{ }and\text{\,\ensuremath{M<p}}
\]

\end_inset


\begin_inset Formula 
\begin{align*}
Y & =\theta_{0}+\theta_{1}Z_{1}+\theta_{2}Z_{2}+\ddd+\theta_{M}Z_{M}+\ep\\
 & =\theta_{0}+\sum_{m=1}^{M}\theta_{m}z_{m}+\ep
\end{align*}

\end_inset

여기서 차원 축소 접근을 원래 모형의 계수들에 대해 제약(constraints)을 두는 것이라고 볼 수 있다.
\begin_inset Formula 
\[
\sum_{m=1}^{M}\theta_{m}z_{m}=\sum_{m=1}^{M}\theta_{m}\sum_{j=1}^{p}\phi_{jm}x_{j}=\sum_{j=1}^{p}\sum_{m=1}^{M}\theta_{m}\phi_{jm}x_{j}=\sum_{j=1}^{p}\b jx_{j}
\]

\end_inset


\begin_inset Formula 
\[
where\text{ \ensuremath{\b j=\sum_{m=1}^{M}\theta_{m}\phi_{jm}}}
\]

\end_inset

이는 bias-variacne trade off 관점에서 잠재적인 bias의 가능성이 있지만 
\begin_inset Formula $M<<p$
\end_inset

인 
\begin_inset Formula $M$
\end_inset

을 고른다면 분산을 상당히 많이 줄여줄 것이다.
\end_layout

\begin_layout Standard

\series bold
An Overview of Principal Components
\end_layout

\begin_layout Standard
아래는 Population과 Ad Spending에 따른 100개의 데이터이다.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/pca1.PNG
	lyxscale 70
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $Z_{1}$
\end_inset

= 초록색 선 = first principal component = 주성분(초록색 선)으로 데이터를 사영(projection =
 가장 짧은 거리로 투영)했을 때, 가장 큰 분산을 가지는 벡터( = 가장 많은 정보를 보존).
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/pca2.PNG
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
첫번째 주성분으로 사영(projection) 후 1차원으로 나타낸 결과.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/pca3.PNG
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
Second Principal Component: 우선, 첫번째 주성분과 관련이 없어야하고( = 첫번째 주성분으로 설명이 된 것에는
 관심이 없음 = 
\begin_inset Formula $Z_{1}$
\end_inset

과 
\begin_inset Formula $Z_{2}$
\end_inset

는 수직[orthogonal]) 가장 큰 분산을 가져야 한다.
\end_layout

\begin_layout Standard
자세한 내용은 10장에서.....ㅎㅎ
\end_layout

\begin_layout Standard

\series bold
6.3.1 Principal Components Regression
\end_layout

\begin_layout Standard
PCR은 위에서 변환된 
\begin_inset Formula $Z_{1},\ddd,Z_{M}$
\end_inset

을 예측변수로써 사용하여 선형회귀를 실시하는 접근법이다.
 선형회귀가 예측변수와 반응변수간에 선형의 관계가 있다고 가정하는 것과 유사하게 PCR도 principal component을 나타내는
 direction이 반응변수와 연관이 있다고 가정한다.
\end_layout

\begin_layout Standard
언제 PCR을 쓰면 좋을까?
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename C:/Users/sbh0613/Desktop/와이빅타/ISL/발제준비/pcr.PNG
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
PCR이 좋은 성능을 보일 때: 처음의 적은 주성분(principal components)로 반응변수의 많은 부분을 설명할 수 있는
 데이터에 PCR을 쓰면 좋다.
 ( = 예측변수들 간의 다중공선성이 나타날 때)
\end_layout

\begin_layout Standard
PCR이 나쁜 성능을 보일 때: 적절한 모델을 만들기 위해 많은 주성분이 필요한 데이터에 대해서는, 굳이 차원을 축소( = 정보의
 손실)를 할 필요가 없다.
 
\end_layout

\begin_layout Standard
기억해야할 것!! PCR을 시행할 때, 표준화를 하는 것이 좋다.
 단위가 다르게 측정이 된 경우, 그 변동성이 주성분을 결정할 때 영향을 미치기 때문이다.
\begin_inset Formula 
\[
\widetilde{x_{ij}}=\df{x_{ij}}{\rr{\df 1n\sum_{i=1}^{n}(x_{ij}-\bar{x}_{j})^{2}}}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
6.3.2 Partial Least Squares
\end_layout

\begin_layout Standard
PCR은 주성분 방향(principal component directions)을 구할 때, 예측변수들만을 이용하여 주성분 방향을
 구한다.
 즉, 원래 주성분이 있고 이것을 학습(supervise)하지 않는다.
 따라서, PCA를 통해서 구한 주성분이 예측변수를 잘 설명한다고 해도(variance가 가장 큰 애들로 구했음) 반응변수를 잘
 설명하리라는 보장은 없다!
\end_layout

\begin_layout Standard
이러한 한계점을 보완하여 나온 것이 Partial Least Squares이다.
 즉, PLS는 주성분을 학습(supervise)을 통하여 도출한다.
 다시 말하면, PLS는 예측변수 뿐만 아니라 반응변수까지 고려하여 이것들을 가장 잘 설명하는 주성분 방향을 구한다.
\end_layout

\begin_layout Standard

\series bold
\size larger
간단한 실습
\end_layout

\begin_layout Standard
Hitters 데이터에 대해서 lasso와 ridge을 이용해서 Hitters 데이터의 Salary 변수를 예측해본다.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

library(ISLR)
\end_layout

\begin_layout Plain Layout

hitters = na.omit(Hitters)
\end_layout

\begin_layout Plain Layout

x=model.matrix(Salary~.
 , hitters)[,-1] # matrix 형태
\end_layout

\begin_layout Plain Layout

y=hitters$Salary # 백터 형태
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# ridge와 lasso 함수가 있는 패키지 불러오기
\end_layout

\begin_layout Plain Layout

library(glmnet)
\end_layout

\begin_layout Plain Layout

# train, test data 나누기
\end_layout

\begin_layout Plain Layout

set.seed(1)
\end_layout

\begin_layout Plain Layout

train = sample(1:nrow(x), nrow(x)/2)
\end_layout

\begin_layout Plain Layout

test = -train
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# ridge 모델 적용하기
\end_layout

\begin_layout Plain Layout

# 변수 표준화는 자동으로!
\end_layout

\begin_layout Plain Layout

# alpha가 0일때는 ridge
\end_layout

\begin_layout Plain Layout

# 기본 값은 k=10(ten folds)
\end_layout

\begin_layout Plain Layout

mod.ridge = glmnet(x[train,],y[train],alpha=0)
\end_layout

\begin_layout Plain Layout

set.seed(1)
\end_layout

\begin_layout Plain Layout

cv.rid = cv.glmnet(x[train,],y[train],alpha=0)
\end_layout

\begin_layout Plain Layout

cv.rid$lambda.min
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# 과연 이때, test mse가 최소일까?
\end_layout

\begin_layout Plain Layout

a = 51:450
\end_layout

\begin_layout Plain Layout

b = rep(0,400)
\end_layout

\begin_layout Plain Layout

c = 0
\end_layout

\begin_layout Plain Layout

for (i in a){
\end_layout

\begin_layout Plain Layout

c = c + 1
\end_layout

\begin_layout Plain Layout

pred.ridge = predict(mod.ridge,s=i,newx=x[test,])
\end_layout

\begin_layout Plain Layout

b[c] = mean((pred.ridge-y[test])^2)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

plot(a,b,type='l',col='red',xlab='lambda',ylab='Test MSE')
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# 최소의 Test MSE를 갖는 람다로 전체 데이터를 적합시킨 후, 계수를 살펴보자
\end_layout

\begin_layout Plain Layout

final.model = glmnet(x,y,alpha=0)
\end_layout

\begin_layout Plain Layout

predict(final.model,type='coefficients',s=212)[1:20,]
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# lasso 모델 적용해보기
\end_layout

\begin_layout Plain Layout

# 변수 표준화는 자동으로!
\end_layout

\begin_layout Plain Layout

# alpha가 1일때는 lasso
\end_layout

\begin_layout Plain Layout

# 기본 값은 k=10(ten folds)
\end_layout

\begin_layout Plain Layout

mod.las = glmnet(x[train,],y[train],alpha=1)
\end_layout

\begin_layout Plain Layout

set.seed(1)
\end_layout

\begin_layout Plain Layout

cv.las = cv.glmnet(x[train,],y[train],alpha=1)
\end_layout

\begin_layout Plain Layout

cv.las$lambda.min
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

final.model = glmnet(x,y,alpha=1)
\end_layout

\begin_layout Plain Layout

predict(final.model,type='coefficients',s=17)[1:20,]
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\end_body
\end_document
