---
title: "R Notebook"
output: 
  github_document:
    pandoc_args: --webtex
---

## 6.5.1 Best Subset Selection

Hitters data로 best subset selection 진행. Salary 칼럼을 다른 변수들로 예측하는 상황.
우선 Salary 칼럼에 결측치가 있으므로 해당 행을 지우자.

```{r}
library(ISLR)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
hitters = na.omit(Hitters)
dim(hitters)
```

leaps library에 있는 regsubsets() function이 best subset selection을 시행해준다. 
문법은 lm과 동일하고 summary()는 각 모델에 대한 best set of variables을 출력해준다.
```{r}
library(leaps)
mod.best = regsubsets(Salary~. , hitters)
summary(mod.best)
```
*은 그 변수가 모델에 포함되었다는 뜻이다. 예를 들어 1개의 변수를 사용하는 애는 Hits와 CRBI를 포함한다는 뜻이다.
regsubsets()은 기본값으로 변수가 8개까지 포함된 best subset model을 출력해주는데 바꾸고 싶으면 nvmax 을 조정하면 된다.
```{r}
mod.best = regsubsets(Salary~. ,data=hitters,nvmax=19)
summary(mod.best)
names(summary(mod.best))
summary(mod.best)$adjr2
summary(mod.best)$rsq
```
RSS, adj R^2, Cp, BIC를 그래프로 그려보는 것은 어떤 모델을 선택할지 도움을 준다.
```{r}
library(ggplot2)
sum.mat = data.frame('rsq' = summary(mod.best)$rsq,'adjr2'=summary(mod.best)$adjr2,'bic'=summary(mod.best)$bic,'num.var' = 1:19)
#var 갯수에 따른 결정계수 그래프
ggplot(sum.mat,aes(x=num.var,y=rsq)) + geom_line(color='red')+ labs(title='rsq and ajd rsq with best subset selection',caption='red: rsq, blue: adj rsq') + geom_line(color='blue',aes(x=num.var,y=adjr2)) + geom_vline(xintercept =sum.mat$num.var[which.max(sum.mat$adjr2)],color='green')

#var 갯수에 따른 adj bic
ggplot(sum.mat,aes(x=num.var, y=bic)) + geom_line(color='red') + labs(title='bic with best subset selection') + geom_vline(xintercept = sum.mat$num.var[which.min(sum.mat$bic)],color='green')
```

# Forward and Backward stepwise selection
best subset selection을 실행하는 regsubsets func에서 method = 'forward' 또는 'backward'라고 하면 된다.

```{r}
mod.fwd = regsubsets(Salary~. , data=hitters, nvmax = 19, method = 'forward')
summary(mod.fwd)
mod.bwd = regsubsets(Salary~. , data=hitters, nvmax = 19, method = 'forward')
summary(mod.bwd)
```
변수가 7개일 때 어떤 변수들이 어떤 값으로 선택되었는지 알고 싶다면?
```{r}
coef(mod.bwd,7)
```

# Choosing among models using the validation set approach and cross-validation

train data와 test data의 index을 정한다.
```{r}
set.seed(1)
train = sample(c(TRUE,FALSE),nrow(hitters),replace=TRUE)
test = !train
```

train data로 best subset selection을 해보자.
```{r}
mod.best = regsubsets(Salary~. , data=hitters[train,],nvmax=19)
```

test data로부터 model matrix을 만든다.
```{r}
test.mat = model.matrix(Salary~. , data=hitters[test,])
```

이제 validation error을 계산해보자.
```{r}
val.error = rep(0,19)
for (i in 1:19){
  coefi = coef(mod.best,id=i)
  pred = test.mat[,names(coefi)] %*% coefi # %*%은 matrix multiplication.
  val.error[i] = mean((hitters$Salary[test]-pred)^2)
}
which.min(val.error) # test error가 가장 낮은 애는 9개의 변수를 포함하는 모델
coef(mod.best,9)
```

regsubsets()에 대한 predict 함수없기 때문에 위와같이 했다. 그렇다면 함수를 직접 만들어보자.
```{r}
predict.regsubsets = function(object,newdata,id,...){
  form = as.formula(object$call[[2]]) # object은 regsubsets() object
  mat = model.matrix(form,newdata) # model.matrix(model fit한 formula, data)
  coefi = coef(object, id=id) # regsubsets() 결과에서 var이 ~개일 때의 coef 저장하기
  xvars = names(coefi) # coefi 칼럼명 뽑아내기
  mar[,xvars]%*%coefi
}
```



## Ridge Regression
ridge와 lasso을 시행하는 함수 glmnet()은 glmnet 패키지 안에 있으며 다른 모델 적합 함수와는 문법이 약간 다르다. matrix 형태의 x와 vector 형태의 y를 쓴다.
```{r}
x = model.matrix(Salary~. , hitters)[,-1]
y = hitters$Salary
```

model.matrix()는 x를 만드는데 유용한 함수이다. 19개의 예측변수를 포함하는 행렬을 만들분만 아니라
범주형 자료를 자동으로 더미화시켜준다. glmnet() 함수는 수치형 자료만 받기 때문에 이렇더 더미화 해주는 것이 중요하다.

```{r}
library(glmnet)
grid = 10^seq(10,-2,length=100)
mod.ridge = glmnet(x,y,alpha=0,lambda=grid) 
```
glmnet()은 alpha = 0이면 ridge이고 = 1이면 lasso을 시행한다. 원래 glmnet()은 자동적으로 람다를 선택하는데 여기서는 10^10부터 10^2까지 해봄.
또한 glmnet()은 자동적으로 변수를 표준화(standardize) 해준다(lasso, ridge을 할때 변수를 표준화해주라고 본문에 나와있음.)
이 기능을 없애고 싶으면 standardize=FALSE 해주면 됨.
모델 적합 결과는 람다 값에 따른 matrix 형태이다. 여기서는 변수가 20개 있고 람다 값을 100개 설정했으므로 20 * 100 matrix이다.
```{r}
dim(coef(mod.ridge))
```

본문에서 살펴봤듯이, 람다가 커질수록(l2 norm이 작아질수록), penalty를 많이 준다는 뜻이고, 계수들이 작아질 것이다.
```{r}
mod.ridge$lambda[50] # 람다가 11498일때
coef(mod.ridge)[,50] # 계수들을 살펴보자
sqrt(sum(coef(mod.ridge)[-1,50]^2)) # 절편 제외 l2 norm은??
```

```{r}
mod.ridge$lambda[60] # 람다가 705일때
coef(mod.ridge)[,60] # 계수들을 살펴보자
sqrt(sum(coef(mod.ridge)[-1,60]^2)) # 절편 제외 l2 norm은??
```

l2 norm이 람다가 작아짐에 따라 커졌음을 확인할 있다!!

predict() func을 새로운 람다에 대해서 ridge reg을 할 때에도 사용할 수 있다. 예를 들어 람다 50에 대해서는,

```{r}
predict(mod.ridge,s=50,type='coefficients')[1:20,]
```

이제 train이랑 test으로 쪼개서 모델을 적합시키는 과정을 살펴본다.
보통 데이터를 이렇게 두개로 쪼개는 것은 두 가방 방법이 있는데 TRUE FALSE 벡터를 생성하는 것, 또는 1:n 중 train data index을 뽑는 것이다.

```{r}
set.seed(1)
train = sample(1:nrow(x),nrow(x)/2)
test = -train
y.test = y[test]
```

이제 train data로 적합하 하고 test data로 성능평 평가해보자. 
```{r}
mod.ridge = glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred = predict(mod.ridge,s=4,newx=x[test,])
mean((y.test - ridge.pred)^2)
```

절편만 포함하는 ridge는?
```{r}
mean((mean(y[train])-y.test)^2)
```

이제 cross validation을 해보자! glmnet library안에 cv.glmnet()가 이미 내장되어 있다. 기본값은 10 folds이다. 바꾸고 싶으면 nfolds =.
```{r}
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train],alpha=0)
cv.out$lambda.min
```

가장 작은 cross validation error을 출력하는 람다는 212이다.그렇다면 이때 test mse는 몇일까?
```{r}
ridge.pred = predict(mod.ridge,s=212,newx=x[test,])
mean((y[test]-ridge.pred)^2)
```

이제 cv를 이용해서 가장 작은 test mse를 출력하는 람다를 찾았으니 전체 train data로 적합을 해보자!
```{r}
final.out = glmnet(x,y,alpha=0,lambda=212)
coef(final.out)
```

앞서 살펴봤듯이, ridge는 어떠한 계수도 0으로 만들지 않는다. 즉, variable selection을 하지 않는다!

## The Lasso
lasso는 ridge와 동일하게 glmnet()을 통해서 시행되며 alpha=1라고 설정해야 한다. 
```{r}
mod.lasso = glmnet(x[train,],y[train],alpha=1,lambda=grid)
```

이제 cross validation을 해보자.
```{r}
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train],alpha=1)
cv.out$lambda.min
```

lasso의 test mse 계산하기
```{r}
lasso.pred = predict(mod.lasso,s=17,newx=x[test,])
mean((y[test]-lasso.pred)^2)
```

test mse는 ridge에서 람다가 212일때랑 거의 차이가 없지만! lasso의 장점은 ridge와는 다르게 몇몇 계수를 0으로 만든다는 점이다.

```{r}
final.lasso.out = glmnet(x,y,lambda=17,alpha=1)
coef(final.lasso.out)
```

따라서 해석의 측면에서 ridge보다 더욱 강점이 있다.

#Lab 3: PCR and PLS regression
## Principal Components Regression
PCR은 pls library의 pcr()을 통해서 할 수 있다. 
기본적인 구조는 lm()과 유사하나, scale=TRUE을 통해서 표준화를 진행하여 변수의 스케일이 결과에 영향을 미치지 않도록, 
validation = 'CV'를 통해서 기본값으로 ten-fold-cv를 통해 M(# of principal components)을 도출하게 한다.
```{r}
library(pls)
set.seed(2)
mod.pcr = pcr(Salary~. , data=hitters, scale=TRUE, validation = 'CV')
summary(mod.pcr)
```

M=0일때부터, M=19일때까지, 각각의 CV score을 볼 수 있는데, 이것이 RMSEP(root mean square error)이므로 실제 MSE를 계산하려면 제곱을 해줘야 한다.

또한 cv score를 validationplot()을 통해 그래프로 볼 수 있고, 이때 val.type='MSEP'을 통해 RMSE가 아니라 MSE로 나오게 설정할 수 있다.

```{r}
validationplot(mod.pcr,val.type='MSEP')
```

그림을 보면 M=16일 때가 최소값을 가지는데 사실 M=1일 때랑 그렇게 큰 차이가 없다. 따라서 M=1이면 충분하다고 결론지을 수 있다.

이제 CV를 통해 PCR을 해보자.
```{r}
set.seed(1)
mod.pcr = pcr(Salary~. , data=hitters, subset=train, scale=TRUE, validation='CV')
validationplot(mod.pcr, val.type="MSEP")
```

mod.pcr의 결과로 나온 MSEP는 어떻게 접근할까??
M=7일때 test mse가 가장 낮다.
```{r}
pcr.pred = predict(mod.pcr,x[test,],ncomp=7)
mean((y[test] - pcr.pred)^2)
```

test mse가 ridge와 비슷하게 낮지만 pcr은 variable selection을 하는 것도 아니고, 계수에 대해서 estimate을 하는 것도 아니기 때문에
모델을 해석하기가 더 어려워졌다!

## Partial Least Squares
PLS는 plst library에 있는 plsr() func을 이용하면 된다. 문법은 pcr() func과 비슷하다.

```{r}
set.seed(1)
mod.pls = plsr(Salary~. , data=hitters, subset=train, scale=TRUE, validation='CV')
summary(mod.pls)
```

가장 작은 cv error는 M = 2일때 발생하였으며, 이때 test mse를 계산해보면,

```{r}
pls.pred = predict(mod.pls, newdata=hitters[test,], ncomp=2)
mean((y[test]-pls.pred)^2)
```

PCR과 PLS의 결과를 살펴보면 PCR은 M = 7일때 percentage of variance가 46%였는데, PLS는 M = 2일때 거의 동일하게 percentage of variance가
46%이다. 이는 PCR이 오직 예측변수에서 variance의 양을 최소화하려고 한다면(unsupervised learning) PLS는 예측변수와 반응변수(supervised learning)
에서 variance을 설명하는 direction을 찾기 때문에 발생하는 것이다.